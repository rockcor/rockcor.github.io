<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-02-13T05:43:27.829Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>复杂网络基础</title>
    <link href="http://example.com/2023/02/13/%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2023/02/13/%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</id>
    <published>2023-02-13T08:50:34.000Z</published>
    <updated>2023-02-13T05:43:27.829Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图论"><a href="#图论" class="headerlink" title="图论"></a>图论</h1><h2 id="集聚系数"><a href="#集聚系数" class="headerlink" title="集聚系数"></a>集聚系数</h2><ol><li><strong>局部集聚系数</strong>: 节点集实际连边数/理论最大连边数</li><li><strong>全局集聚系数</strong>: 3x三角形个数/三元组个数<blockquote><p>如何计算三元组个数：$C_{d_i}^2$</p></blockquote></li></ol><h2 id="介数"><a href="#介数" class="headerlink" title="介数"></a>介数</h2><p>节点i的介数：所有最短路径中，经过节点i的数量<br>边ij的介数：所有最短路径中，经过边ij的数量</p><h2 id="核度"><a href="#核度" class="headerlink" title="核度"></a>核度</h2><p>若一个节点属于k核而不属于（k+1）核，则该节点核度为k</p><h1 id="随机网络"><a href="#随机网络" class="headerlink" title="随机网络"></a>随机网络</h1><h2 id="ER生成算法"><a href="#ER生成算法" class="headerlink" title="ER生成算法"></a>ER生成算法</h2><p>G(N,L) vs. G(N,p)</p><pre><code class="python">def GNL(N,L)    G=nx.Graph()    nlist=list(G.add_nodes_from(range(N)))    edge_count=0    while edge_count&lt;L:        u=random.choice(nlist)        v=random.choice(nlist)        if u==v or G.has_edge(u,v):            continue        else:            G.add_edge(u,v)            edge_count+=1    return G</code></pre><pre><code class="python">def GNP(N,p)    G=nx.Graph()    G.add_nodes_from(range(N))    edges=itertools.combinations(range(N),2)    for e in edges:        if random.random()&lt;p:            G.add_edge(*e)    return G</code></pre><h2 id="ER随机网络的性质"><a href="#ER随机网络的性质" class="headerlink" title="ER随机网络的性质"></a>ER随机网络的性质</h2><ol><li>$平均度=p(N-1)$</li><li>度分布为二项分布，在较大网络中近似为泊松分布<br><img src="/.com//Pasted%20image%2020230127211443.png"></li><li>当N趋于无穷，度分布为泊松分布，平均距离小，集聚系数小</li><li>巨连通分支规模和p的关系<blockquote><p>横坐标为$pN$，纵坐标为$巨连通分支节点数/N$</p></blockquote></li></ol><p><img src="/.com//Pasted%20image%2020230127212153.png"></p><p><img src="/.com//Pasted%20image%2020230127212953.png"></p><h1 id="小世界网络"><a href="#小世界网络" class="headerlink" title="小世界网络"></a>小世界网络</h1><p><strong>小</strong>的定义：平均距离正比于$lnN$</p><h2 id="WS模型"><a href="#WS模型" class="headerlink" title="WS模型"></a>WS模型</h2><p>在k近邻规则网络中对边进行重连</p><blockquote><p>性质：随机重连前，集聚系数较高，平均距离较大；随机重连后，平均距离变小，集聚系数的降低需要更高的重连概率，越来越趋近于随机网络<br><img src="/.com//Pasted%20image%2020230128144525.png"></p></blockquote><h2 id="NW模型"><a href="#NW模型" class="headerlink" title="NW模型"></a>NW模型</h2><p>在k近邻规则网络中以p的概率加边</p><h1 id="无标度（幂律）网络"><a href="#无标度（幂律）网络" class="headerlink" title="无标度（幂律）网络"></a>无标度（幂律）网络</h1><h2 id="离散形式"><a href="#离散形式" class="headerlink" title="离散形式"></a>离散形式</h2><p>$$<br>p_k=Ck^{-\gamma}<br>$$<br>$$<br>\sum_{k=1}^\infty p_k=1<br>$$</p><h2 id="和随机网络的对比"><a href="#和随机网络的对比" class="headerlink" title="和随机网络的对比"></a>和随机网络的对比</h2><p>两头多，中间少</p><blockquote><p>紫色为无标度网络，绿色为随机网络（泊松分布）。度分布-双对数坐标轴。<br><img src="/.com//image-20230128194302960.png"></p></blockquote><h2 id="最大度期望"><a href="#最大度期望" class="headerlink" title="最大度期望"></a>最大度期望</h2><p>$$<br>k_{max}=k_{min}N^{\frac{1}{\gamma-1}}<br>$$</p><ol><li>总节点数越大，最大度和最小度的期望差距越大</li><li>$\gamma$ 越大，最大度和最小度的期望差距越小</li></ol><h2 id="平均距离"><a href="#平均距离" class="headerlink" title="平均距离"></a>平均距离</h2><p><img src="/.com//image-20230128203233246.png"></p><h2 id="集聚系数-1"><a href="#集聚系数-1" class="headerlink" title="集聚系数"></a>集聚系数</h2><p>$$<br>\langle C \rangle \sim\frac{(lnN)^2}{N}<br>$$</p><h2 id="度保持的随机化"><a href="#度保持的随机化" class="headerlink" title="度保持的随机化"></a>度保持的随机化</h2><p>nx.double_edge_swap</p><pre><code>u--v            u  v       becomes  |  |x--y            x  y</code></pre><h1 id="BA无标度网络"><a href="#BA无标度网络" class="headerlink" title="BA无标度网络"></a>BA无标度网络</h1><ol><li>网络是逐步生长的</li><li>每个时间步增加m个节点，优先选择度高的节点（偏好连接）<br><img src="/.com//image-20230128233914746.png"></li></ol><h2 id="非线性偏好连接"><a href="#非线性偏好连接" class="headerlink" title="非线性偏好连接"></a>非线性偏好连接</h2><p><img src="/.com//image-20230128235913105.png"></p><h1 id="网络鲁棒性"><a href="#网络鲁棒性" class="headerlink" title="网络鲁棒性"></a>网络鲁棒性</h1><h2 id="集聚影响力（Collective-Influence）"><a href="#集聚影响力（Collective-Influence）" class="headerlink" title="集聚影响力（Collective Influence）"></a>集聚影响力（Collective Influence）</h2><p><img src="/.com//image-20230129205022475.png"></p><blockquote><p>可以用偏导符号表示k阶邻居 #H2GCN</p></blockquote><h1 id="社团检测"><a href="#社团检测" class="headerlink" title="社团检测"></a>社团检测</h1><p><strong>强社区</strong>：社区内部节点的度都大于外部节点的度<br><strong>弱社区</strong>：社区内部节点的度之和大于外部节点的度之和</p><h2 id="层次聚类算法"><a href="#层次聚类算法" class="headerlink" title="层次聚类算法"></a>层次聚类算法</h2><h3 id="劳沃斯算法（凝聚算法）"><a href="#劳沃斯算法（凝聚算法）" class="headerlink" title="劳沃斯算法（凝聚算法）"></a>劳沃斯算法（凝聚算法）</h3><ol><li>将每个节点视为一个社区</li><li>计算社区两两相似度（平均簇相似度）</li><li>合并相似度最高的两个社区（如果有至少一条连边），绘制树状图</li><li>重复步骤2和3，直到所有节点被并入一个社区</li></ol><h3 id="格文-纽曼算法（分裂算法）"><a href="#格文-纽曼算法（分裂算法）" class="headerlink" title="格文-纽曼算法（分裂算法）"></a>格文-纽曼算法（分裂算法）</h3><ol><li>计算每条边的介数，删除最高的若干条，直到有子图分离</li><li>将分离的若干个子图作为新的叶子节点，直到这些子图只有一个节点</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;图论&quot;&gt;&lt;a href=&quot;#图论&quot; class=&quot;headerlink&quot; title=&quot;图论&quot;&gt;&lt;/a&gt;图论&lt;/h1&gt;&lt;h2 id=&quot;集聚系数&quot;&gt;&lt;a href=&quot;#集聚系数&quot; class=&quot;headerlink&quot; title=&quot;集聚系数&quot;&gt;&lt;/a&gt;集聚系数&lt;/h</summary>
      
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="math" scheme="http://example.com/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>头条大数据推荐项目</title>
    <link href="http://example.com/2023/02/13/%E5%A4%B4%E6%9D%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/"/>
    <id>http://example.com/2023/02/13/%E5%A4%B4%E6%9D%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/</id>
    <published>2023-02-13T05:44:39.000Z</published>
    <updated>2023-02-13T05:42:09.678Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/.com//image-20230211193156286.png"></p><h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><h2 id="启动-hadoop、hive-连接-mysql"><a href="#启动-hadoop、hive-连接-mysql" class="headerlink" title="启动 hadoop、hive(连接 mysql)"></a>启动 hadoop、hive(连接 mysql)</h2><pre><code class="shell">#~/hadoop_code/start_hive.shstart-all.shservice docker startdocker start mysqlhive --service metastore &amp;</code></pre><pre><code class="shell">#查看mysqldocker exec -it mysql bashmysql -uroot -p#密码: passwordctrl+P+Q 退出</code></pre><h2 id="启动-hbase、spark、thriftserver"><a href="#启动-hbase、spark、thriftserver" class="headerlink" title="启动 hbase、spark、thriftserver"></a>启动 hbase、spark、thriftserver</h2><pre><code class="shell">cd ~/bigdatastart-hbase.sh./spark/sbin/start-all.shhbase thrift start</code></pre><h2 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h2><pre><code class="shell">jps10948 ThriftServer3816 ResourceManager3145 DataNode6571 HMaster4813 RunJar7667 Master13557 Jps6998 HRegionServer9691 Worker9948 RunJar3645 SecondaryNameNode2751 NameNode4223 NodeManager6463 HQuorumPeer</code></pre><h1 id="离线计算更新物品画像"><a href="#离线计算更新物品画像" class="headerlink" title="离线计算更新物品画像"></a>离线计算更新物品画像</h1><h2 id="用-Sqoop-迁移和同步数据库"><a href="#用-Sqoop-迁移和同步数据库" class="headerlink" title="用 Sqoop 迁移和同步数据库"></a>用 Sqoop 迁移和同步数据库</h2><p>业务数据通常存放在 mysql 数据库中，我们需要把它定期同步到 hadoop 的 hive 数据仓库中。</p><pre><code class="sql">create database if not exists toutiao comment "user,news information of 136 mysql" location '/user/hive/warehouse/toutiao.db/';</code></pre><pre><code class="shell">sqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P</code></pre><p>密码：<strong>password</strong><br>会显示连接到的数据库:</p><pre><code>information_schemahivemysqlperformance_schemasystoutiao</code></pre><p>写增量导入的 Sqoop 脚本</p><pre><code class="shell">#/root/toutiao_project/scripts/import_incremental.shtime=`date +"%Y-%m-%d" -d "-1day"`declare -A checkcheck=([user_profile]=update_time [user_basic]=last_login [news_channel]=update_time)declare -A mergemerge=([user_profile]=user_id [user_basic]=user_id [news_channel]=channel_id)for k in ${!check[@]}do    sqoop import \        --connect jdbc:mysql://192.168.19.137/toutiao \        --username root \        --password password \        --table $k \        --m 4 \        --target-dir /user/hive/warehouse/toutiao.db/$k \        --incremental lastmodified \        --check-column ${check[$k]} \        --merge-key ${merge[$k]} \        --last-value ${time}done</code></pre><p>写 crontab-shell 脚本让 Sqoop 定时运行</p><pre><code class="shell">crontab -e#每30分钟运行一次*/30 * * * * /root/toutiao_project/scripts/import_incremental.shservice crond start</code></pre><blockquote><p>这里 MySQL 里面没有创建好，实际会报错，不管。</p></blockquote><h2 id="用户行为埋点收集"><a href="#用户行为埋点收集" class="headerlink" title="用户行为埋点收集"></a>用户行为埋点收集</h2><h3 id="埋点设置"><a href="#埋点设置" class="headerlink" title="埋点设置"></a>埋点设置</h3><pre><code class="json"># 曝光的参数，{"actionTime":"2019-04-10 18:15:35","readTime":"","channelId":0,"param":{"action": "exposure", "userId": "2", "articleId": "[18577, 14299]", "algorithmCombine": "C2"}}# 对文章发生行为的参数{"actionTime":"2019-04-10 18:12:11","readTime":"2886","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}{"actionTime":"2019-04-10 18:15:32","readTime":"","channelId":18,"param":{"action": "click", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}{"actionTime":"2019-04-10 18:15:34","readTime":"1053","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}...</code></pre><h3 id="用-flume-收集到-hive-中"><a href="#用-flume-收集到-hive-中" class="headerlink" title="用 flume 收集到 hive 中"></a>用 flume 收集到 hive 中</h3><p>创建 flume 配置文件</p><pre><code class="shell">#/root/bigdata/flume/collect_click.confa1.sources = s1a1.sinks = k1a1.channels = c1# 实时查看日志文件尾a1.sources.s1.channels= c1a1.sources.s1.type = execa1.sources.s1.command = tail -F /root/logs/userClick.log# 设置两个拦截器 1.格式过滤 2.附加时间戳a1.sources.s1.interceptors=i1 i2a1.sources.s1.interceptors.i1.type=regex_filtera1.sources.s1.interceptors.i1.regex=\\{.*\\}a1.sources.r1.interceptors.i1.excludeEvents = falsea1.sources.s1.interceptors.i2.type=timestamp# 指定缓冲区和batchdataa1.channels.c1.type=memorya1.channels.c1.capacity=30000a1.channels.c1.transactionCapacity=1000# 连接hdfsa1.sinks.k1.type=hdfsa1.sinks.k1.channel=c1a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%da1.sinks.k1.hdfs.useLocalTimeStamp = truea1.sinks.k1.hdfs.fileType=DataStreama1.sinks.k1.hdfs.writeFormat=Texta1.sinks.k1.hdfs.rollInterval=0a1.sinks.k1.hdfs.rollSize=10240a1.sinks.k1.hdfs.rollCount=0a1.sinks.k1.hdfs.idleTimeout=60</code></pre><p>hive 中创建数据库和表</p><pre><code class="sql">create database if not exists profile comment "user action" location '/user/hive/warehouse/profile.db/';create table user_action(actionTime STRING comment "user actions time",readTime STRING comment "user reading time",channelId INT comment "article channel id",param map comment "action parameter")COMMENT "user primitive action"PARTITIONED BY(dt STRING)ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'LOCATION '/user/hive/warehouse/profile.db/user_action';</code></pre><p><code>ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'</code>:添加 json 格式匹配功能</p><p>flume 会自动生成目录，在 hive 内部表上直接同步。但是如果想要通过 spark sql 获取内容，每天还是要主动关联：</p><pre><code class="sql">alter table user_action add partition (dt='2023-02-11') location "/user/hive/warehouse/profile.db/user_action/2023-02-11/"</code></pre><h3 id="使用-supervisor-管理-flume-进程"><a href="#使用-supervisor-管理-flume-进程" class="headerlink" title="使用 supervisor 管理 flume 进程"></a>使用 supervisor 管理 flume 进程</h3><p>flume 及其依赖写入脚本/root/toutiao_project/scripts/collect-click.sh</p><pre><code class="shell">#!/usr/bin/env bashexport JAVA_HOME=/root/bigdata/jdkexport HADOOP_HOME=/root/bigdata/hadoopexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1</code></pre><p>在/etc/supervisor 的 reco.conf 添加</p><pre><code class="shell">[program:collect-click]command=/bin/bash /root/toutiao_project/scripts/collect_click.shuser=rootautorestart=trueredirect_stderr=truestdout_logfile=/root/logs/collect.logloglevel=infostopsignal=KILLstopasgroup=truekillasgroup=true</code></pre><p>最后用 supervisord 启动收集</p><pre><code class="shell">pip install supervisorsupervisord -c /etc/supervisord.confsupervisorctl status</code></pre><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><pre><code class="shell">echo {\"actionTime\":\"2023-02-11 21:04:39\",\"readTime\":\"\",\"channelId\":18,\"param\":{\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"}} &gt;&gt; userClick.log</code></pre><p>在 <a href="http://192.168.19.137:50070/explorer.html#/user/hive/warehouse/profile.db/user_action/">前端页面</a> 和 hive 中应当看到结果。</p><h2 id="离线文章画像计算"><a href="#离线文章画像计算" class="headerlink" title="离线文章画像计算"></a>离线文章画像计算</h2><h3 id="原始文章数据合并"><a href="#原始文章数据合并" class="headerlink" title="原始文章数据合并"></a>原始文章数据合并</h3><ol><li>创建 spark 基类</li><li>启动 jupyter</li></ol><pre><code class="shell">source activate py365jupyter notebook --allow-root --ip=192.168.19.137# 密码：123</code></pre><ol start="3"><li>运行 full_call/merge_data</li></ol><h3 id="历史文章-tfidf-计算"><a href="#历史文章-tfidf-计算" class="headerlink" title="历史文章 tfidf 计算"></a>历史文章 tfidf 计算</h3><ol><li>jieba 分词，去除停用词，保留名词、英文和自定义词库中的词</li><li>使用 spark ML 中 CountVectorizer 包进行词频统计，得到词袋模型/字典<br><img src="/.com//image-20230211214707213.png"></li><li>使用 spark ML 中 IDF 包进一步计算每个单词的权重</li><li>根据索引和权重排序得到可以每篇文章权重最高的 20 个词</li></ol><h3 id="历史文章-textrank-计算"><a href="#历史文章-textrank-计算" class="headerlink" title="历史文章 textrank 计算"></a>历史文章 textrank 计算</h3><pre><code class="python">def textrank(partition):    import os    import jieba    import jieba.analyse    import jieba.posseg as pseg    import codecs    abspath = "/root/words"    # 结巴加载用户词典    userDict_path = os.path.join(abspath, "ITKeywords.txt")    jieba.load_userdict(userDict_path)    # 停用词文本    stopwords_path = os.path.join(abspath, "stopwords.txt")    def get_stopwords_list():        """返回stopwords列表"""        stopwords_list = [i.strip()                          for i in codecs.open(stopwords_path).readlines()]        return stopwords_list    # 所有的停用词列表    stopwords_list = get_stopwords_list()    class TextRank(jieba.analyse.TextRank):        def __init__(self, window=20, word_min_len=2):            super(TextRank, self).__init__()            self.span = window  # 窗口大小            self.word_min_len = word_min_len  # 单词的最小长度            # 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac            self.pos_filt = frozenset(                ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', "nw", "nz", "PER", "LOC", "ORG"))        def pairfilter(self, wp):            """过滤条件，返回True或者False"""            if wp.flag == "eng":                if len(wp.word) &lt;= 2:                    return False            if wp.flag in self.pos_filt and len(wp.word.strip()) &gt;= self.word_min_len \                    and wp.word.lower() not in stopwords_list:                return True    # TextRank过滤窗口大小为5，单词最小为2    textrank_model = TextRank(window=5, word_min_len=2)    allowPOS = ('n', "x", 'eng', 'nr', 'ns', 'nt', "nw", "nz", "c")</code></pre><p>同样可以给出 20 个关键词。但是最终结果由 Textank * IDF 再取前 20 给出<br><img src="/.com//image-20230211220415129.png"></p><h3 id="训练词向量模型-word2vec-和增量文章编码"><a href="#训练词向量模型-word2vec-和增量文章编码" class="headerlink" title="训练词向量模型 word2vec 和增量文章编码"></a>训练词向量模型 word2vec 和增量文章编码</h3><pre><code class="python">from pyspark.ml.feature import Word2Vec# minCount忽略总频率低于此频率的所有单词w2v = Word2Vec(vectorSize=100, inputCol='words', outputCol='model', minCount=3)w2v_model = w2v.fit(words_df)w2v_model.write().overwrite().save("hdfs://hadoop-master:9000/headlines/models/test.word2vec")from pyspark.ml.feature import Word2VecModelword_vec = Word2VecModel.load("hdfs://hadoop-master:9000/headlines/models/test.word2vec")vectors = word_vec.getVectors()</code></pre><p>编码后和每个单词权重相乘，最终得到每篇文章的特征向量（文章画像）</p><h3 id="用-Apscheduler-定时更新文章画像"><a href="#用-Apscheduler-定时更新文章画像" class="headerlink" title="用 Apscheduler 定时更新文章画像"></a>用 Apscheduler 定时更新文章画像</h3><ol><li>增量更新文章编码，包括 hive 里的 article_profile<blockquote><p>新词可以用平均值填充</p></blockquote></li><li>定期重新计算 tfidf、textrank 和 word2vec 模型</li><li>Apsheduler 是 crontab 升级版</li></ol><pre><code class="python">import sysimport osBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))sys.path.insert(0, os.path.join(BASE_DIR))sys.path.insert(0, os.path.join(BASE_DIR, 'reco_sys'))from apscheduler.schedulers.blocking import BlockingSchedulerfrom apscheduler.executors.pool import ProcessPoolExecutorfrom scheduler.update import update_article_profile# 创建scheduler，多进程执行executors = {    'default': ProcessPoolExecutor(3)}scheduler = BlockingScheduler(executors=executors)# 添加定时更新任务更新文章画像,每隔一小时更新scheduler.add_job(update_article_profile, trigger='interval', hours=1)scheduler.start()</code></pre><h1 id="离线用户召回集与排序计算"><a href="#离线用户召回集与排序计算" class="headerlink" title="离线用户召回集与排序计算"></a>离线用户召回集与排序计算</h1><h2 id="用户画像存储与获取"><a href="#用户画像存储与获取" class="headerlink" title="用户画像存储与获取"></a>用户画像存储与获取</h2><p>用户画像需要快速迭代，方便读取，选择存储在 hbase 中。这里我们从 hbase 关联到 hive。</p><pre><code class="sql">create external table user_profile_hbase(user_id STRING comment "userID",information map&lt;string, DOUBLE&gt; comment "user basic information",article_partial map&lt;string, DOUBLE&gt; comment "article partial",env map&lt;string, INT&gt; comment "user env")COMMENT "user profile table"STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,basic:,partial:,env:")TBLPROPERTIES ("hbase.table.name" = "user_profile");</code></pre><p>读取 user_article_basic 表，<strong>合并行为表</strong>与<strong>文章画像中的主题词</strong></p><pre><code class="python"># 获取基本用户行为信息，然后进行文章画像的主题词合并uup.spark.sql("use profile")# 取出日志中的channel_iduser_article_ = uup.spark.sql("select * from user_article_basic").drop('channel_id')uup.spark.sql('use article')article_label = uup.spark.sql("select article_id, channel_id, topics from article_profile")# 合并使用文章中正确的channel_idclick_article_res = user_article_.join(article_label, how='left', on=['article_id'])</code></pre><h3 id="用户权重计算公式"><a href="#用户权重计算公式" class="headerlink" title="用户权重计算公式"></a>用户权重计算公式</h3><p><strong>用户标签权重 =( 行为类型权重之和) × 时间衰减</strong></p><table><thead><tr><th>行为</th><th>分值</th></tr></thead><tbody><tr><td>阅读时间(&lt;1000)</td><td>1</td></tr><tr><td>阅读时间(&gt;=1000)</td><td>2</td></tr><tr><td>收藏</td><td>2</td></tr><tr><td>分享</td><td>3</td></tr><tr><td>点击</td><td>5</td></tr><tr><td><strong>时间衰减</strong>=1/(log(t)+1) ,t 为时间发生时间距离当前时间的大小</td><td></td></tr></tbody></table><p>使用 happybase 关联文章表，统计每个词的标签权重，得到用户的关键词喜好 top10</p><pre><code class="python">import happybase#  用于读取hbase缓存结果配置pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)with pool.connection() as conn:    table = conn.table('user_profile')    # 获取每个键 对应的所有列的结果    data = table.row(b'user:2', columns=[b'partial'])    conn.close()</code></pre><p>完善代码后，添加到 Apscheduler 中</p><pre><code class="python">scheduler.add_job(update_user_profile, trigger='interval', hours=2)</code></pre><h2 id="召回排序"><a href="#召回排序" class="headerlink" title="召回排序"></a>召回排序</h2><ul><li>用户冷启动（前期点击行为较少情况）<ul><li>非个性化推荐<ul><li><strong>热门召回</strong>：自定义热门规则，根据当前时间段热点定期更新维护<em>热点文章库</em></li><li><strong>新文章召回</strong>：为了提高新文章的曝光率，建立<em>新文章库</em>，进行推荐</li></ul></li><li>个性化推荐：<ul><li><strong>基于内容的协同过滤在线召回</strong>：基于用户实时兴趣画像相似的召回结果用于首页的个性化推荐</li></ul></li></ul></li><li>后期离线部分（用户点击行为较多，用户画像完善）<ul><li>建立用户长期兴趣画像（详细）：包括用户各个维度的兴趣特征</li><li>训练排序模型<ul><li><strong>LR 模型、FTRL、Wide&amp;Deep</strong></li></ul></li><li>离线部分的召回：<ul><li><strong>基于模型协同过滤推荐离线召回</strong>：ALS</li><li><strong>基于内容的离线召回</strong>：或者称基于用户画像的召回</li></ul></li></ul></li></ul><h3 id="基于-ALS-模型的召回"><a href="#基于-ALS-模型的召回" class="headerlink" title="基于 ALS 模型的召回"></a>基于 ALS 模型的召回</h3><pre><code class="python">from pyspark.ml.recommendation import ALS# 模型训练和推荐默认每个用户固定文章个数als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)model = als.fit(als_user_article_click)recall_res = model.recommendForAllUsers(100)</code></pre><p>召回结果存储</p><pre><code class="python">def save_offline_recall_hbase(partition):    """离线模型召回结果存储    """    import happybase    pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)    for row in partition:        with pool.connection() as conn:            # 获取历史看过的该频道文章            history_table = conn.table('history_recall')            # 多个版本            data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),                                       'channel:{}'.format(row.channel_id).encode())            history = []            if len(data) &gt;= 2:                for l in data[:-1]:                    history.extend(eval(l))            else:                history = []            # 过滤reco_article与history            reco_res = list(set(row.article_list) - set(history))            if reco_res:                table = conn.table('cb_recall')                # 默认放在推荐频道                table.put('recall:user:{}'.format(row.user_id).encode(),                          {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})                conn.close()                # 放入历史推荐过文章                history_table.put("reco:his:{}".format(row.user_id).encode(),                                  {'channel:{}'.format(row.channel_id): str(reco_res).encode()})            conn.close()als_recall.foreachPartition(save_offline_recall_hbase)</code></pre><h3 id="基于内容的召回"><a href="#基于内容的召回" class="headerlink" title="基于内容的召回"></a>基于内容的召回</h3><p>即根据 LHS 等算法，快速得到用户当前点击文章的相似文章集，进行推荐。</p><pre><code class="python"># 循环partitionfor row in partition:    # 获取相似文章结果表    similar_article = similar_table.row(str(row.article_id).encode(),                                        columns=[b'similar'])    # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千    _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)    if _srt:        # 每次行为推荐若干篇文章        reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]</code></pre><blockquote><p>基于内容和基于模型的结果存入同一张 hbase 表</p></blockquote><pre><code class="sql">create external table cb_recall_hbase(user_id STRING comment "userID",als map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment "als recall",content map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment "content recall",online map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment "online recall")COMMENT "user recall table"STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,als:,content:,online:")TBLPROPERTIES ("hbase.table.name" = "cb_recall");</code></pre><h3 id="离线排序模型-CTR"><a href="#离线排序模型-CTR" class="headerlink" title="离线排序模型 CTR"></a>离线排序模型 CTR</h3><p>CTR（Click-Through Rate）预估：给定一个 Item，预测该 Item 会被点击的概率<br>最基础的模型目前都是基于 LR 的点击率预估策略，目前在工业使用模型做预估的有这么几种类型</p><ul><li>宽模型 + 特征⼯程<ul><li>LR/MLR + 非 ID 类特征(⼈⼯离散/GBDT/FM)</li><li>spark 中可以直接使用</li></ul></li><li>宽模型 + 深模型<ul><li>wide&amp;deep,DeepFM</li><li>使用 TensorFlow 进行训练</li></ul></li><li>深模型：<ul><li>DNN + 特征 embedding</li><li>使用 TensorFlow 进行训练</li></ul></li></ul><p>特征包含：用户画像关键词 10+文章画像关键词 10+channel_id(25, onehot)+文章主题词向量(concat, 100)</p><pre><code class="python">cols = ['article_id', 'user_id', 'channel_id', 'articlevector', 'weights', 'article_weights', 'clicked']train_version_two = VectorAssembler().setInputCols(cols[2:6]).setOutputCol("features").transform(train)</code></pre><p><img src="/.com//image-20230212102837399.png"><br>训练线性回归模型，可以服务于在线召回</p><pre><code class="python">lr = LogisticRegression()model = lr.setLabelCol("clicked").setFeaturesCol("features").fit(train_version_two)model.save("hdfs://hadoop-master:9000/headlines/models/lr.obj")</code></pre><p>定期重新训练</p><h1 id="实时计算业务"><a href="#实时计算业务" class="headerlink" title="实时计算业务"></a>实时计算业务</h1><p>实时（在线）计算：</p><ul><li>解决用户冷启动问题</li><li>实时计算能够根据用户的点击实时反馈，快速跟踪用户的喜好</li></ul><p>日志数据我们已经收集到 hadoop 中，但是做实时分析的时候，我们需要将每个时刻用户产生的点击行为收集到 KAFKA 当中，等待 spark streaming 程序去消费。</p><h2 id="Kafka-简介"><a href="#Kafka-简介" class="headerlink" title="Kafka 简介"></a>Kafka 简介</h2><p><strong>Kafka</strong>是由<a href="https://zh.wikipedia.org/wiki/Apache%E8%BD%AF%E4%BB%B6%E5%9F%BA%E9%87%91%E4%BC%9A" title="Apache软件基金会">Apache 软件基金会</a>开发的一个<a href="https://zh.wikipedia.org/wiki/%E5%BC%80%E6%BA%90" title="开源">开源</a><a href="https://zh.wikipedia.org/wiki/%E6%B5%81%E5%A4%84%E7%90%86" title="流处理">流处理</a>平台，由<a href="https://zh.wikipedia.org/wiki/Scala" title="Scala">Scala</a>和<a href="https://zh.wikipedia.org/wiki/Java" title="Java">Java</a>编写。该项目的目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个“按照分布式事务日志架构的大规模发布/订阅消息队列”，这使它作为企业级基础设施来处理流式数据非常有价值。</p><h2 id="flume-收集日志到-kafka"><a href="#flume-收集日志到-kafka" class="headerlink" title="flume 收集日志到 kafka"></a>flume 收集日志到 kafka</h2><p>开启 zookeeper,需要在一直在服务器端实时运行，以守护进程运行</p><pre><code class="shell">/root/bigdata/kafka/bin/zookeeper-server-start.sh -daemon /root/bigdata/kafka/config/zookeeper.properties</code></pre><p>以及 kafka</p><pre><code>/root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties</code></pre><p>测试</p><pre><code class="shell"> #开启消息生产者/root/bigdata/kafka/bin/kafka-console-producer.sh --broker-list 192.168.19.137:9092 --sync --topic click-trace #开启消费者/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace#在生产者窗口输入任意内容测试</code></pre><p>修改原来收集日志的文件，添加 flume 收集日志行为到 kafka 的 source, channel, sink</p><pre><code>a1.sources = s1a1.sinks = k1 k2a1.channels = c1 c2a1.sources.s1.channels= c1 c2a1.sources.s1.type = execa1.sources.s1.command = tail -F /root/logs/userClick.loga1.sources.s1.interceptors=i1 i2a1.sources.s1.interceptors.i1.type=regex_filtera1.sources.r1.interceptors.i1.excludeEvents = falsea1.sources.s1.interceptors.i1.regex=\\{.*\\}a1.sources.s1.interceptors.i2.type=timestamp# channel1a1.channels.c1.type=memorya1.channels.c1.capacity=30000a1.channels.c1.transactionCapacity=1000# channel2a1.channels.c2.type=memorya1.channels.c2.capacity=30000a1.channels.c2.transactionCapacity=1000# k1a1.sinks.k1.type=hdfsa1.sinks.k1.channel=c1a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%da1.sinks.k1.hdfs.useLocalTimeStamp = truea1.sinks.k1.hdfs.fileType=DataStreama1.sinks.k1.hdfs.writeFormat=Texta1.sinks.k1.hdfs.rollInterval=0a1.sinks.k1.hdfs.rollSize=10240a1.sinks.k1.hdfs.rollCount=0a1.sinks.k1.hdfs.idleTimeout=60# k2a1.sinks.k2.channel=c2a1.sinks.k2.type=org.apache.flume.sink.kafka.KafkaSinka1.sinks.k2.kafka.bootstrap.servers=192.168.19.137:9092a1.sinks.k2.kafka.topic=click-tracea1.sinks.k2.kafka.batchSize=20a1.sinks.k2.kafka.producer.requiredAcks=1</code></pre><p>添加 supervisor 配置</p><pre><code>[program:kafka]command=/bin/bash /root/toutiao_project/scripts/start_kafka.shuser=rootautorestart=trueredirect_stderr=truestdout_logfile=/root/logs/kafka.logloglevel=infostopsignal=KILLstopasgroup=truekillasgroup=true</code></pre><p>用 supervisorctl 启动后测试</p><h2 id="实时召回集业务"><a href="#实时召回集业务" class="headerlink" title="实时召回集业务"></a>实时召回集业务</h2><p>实时召回基于相似度的文章推荐</p><p>创建 online 文件夹，建立在线实时处理程序</p><ul><li>目的：对用户日志进行处理，实时达到求出相似文章，放入用户召回集合中</li><li>步骤：<ul><li>1、配置 spark streaming 信息</li><li>2、读取点击行为日志数据，获取相似文章列表</li><li>3、过滤历史文章集合</li><li>4、存入召回结果以及历史记录结果</li></ul></li></ul><p>happybase 和 kafka 对接 spark streaming 的配置</p><pre><code class="python"># 增加spark online 启动配置class DefaultConfig(object):    """默认的一些配置信息    """    SPARK_ONLINE_CONFIG = (        ("spark.app.name", "onlineUpdate"),  # 设置启动的spark的app名称，没有提供，将随机产生一个名称        ("spark.master", "yarn"),        ("spark.executor.instances", 4)    )# 添加sparkstreaming启动对接kafka的配置from pyspark import SparkConffrom pyspark.sql import SparkSessionfrom pyspark import SparkContextfrom pyspark.streaming import StreamingContextfrom pyspark.streaming.kafka import KafkaUtilsfrom setting.default import DefaultConfigimport happybase#  用于读取hbase缓存结果配置pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)# 1、创建confconf = SparkConf()conf.setAll(DefaultConfig.SPARK_ONLINE_CONFIG)# 建立spark session以及spark streaming contextsc = SparkContext(conf=conf)# 创建Streaming Contextstream_c = StreamingContext(sc, 60)# KAFKA配置KAFKA_SERVER = "192.168.19.137:9092"# 基于内容召回配置，用于收集用户行为，获取相似文章实时推荐similar_kafkaParams = {"metadata.broker.list": DefaultConfig.KAFKA_SERVER, "group.id": 'similar'}SIMILAR_DS = KafkaUtils.createDirectStream(stream_c, ['click-trace'], similar_kafkaParams)</code></pre><p>主代码</p><pre><code class="python">class OnlineRecall(object):"""在线处理计算平台"""    def __init__(self):        pass        def _update_online_cb(self):            """            通过点击行为更新用户的cb召回表中的online召回结果            :return:            """            def foreachFunc(rdd):                    for data in rdd.collect():                    logger.info(                        "{}, INFO: rdd filter".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))                    # 判断日志行为类型，只处理点击流日志                    if data["param"]["action"] in ["click", "collect", "share"]:                        # print(data)                        with pool.connection() as conn:                            try:                                # 相似文章表                                sim_table = conn.table("article_similar")                                    # 根据用户点击流日志涉及文章找出与之最相似文章(基于内容的相似)，选取TOP-k相似的作为召回推荐结果                                _dic = sim_table.row(str(data["param"]["articleId"]).encode(), columns=[b"similar"])                                _srt = sorted(_dic.items(), key=lambda obj: obj[1], reverse=True)  # 按相似度排序                                if _srt:                                        topKSimIds = [int(i[0].split(b":")[1]) for i in _srt[:self.k]]                                        # 根据历史推荐集过滤，已经给用户推荐过的文章                                    history_table = conn.table("history_recall")                                        _history_data = history_table.cells(                                        b"reco:his:%s" % data["param"]["userId"].encode(),                                        b"channel:%d" % data["channelId"]                                    )                                    # print("_history_data: ", _history_data)                                        history = []                                    if len(data) &gt;= 2:                                        for l in data[:-1]:                                            history.extend(eval(l))                                    else:                                        history = []                                        # 根据历史召回记录，过滤召回结果                                    recall_list = list(set(topKSimIds) - set(history_data))                                        # print("recall_list: ", recall_list)                                    logger.info("{}, INFO: store user:{} cb_recall data".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), data["param"]["userId"]))                                    if recall_list:                                        # 如果有推荐结果集，那么将数据添加到cb_recall表中，同时记录到历史记录表中                                        logger.info(                                            "{}, INFO: get online-recall data".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))                                        recall_table = conn.table("cb_recall")                                            recall_table.put(                                            b"recall:user:%s" % data["param"]["userId"].encode(),                                            {b"online:%d" % data["channelId"]: str(recall_list).encode()}                                        )                                            history_table.put(                                            b"reco:his:%s" % data["param"]["userId"].encode(),                                            {b"channel:%d" % data["channelId"]: str(recall_list).encode()}                                        )                            except Exception as e:                                logger.info("{}, WARN: {}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))                            finally:                                conn.close()                SIMILAR_DS.map(lambda x: json.loads(x[1])).foreachRDD(foreachFunc)                return None</code></pre><h1 id="推荐业务流实现与-AB-测试"><a href="#推荐业务流实现与-AB-测试" class="headerlink" title="推荐业务流实现与 AB 测试"></a>推荐业务流实现与 AB 测试</h1><ul><li><p>逻辑流程</p><ul><li>1、后端发送推荐请求，实时推荐系统拿到请求参数<ul><li>grpc对接</li></ul></li><li>2、根据用户进行ABTest分流<ul><li>ABTest实验中心，用于进行分流任务，方便测试调整不同的模型上线</li></ul></li><li>3、推荐中心服务<ul><li>根据用户在ABTest分配的算法进行召回服务和排序服务读取返回结果</li></ul></li><li>4、返回推荐结果和埋点参数封装</li></ul></li></ul><p><img src="/.com//image-20230212160039176.png"></p><h2 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h2><ul><li>gRPC是由Google公司开源的高性能RPC框架。</li><li>gRPC支持多语言<br>gRPC原生使用C、Java、Go进行了三种实现，而C语言实现的版本进行封装后又支持C++、C#、Node、ObjC、 Python、Ruby、PHP等开发语言</li><li>gRPC支持多平台<br>支持的平台包括：Linux、Android、iOS、MacOS、Windows</li><li>gRPC的消息协议使用Google自家开源的Protocol Buffers协议机制（proto3） 序列化</li><li>gRPC的传输使用HTTP/2标准，支持双向流和连接多路复用</li></ul><h3 id="创建user-reco-proto协议文件"><a href="#创建user-reco-proto协议文件" class="headerlink" title="创建user_reco.proto协议文件"></a>创建user_reco.proto协议文件</h3><ul><li>用户刷新feed流接口<ul><li>user_recommend(User) returns (Track)</li></ul></li><li>文章相似(猜你喜欢)接口<ul><li>article_recommend(Article) returns(Similar)</li></ul></li></ul><p>编写grpc_tools.protoc</p><pre><code class="shell">syntax = "proto3";message User {    string user_id = 1;    int32 channel_id = 2;    int32 article_num = 3;    int64 time_stamp = 4;}// int32 ---&gt; int64 article_idmessage Article {    int64 article_id = 1;    int32 article_num = 2;}message param2 {    string click = 1;    string collect = 2;    string share = 3;    string read = 4;}message param1 {    int64 article_id = 1;    param2 params = 2;}message Track {    string exposure = 1;    repeated param1 recommends = 2;    int64 time_stamp = 3;}message Similar {    repeated int64 article_id = 1;}service UserRecommend {    // feed recommend    rpc user_recommend(User) returns (Track) {}    rpc article_recommend(Article) returns(Similar) {}}</code></pre><p>通过命令生成</p><pre><code class="shell">python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. user_reco.proto</code></pre><h3 id="服务端编写"><a href="#服务端编写" class="headerlink" title="服务端编写"></a>服务端编写</h3><pre><code class="python"># route.py# 基于用户推荐的rpc服务推荐# 定义指定的rpc服务输入输出参数格式protoRPC_SERVER = '192.168.19.137:9999'class UserRecommendServicer(user_reco_pb2_grpc.UserRecommendServicer):    """    对用户进行技术文章推荐    """    def user_recommend(self, request, context):        """        用户feed流推荐        :param request:        :param context:        :return:        """        # 选择C4组合        user_id = request.user_id        channel_id = request.channel_id        article_num = request.article_num        time_stamp = request.time_stamp        # 解析参数，并进行推荐中心推荐(暂时使用假数据替代)        class Temp(object):            user_id = -10            algo = 'test'            time_stamp = -10        tp = Temp()        tp.user_id = user_id        tp.time_stamp = time_stamp        _track = add_track([], tp)        # 解析返回参数到rpc结果参数        # 参数如下        # [       {"article_id": 1, "param": {"click": "", "collect": "", "share": "", 'detentionTime':''}},        #         {"article_id": 2, "param": {"click": "", "collect": "", "share": "", 'detentionTime':''}},        #         {"article_id": 3, "param": {"click": "", "collect": "", "share": "", 'detentionTime':''}},        #         {"article_id": 4, "param": {"click": "", "collect": "", "share": "", 'detentionTime':''}}        #     ]        # 第二个rpc参数        _param1 = []        for _ in _track['recommends']:            # param的封装            _params = user_reco_pb2.param2(click=_['param']['click'],                                           collect=_['param']['collect'],                                           share=_['param']['share'],                                           read=_['param']['read'])            _p2 = user_reco_pb2.param1(article_id=_['article_id'], params=_params)            _param1.append(_p2)        # param        return user_reco_pb2.Track(exposure=_track['param'], recommends=_param1, time_stamp=_track['timestamp'])#    def article_recommend(self, request, context):#        """#       文章相似推荐#       :param request:#       :param context:#       :return:#       """#       # 获取web参数#       article_id = request.article_id#       article_num = request.article_num##        # 进行文章相似推荐,调用推荐中心的文章相似#       _article_list = article_reco_list(article_id, article_num, 105)##       # rpc参数封装#       return user_reco_pb2.Similar(article_id=_article_list)def add_track(res, temp):    """    封装埋点参数    :param res: 推荐文章id列表    :param cb: 合并参数    :param rpc_param: rpc参数    :return: 埋点参数        文章列表参数        单文章参数    """    # 添加埋点参数    track = {}    # 准备曝光参数    # 全部字符串形式提供，在hive端不会解析问题    _exposure = {"action": "exposure", "userId": temp.user_id, "articleId": json.dumps(res),                 "algorithmCombine": temp.algo}    track['param'] = json.dumps(_exposure)    track['recommends'] = []    # 准备其它点击参数    for _id in res:        # 构造字典        _dic = {}        _dic['article_id'] = _id        _dic['param'] = {}        # 准备click参数        _p = {"action": "click", "userId": temp.user_id, "articleId": str(_id),              "algorithmCombine": temp.algo}        _dic['param']['click'] = json.dumps(_p)        # 准备collect参数        _p["action"] = 'collect'        _dic['param']['collect'] = json.dumps(_p)        # 准备share参数        _p["action"] = 'share'        _dic['param']['share'] = json.dumps(_p)        # 准备detentionTime参数        _p["action"] = 'read'        _dic['param']['read'] = json.dumps(_p)        track['recommends'].append(_dic)    track['timestamp'] = temp.time_stamp    return trackdef serve():    # 多线程服务器    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))    # 注册本地服务    user_reco_pb2_grpc.add_UserRecommendServicer_to_server(UserRecommendServicer(), server)    # 监听端口    server.add_insecure_port(DefaultConfig.RPC_SERVER)    # 开始接收请求进行服务    server.start()    # 使用 ctrl+c 可以退出服务    _ONE_DAY_IN_SECONDS = 60 * 60 * 24    try:        while True:            time.sleep(_ONE_DAY_IN_SECONDS)    except KeyboardInterrupt:        server.stop(0)if __name__ == '__main__':    # 测试grpc服务    serve()</code></pre><p>客户端测试代码：</p><pre><code class="python">import osimport sysBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))sys.path.insert(0, os.path.join(BASE_DIR))from abtest import user_reco_pb2_grpcfrom abtest import user_reco_pb2import grpcfrom setting.default import DefaultConfigimport timedef test():    article_dict = {}    # 构造传入数据    req_article = user_reco_pb2.User()    req_article.user_id = '1115629498121846784'    req_article.channel_id = 18    req_article.article_num = 10    req_article.time_stamp = int(time.time() * 1000)    # req_article.time_stamp = 1555573069870    with grpc.insecure_channel(DefaultConfig.RPC_SERVER) as rpc_cli:        print('''''')        try:            stub = user_reco_pb2_grpc.UserRecommendStub(rpc_cli)            resp = stub.user_recommend(req_article)        except Exception as e:            print(e)            article_dict['param'] = []        else:            # 解析返回结果参数            article_dict['exposure_param'] = resp.exposure            reco_arts = resp.recommends            reco_art_param = []            reco_list = []            for art in reco_arts:                reco_art_param.append({                    'artcle_id': art.article_id,                    'params': {                        'click': art.params.click,                        'collect': art.params.collect,                        'share': art.params.share,                        'read': art.params.read                    }                })                reco_list.append(art.article_id)            article_dict['param'] = reco_art_param            # 文章列表以及参数（曝光参数 以及 每篇文章的点击等参数）            print(reco_list, article_dict)if __name__ == '__main__':    test()</code></pre><h2 id="通过哈希分桶进行流量切分"><a href="#通过哈希分桶进行流量切分" class="headerlink" title="通过哈希分桶进行流量切分"></a>通过哈希分桶进行流量切分</h2><pre><code class="python">def feed_recommend(user_id, channel_id, article_num, time_stamp):    """    1、根据web提供的参数，进行分流    2、找到对应的算法组合之后，去推荐中心调用不同的召回和排序服务    3、进行埋点参数封装    :param user_id:用户id    :param article_num:推荐文章个数    :return: track:埋点参数结果: 参考上面埋点参数组合    """    #  产品前期推荐由于较少的点击行为，所以去做 用户冷启动 + 文章冷启动    # 用户冷启动：'推荐'频道：热门频道的召回+用户实时行为画像召回（在线的不保存画像）  'C2'组合    #            # 其它频道：热门召回 + 新文章召回   'C1'组合    # 定义返回参数的类    class TempParam(object):        user_id = -10        channel_id = -10        article_num = -10        time_stamp = -10        algo = ""    temp = TempParam()    temp.user_id = user_id    temp.channel_id = channel_id    temp.article_num = article_num    # 请求的时间戳大小    temp.time_stamp = time_stamp    # 先读取缓存数据redis+待推荐hbase结果    # 如果有返回并加上埋点参数    # 并且写入hbase 当前推荐时间戳用户（登录和匿名）的历史推荐文章列表    # 传入用户id为空的直接召回结果    if temp.user_id == "":        temp.algo = ""        return add_track([], temp)    # 进行分桶实现分流，制定不同的实验策略    bucket = hashlib.md5(user_id.encode()).hexdigest()[:1]    if bucket in RAParam.BYPASS[0]['Bucket']:        temp.algo = RAParam.BYPASS[0]['Strategy']    else:        temp.algo = RAParam.BYPASS[1]['Strategy']    # 推荐服务中心推荐结果(这里做测试)    track = add_track([], temp)    return track</code></pre><h2 id="推荐服务中心"><a href="#推荐服务中心" class="headerlink" title="推荐服务中心"></a>推荐服务中心</h2><ul><li>根据时间戳<ul><li>时间戳T小于HBASE历史推荐记录，则获取历史记录，返回该时间戳T上次的时间戳T-1</li><li>时间戳T大于HBASE历史推荐记录，则获取新推荐，则获取HBASE数据库中最近的一次时间戳<ul><li>如果有缓存，从缓存中拿，并且写入推荐历史表中</li><li>如果没有缓存，就进行一次指定算法组合的召回结果读取，排序，然后写入待推荐wait_recommend中，其中推荐出去的放入历史推荐表中</li></ul></li></ul></li></ul><h3 id="推荐中心业务逻辑"><a href="#推荐中心业务逻辑" class="headerlink" title="推荐中心业务逻辑"></a>推荐中心业务逻辑</h3><pre><code class="python">def feed_recommend_logic(self, temp):    """推荐流业务逻辑    :param temp:ABTest传入的业务请求参数    """    # 判断用请求的时间戳大小决定获取历史记录还是刷新推荐文章    try:        last_stamp = self.hbu.get_table_row('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(),                                            'channel:{}'.format(temp.channel_id).encode(), include_timestamp=True)[1]        logger.info("{} INFO get user_id:{} channel:{} history last_stamp".format(            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))    except Exception as e:        logger.warning("{} WARN read history recommend exception:{}".format(            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))        last_stamp = 0    # 如果小于，走一遍正常的推荐流程，缓存或者召回排序    logger.info("{} INFO history last_stamp:{},temp.time_stamp:{}".                format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), last_stamp, temp.time_stamp))    if last_stamp &lt; temp.time_stamp:        # 获取        res = redis_cache.get_reco_from_cache(temp, self.hbu)        # 如果没有，然后走一遍算法推荐 召回+排序，同时写入到hbase待推荐结果列表        if not res:            logger.info("{} INFO get user_id:{} channel:{} recall/sort data".                        format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))            res = self.user_reco_list(temp)        temp.time_stamp = int(last_stamp)        track = add_track(res, temp)    else:        logger.info("{} INFO read user_id:{} channel:{} history recommend data".format(            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))        try:            row = self.hbu.get_table_cells('history_recommend',                                      'reco:his:{}'.format(temp.user_id).encode(),                                      'channel:{}'.format(temp.channel_id).encode(),                                      timestamp=temp.time_stamp + 1,                                      include_timestamp=True)        except Exception as e:            logger.warning("{} WARN read history recommend exception:{}".format(                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))            row = []            res = []        # 1、如果没有历史数据，返回时间戳0以及结果空列表        # 2、如果历史数据只有一条，返回这一条历史数据以及时间戳正好为请求时间戳，修改时间戳为0        # 3、如果历史数据多条，返回最近一条历史数据，然后返回        if not row:            temp.time_stamp = 0            res = []        elif len(row) == 1 and row[0][1] == temp.time_stamp:            res = eval(row[0][0])            temp.time_stamp = 0        elif len(row) &gt;= 2:            res = eval(row[0][0])            temp.time_stamp = int(row[1][1])        res = list(map(int, res))        logger.info(            "{} INFO history:{}, {}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), res, temp.time_stamp))        track = add_track(res, temp)        # 曝光参数设置为空        track['param'] = ''    return track</code></pre><h3 id="获取用户召回结果"><a href="#获取用户召回结果" class="headerlink" title="获取用户召回结果"></a>获取用户召回结果</h3><pre><code class="python">  def user_reco_list(self, temp):        """        获取用户的召回结果进行推荐        :param temp:        :return:        """        reco_set = []        # 1、循环算法组合参数，遍历不同召回结果进行过滤        for _num in RAParam.COMBINE[temp.algo][1]:            # 进行每个召回结果的读取100,101,102,103,104            if _num == 103:                # 新文章召回读取                _res = self.recall_service.read_redis_new_article(temp.channel_id)                reco_set = list(set(reco_set).union(set(_res)))            elif _num == 104:                # 热门文章召回读取                _res = self.recall_service.read_redis_hot_article(temp.channel_id)                reco_set = list(set(reco_set).union(set(_res)))            else:                _res = self.recall_service.\                    read_hbase_recall_data(RAParam.RECALL[_num][0],                                           'recall:user:{}'.format(temp.user_id).encode(),                                           '{}:{}'.format(RAParam.RECALL[_num][1], temp.channel_id).encode())                # 进行合并某个协同过滤召回的结果                reco_set = list(set(reco_set).union(set(_res)))        # reco_set都是新推荐的结果，进行过滤        history_list = []        try:            data = self.hbu.get_table_cells('history_recommend',                                            'reco:his:{}'.format(temp.user_id).encode(),                                            'channel:{}'.format(temp.channel_id).encode())            for _ in data:                history_list = list(set(history_list).union(set(eval(_))))            logger.info("{} INFO filter user_id:{} channel:{} history data".format(                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))        except Exception as e:            logger.warning(                "{} WARN filter history article exception:{}".format(datetime.now().                                                                     strftime('%Y-%m-%d %H:%M:%S'), e))        # 如果0号频道有历史记录，也需要过滤        try:            data = self.hbu.get_table_cells('history_recommend',                                            'reco:his:{}'.format(temp.user_id).encode(),                                            'channel:{}'.format(0).encode())            for _ in data:                history_list = list(set(history_list).union(set(eval(_))))            logger.info("{} INFO filter user_id:{} channel:{} history data".format(                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, 0))        except Exception as e:            logger.warning(                "{} WARN filter history article exception:{}".format(datetime.now().                                                                     strftime('%Y-%m-%d %H:%M:%S'), e))        # 过滤操作 reco_set 与history_list进行过滤        reco_set = list(set(reco_set).difference(set(history_list)))        # 排序代码逻辑        # _sort_num = RAParam.COMBINE[temp.algo][2][0]        # reco_set = sort_dict[RAParam.SORT[_sort_num]](reco_set, temp, self.hbu)        # 如果没有内容，直接返回        if not reco_set:            return reco_set        else:            # 类型进行转换            reco_set = list(map(int, reco_set))            # 跟后端需要推荐的文章数量进行比对 article_num            # article_num &gt; reco_set            if len(reco_set) &lt;= temp.article_num:                res = reco_set            else:                # 之取出推荐出去的内容                res = reco_set[:temp.article_num]                # 剩下的推荐结果放入wait_recommend等待下次帅新的时候直接推荐                self.hbu.get_table_put('wait_recommend',                                       'reco:{}'.format(temp.user_id).encode(),                                       'channel:{}'.format(temp.channel_id).encode(),                                       str(reco_set[temp.article_num:]).encode(),                                       timestamp=temp.time_stamp)                logger.info(                    "{} INFO put user_id:{} channel:{} wait data".format(                        datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))            # 放入历史记录表当中            self.hbu.get_table_put('history_recommend',                                   'reco:his:{}'.format(temp.user_id).encode(),                                   'channel:{}'.format(temp.channel_id).encode(),                                   str(res).encode(),                                   timestamp=temp.time_stamp)            # 放入历史记录日志            logger.info(                "{} INFO store recall/sorted user_id:{} channel:{} history_recommend data".format(                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))            return res</code></pre><h3 id="在线预测"><a href="#在线预测" class="headerlink" title="在线预测"></a>在线预测</h3><p>除了对召回集进行排序以外，还可以在在线平台上使用离线训练好的点击率模型，得到高点击召回集。</p><pre><code class="python">def lr_sort_service(reco_set, temp, hbu):    """    排序返回推荐文章    :param reco_set:召回合并过滤后的结果    :param temp: 参数    :param hbu: Hbase工具    :return:    """    # 排序    # 1、读取用户特征中心特征    try:        user_feature = eval(hbu.get_table_row('ctr_feature_user',                                              '{}'.format(temp.user_id).encode(),                                              'channel:{}'.format(temp.channel_id).encode()))        logger.info("{} INFO get user user_id:{} channel:{} profile data".format(            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))    except Exception as e:        user_feature = []    if user_feature:        # 2、读取文章特征中心特征        result = []        for article_id in reco_set:            try:                article_feature = eval(hbu.get_table_row('ctr_feature_article',                                                         '{}'.format(article_id).encode(),                                                         'article:{}'.format(article_id).encode()))            except Exception as e:                article_feature = [0.0] * 111            f = []            # 第一个channel_id            f.extend([article_feature[0]])            # 第二个article_vector            f.extend(article_feature[11:])            # 第三个用户权重特征            f.extend(user_feature)            # 第四个文章权重特征            f.extend(article_feature[1:11])            vector = DenseVector(f)            result.append([temp.user_id, article_id, vector])        # 4、预测并进行排序是筛选        df = pd.DataFrame(result, columns=["user_id", "article_id", "features"])        test = SORT_SPARK.createDataFrame(df)        # 加载逻辑回归模型        model = LogisticRegressionModel.load("hdfs://hadoop-master:9000/headlines/models/LR.obj")        predict = model.transform(test)        def vector_to_double(row):            return float(row.article_id), float(row.probability[1])        res = predict.select(['article_id', 'probability']).rdd.map(vector_to_double).toDF(            ['article_id', 'probability']).sort('probability', ascending=False)        article_list = [i.article_id for i in res.collect()]        logger.info("{} INFO sorting user_id:{} recommend article".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'),                                                                          temp.user_id))        # 排序后，只将排名在前100个文章ID返回给用户推荐        if len(article_list) &gt; 100:            article_list = article_list[:100]        reco_set = list(map(int, article_list))    return reco_set</code></pre><h3 id="多路召回"><a href="#多路召回" class="headerlink" title="多路召回"></a>多路召回</h3><pre><code class="python">import osimport sysBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))sys.path.insert(0, os.path.join(BASE_DIR))from server import redis_clientfrom server import poolimport loggingfrom datetime import datetimefrom abtest.utils import HBaseUtilslogger = logging.getLogger('recommend')class ReadRecall(object):    """读取召回集的结果    """    def __init__(self):        self.client = redis_client        self.hbu = HBaseUtils(pool)    def read_hbase_recall_data(self, table_name, key_format, column_format):        """获取指定用户的对应频道的召回结果,在线画像召回，离线画像召回，离线协同召回        :return:        """        # 获取family对应的值        # 数据库中的键都是bytes类型，所以需要进行编码相加        # 读取召回结果多个版本合并        recall_list = []        try:            data = self.hbu.get_table_cells(table_name, key_format, column_format)            for _ in data:                recall_list = list(set(recall_list).union(set(eval(_))))            # 读取所有这个用户的在线推荐的版本，清空该频道的数据            # self.hbu.get_table_delete(table_name, key_format, column_format)        except Exception as e:            logger.warning(                "{} WARN read recall data exception:{}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))        return recall_list    def read_redis_new_data(self, channel_id):        """获取redis新文章结果        :param channel_id:        :return:        """        # format结果        logger.info("{} INFO read channel:{} new recommend data".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id))        _key = "ch:{}:new".format(channel_id)        try:            res = self.client.zrevrange(_key, 0, -1)        except redis.exceptions.ResponseError as e:            logger.warning("{} WARN read new article exception:{}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))            res = []        return list(map(int, res))    def read_redis_hot_data(self, channel_id):        """获取redis热门文章结果        :param channel_id:        :return:        """        # format结果        logger.info("{} INFO read channel:{} hot recommend data".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id))        _key = "ch:{}:hot".format(channel_id)        try:            _res = self.client.zrevrange(_key, 0, -1)        except redis.exceptions.ResponseError as e:            logger.warning("{} WARN read hot article exception:{}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))            _res = []        # 每次返回前50热门文章        res = list(map(int, _res))        if len(res) &gt; 50:            res = res[:50]        return res    def read_hbase_article_similar(self, table_name, key_format, article_num):        """获取文章相似结果        :param article_id: 文章id        :param article_num: 文章数量        :return:        """        # 第一种表结构方式测试：        # create 'article_similar', 'similar'        # put 'article_similar', '1', 'similar:1', 0.2        # put 'article_similar', '1', 'similar:2', 0.34        try:            _dic = self.hbu.get_table_row(table_name, key_format)            res = []            _srt = sorted(_dic.items(), key=lambda obj: obj[1], reverse=True)            if len(_srt) &gt; article_num:                _srt = _srt[:article_num]            for _ in _srt:                res.append(int(_[0].decode().split(':')[1]))        except Exception as e:            logger.error("{} ERROR read similar article exception: {}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))            res = []        return resif __name__ == '__main__':    rr = ReadRecall()    print(rr.read_hbase_article_similar('article_similar', b'13342', 10))    print(rr.read_hbase_recall_data('cb_recall', b'recall:user:1115629498121846784', b'als:18'))    # rr = ReadRecall()    # print(rr.read_redis_new_data(18))</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/.com//image-20230211193156286.png&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;环境配置&quot;&gt;&lt;a href=&quot;#环境配置&quot; class=&quot;headerlink&quot; title=&quot;环境配置&quot;&gt;&lt;/a&gt;环境配置&lt;/h1&gt;&lt;h2 id=&quot;启动-</summary>
      
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="BigData" scheme="http://example.com/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/day1_%E8%AF%BE%E5%A0%82%E7%BA%AA%E8%A6%81/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/day1_%E8%AF%BE%E5%A0%82%E7%BA%AA%E8%A6%81/</id>
    <published>2023-02-13T05:36:25.634Z</published>
    <updated>2021-03-07T14:38:33.601Z</updated>
    
    <content type="html"><![CDATA[<p>7天 基础</p><ul><li>推荐系统相关概念 基本算法</li><li>推荐算法<ul><li>原生python 实现推荐算法</li></ul></li><li>lambda架构 5天<ul><li>hadoop</li><li>hive hbase</li><li>spark core</li><li>spark sql spark streaming</li><li>案例 基于电商用户行为</li></ul></li></ul><p>7天 项目</p><h3 id="推荐概念"><a href="#推荐概念" class="headerlink" title="推荐概念"></a>推荐概念</h3><ul><li>信息过滤系统 解决 信息过载 用户需求不明确的问题<ul><li>利用一定的规则将物品排序 展示给需求不明确的用户</li></ul></li><li>推荐 搜索区别<ul><li>推荐个性化较强，用户被动的接受，希望能够提供持续的服务</li><li>搜索个性化弱，用户主动搜索，快速满足用户的需求</li></ul></li><li>推荐和 web项目区别<ul><li>构建稳定的信息流通通道</li><li>推荐 信息过滤系统</li><li>web 对结果有明确预期</li><li>推荐 结果是概率问题</li></ul></li></ul><h3 id="Lambda-架构介绍"><a href="#Lambda-架构介绍" class="headerlink" title="Lambda 架构介绍"></a>Lambda 架构介绍</h3><ul><li>离线计算和实时计算共同提供服务的问题</li><li>离线计算优缺点<ul><li>优点 能够处理的数据量可以很大 比如pb级别</li><li>缺点 速度比较慢 分钟级别的延迟</li></ul></li><li>实时计算<ul><li>优点 响应快 来一条数据处理一条 ms级别响应</li><li>缺点 处理的数据量小一些</li></ul></li><li>离线计算的框架<ul><li>hadoop hdfs mapreduce</li><li>spark core , spark sql</li><li>hive</li></ul></li><li>实时计算框架<ul><li>spark streaming</li><li>storm</li><li>flink</li></ul></li><li>消息中间件<ul><li>flume 日志采集系统</li><li>kafka 消息队列</li></ul></li><li>存储相关<ul><li>hbase nosql数据库</li><li>hive  sql操作hdfs数据</li></ul></li></ul><h3 id="推荐算法架构"><a href="#推荐算法架构" class="headerlink" title="推荐算法架构"></a>推荐算法架构</h3><ul><li><p>召回</p><ul><li><p>协同过滤  算相似度 memory base</p><p>​                  基于模型的 model base  矩阵分解</p></li><li><p>基于内容</p><ul><li>分词</li><li>词权重（提取关键词） tf-idf</li><li>word2Vec 词向量</li><li>物品向量</li></ul></li></ul></li><li><p>排序</p><ul><li>逻辑回归</li></ul></li><li><p>策略调整</p></li></ul><h3 id="推荐模型构建流程"><a href="#推荐模型构建流程" class="headerlink" title="推荐模型构建流程"></a>推荐模型构建流程</h3><ul><li><p>数据收集</p><ul><li>显性评分</li><li>隐性数据</li></ul></li><li><p>特征工程</p><ul><li>协同过滤：用户-物品 评分矩阵</li><li>基于内容：分词 tf-idf word2Vec</li></ul></li><li><p>训练模型</p><ul><li>协同过滤<ul><li>kNN</li><li>矩阵分解 梯度下降 ALS</li></ul></li></ul></li><li><p>评估、模型上线</p></li></ul><h3 id="协同过滤思路介绍"><a href="#协同过滤思路介绍" class="headerlink" title="协同过滤思路介绍"></a>协同过滤思路介绍</h3><ul><li>CF 物以类聚人以群分</li><li>做协同过滤的话 首先特征工程把 用户-物品的评分矩阵创建出来</li><li>基于用户的协同过滤<ul><li>给用户A 找到最相似的N个用户</li><li>N个用户消费过哪些物品</li><li>N个用户消费过的物品中-A用户消费过的就是推荐结果</li></ul></li><li>基于物品的协同过滤<ul><li>给物品A 找到最相似的N个物品</li><li>A用户消费记录 找到这些物品的相似物品</li><li>从这些相似物品先去重-A用户消费过的就是推荐结果</li></ul></li></ul><h3 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h3><ul><li>余弦相似度、皮尔逊相关系数<ul><li>向量的夹角余弦值</li><li>皮尔逊会对向量的每一个分量做中心化</li><li>余弦只考虑方向 不考虑向量长度</li><li>如果评分数据是连续的数值比较适合中余弦、皮尔逊计算相似度</li></ul></li><li>杰卡德相似度<ul><li>交集/并集</li><li>计算评分是0 1 布尔值的相似度</li></ul></li></ul><h3 id="使用不同相似度计算方式实现协同过滤"><a href="#使用不同相似度计算方式实现协同过滤" class="headerlink" title="使用不同相似度计算方式实现协同过滤"></a>使用不同相似度计算方式实现协同过滤</h3><ul><li><p>如果 买/没买 点/没点数据 0/1 适合使用杰卡德相似度</p><ul><li>from sklearn.metrics import jaccard_similarity_score</li><li>jaccard_similarity_score(df[‘Item A’],df[‘Item B’])</li><li>from sklearn.metrics.pairwise import pairwise_distances</li><li>user_similar = 1-pairwise_distances(df,metric=’jaccard’)</li></ul></li><li><p>一般用评分去做协同过滤 推荐使用皮尔逊相关系数</p><ul><li><p>评分预测</p></li><li><p>$$<br> pred(u,i)=\hat{r}<em>{ui}=\cfrac{\sum</em>{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}<br>$$</p></li></ul></li><li><p>基于用户和基于物品的协同过滤 严格上说，属于两种算法，实践中可以都做出来，对比效果，选择最靠谱的</p></li></ul><h3 id="协同过滤-基于模型的算法"><a href="#协同过滤-基于模型的算法" class="headerlink" title="协同过滤 基于模型的算法"></a>协同过滤 基于模型的算法</h3><ul><li>用户-物品矩阵比较稀疏的时候 直接去取物品向量 用户向量计算相似度 不太适合</li><li>基于模型的方法可以解决用户-物品矩阵比较稀疏的问题</li><li>矩阵分解<ul><li>把大的矩阵拆成两个小的 用户矩阵 物品矩阵  MXN 大矩阵   M X K    K X N  K&lt;&lt;M  k&lt;&lt;N</li><li>大矩阵 约等于 用户矩阵 乘 物品矩阵</li><li>使用als 交替最小二乘法来优化损失 spark ML  recommandation 包封装了als</li><li>优化之后的用户矩阵  取出用户向量</li><li>优化之后的物品矩阵  取出物品向量</li><li>用户向量点乘物品向量 得到最终评分的预测</li></ul></li></ul><h3 id="推荐系统的评价"><a href="#推荐系统的评价" class="headerlink" title="推荐系统的评价"></a>推荐系统的评价</h3><ul><li><p>准确率 覆盖率</p><ul><li>准确率<ul><li>学术  rmse mas   点击率预估 精准率</li><li>工程  A/B test 对比不同的算法 在线上运行对关键指标的影响 <ul><li>baseline 基准线 热门排行  </li><li>灰度发布</li></ul></li></ul></li></ul></li><li><p>EE</p><ul><li>Exploitation &amp; Exploration 探索与利用问题</li><li>Exploitation 利用用户的历史行为 只给他曾经看过的/消费过的相似物品</li><li>Exploration(探测 搜索) 发现用户的新兴趣</li><li>ee问题 实际上是矛盾</li></ul></li><li><p>评估手段</p><ul><li>离线评估和在线评估结合, 定期做问卷调查<ul><li>在线评估<ul><li>灰度发布 &amp; A/B测试</li></ul></li></ul></li></ul></li></ul><h3 id="推荐系统的冷启动"><a href="#推荐系统的冷启动" class="headerlink" title="推荐系统的冷启动"></a>推荐系统的冷启动</h3><ul><li>用户冷启动<ul><li>尽可能收集用户信息 构建用户画像（打标签）</li><li>根据用户的标签可以做人群聚类 用以有用户的行为做推荐</li><li>更多的使用流行度推荐</li></ul></li><li>物品冷启动<ul><li>物品打标签 构建物品画像</li><li>基于内容的推荐</li></ul></li><li>系统冷启动<ul><li>如果应用缺少用户行为数据-&gt;基于内容的推荐</li><li>随着用户行为积累的越来越多-&gt;协同过滤</li><li>基于内容和协同过滤共同工作</li></ul></li></ul><h3 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h3><ul><li>给物品打标签<ul><li>系统自己提取从业务数据库中提取（商品的标题副标题等）</li><li>用户填写</li><li>中文分词 利用算法计算词的权重 <ul><li>tf-idf  tf term frequency 词频  5/100 *2 <ul><li>idf 逆文档频率 log 10   文本库篇数/出现关键词的文章篇数</li><li>1000 10python  1000/10 100   2</li><li>1000/1000 log(1) = 0</li></ul></li><li>textrank</li></ul></li></ul></li><li>利用标签的文字 转换成词向量<ul><li>word2Vec 词-&gt;向量</li><li>用向量来表示语义</li><li>如果两个词的词向量相似度比较高 认为这两个词的语义相近</li></ul></li><li>利用词向量 构建物品的向量<ul><li>一个物品有N个关键词 每一个关键词对应一个词向量</li><li>求和（权重*词向量）/N</li><li>利用N个关键词的词向量获取物品向量</li></ul></li><li>通过物品向量计算相似度<ul><li>皮尔逊 相关系数 计算物品向量的相似度</li></ul></li></ul><h3 id="基于内容的推荐-基于物品的协同过滤-区别"><a href="#基于内容的推荐-基于物品的协同过滤-区别" class="headerlink" title="基于内容的推荐 基于物品的协同过滤 区别"></a>基于内容的推荐 基于物品的协同过滤 区别</h3><ul><li>content_base ：词向量-&gt;物品向量-&gt;计算相似度</li><li>item_based cf :user-item matrix-&gt;物品向量-&gt;相似度</li><li>content_base  item_based cf 不一样<ul><li>物品向量构建过程有区别</li><li>基于内容的推荐<ul><li>物品向量 文本（物品描述信息，系统填标签，用户填标签）</li></ul></li><li>基于物品的协同过滤<ul><li>用户对物品的评分矩阵 用户的行为数据中来</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;7天 基础&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推荐系统相关概念 基本算法&lt;/li&gt;
&lt;li&gt;推荐算法&lt;ul&gt;
&lt;li&gt;原生python 实现推荐算法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;lambda架构 5天&lt;ul&gt;
&lt;li&gt;hadoop&lt;/li&gt;
&lt;li&gt;hive hbas</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/08_%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%EF%BC%9AItem-Based%20CF%20%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/08_%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%EF%BC%9AItem-Based%20CF%20%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86/</id>
    <published>2023-02-13T05:36:25.632Z</published>
    <updated>2021-03-07T14:38:33.471Z</updated>
    
    <content type="html"><![CDATA[<h2 id="案例–算法实现：Item-Based-CF-预测评分"><a href="#案例–算法实现：Item-Based-CF-预测评分" class="headerlink" title="案例–算法实现：Item-Based CF 预测评分"></a>案例–算法实现：Item-Based CF 预测评分</h2><p><strong>评分预测公式：</strong><br>$$<br>pred(u,i)=\hat{r}<em>{ui}=\cfrac{\sum</em>{j\in I_{rated}}sim(i,j)*r_{uj}}{\sum_{j\in I_{rated}}sim(i,j)}<br>$$</p><h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h4><ul><li><p>实现评分预测方法：<code>predict</code></p><ul><li><p>方法说明：</p><p>利用原始评分矩阵、以及物品间两两相似度，预测指定用户对指定物品的评分。</p><p>如果无法预测，则抛出异常</p></li></ul><pre><code class="python"># ......def predict(uid, iid, ratings_matrix, item_similar):    '''    预测给定用户对给定物品的评分值    :param uid: 用户ID    :param iid: 物品ID    :param ratings_matrix: 用户-物品评分矩阵    :param item_similar: 物品两两相似度矩阵    :return: 预测的评分值    '''    print("开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分..."%(uid, iid))    # 1. 找出iid物品的相似物品    similar_items = item_similar[iid].drop([iid]).dropna()    # 相似物品筛选规则：正相关的物品    similar_items = similar_items.where(similar_items&gt;0).dropna()    if similar_items.empty is True:        raise Exception("物品&lt;%d&gt;没有相似的物品" %id)    # 2. 从iid物品的近邻相似物品中筛选出uid用户评分过的物品    ids = set(ratings_matrix.loc[uid].dropna().index)&amp;set(similar_items.index)    finally_similar_items = similar_items.loc[list(ids)]    # 3. 结合iid物品与其相似物品的相似度和uid用户对其相似物品的评分，预测uid对iid的评分    sum_up = 0    # 评分预测公式的分子部分的值    sum_down = 0    # 评分预测公式的分母部分的值    for sim_iid, similarity in finally_similar_items.iteritems():        # 近邻物品的评分数据        sim_item_rated_movies = ratings_matrix[sim_iid].dropna()        # uid用户对相似物品物品的评分        sim_item_rating_from_user = sim_item_rated_movies[uid]        # 计算分子的值        sum_up += similarity * sim_item_rating_from_user        # 计算分母的值        sum_down += similarity    # 计算预测的评分值并返回    predict_rating = sum_up/sum_down    print("预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f" % (uid, iid, predict_rating))    return round(predict_rating, 2)if __name__ == '__main__':    ratings_matrix = load_data(DATA_PATH)    item_similar = compute_pearson_similarity(ratings_matrix, based="item")    # 预测用户1对物品1的评分    predict(1, 1, ratings_matrix, item_similar)    # 预测用户1对物品2的评分    predict(1, 2, ratings_matrix, item_similar)</code></pre></li><li><p>实现预测全部评分方法：<code>predict_all</code></p><pre><code class="python"># ......def predict_all(uid, ratings_matrix, item_similar):    '''    预测全部评分    :param uid: 用户id    :param ratings_matrix: 用户-物品打分矩阵    :param item_similar: 物品两两间的相似度    :return: 生成器，逐个返回预测评分    '''    # 准备要预测的物品的id列表    item_ids = ratings_matrix.columns    # 逐个预测    for iid in item_ids:        try:            rating = predict(uid, iid, ratings_matrix, item_similar)        except Exception as e:            print(e)        else:            yield uid, iid, ratingif __name__ == '__main__':    ratings_matrix = load_data(DATA_PATH)    item_similar = compute_pearson_similarity(ratings_matrix, based="item")    for i in predict_all(1, ratings_matrix, item_similar):        pass</code></pre></li><li><p>添加过滤规则</p><pre><code class="python">def _predict_all(uid, item_ids,ratings_matrix, item_similar):    '''    预测全部评分    :param uid: 用户id    :param item_ids: 要预测物品id列表    :param ratings_matrix: 用户-物品打分矩阵    :param item_similar: 物品两两间的相似度    :return: 生成器，逐个返回预测评分    '''    # 逐个预测    for iid in item_ids:        try:            rating = predict(uid, iid, ratings_matrix, item_similar)        except Exception as e:            print(e)        else:            yield uid, iid, ratingdef predict_all(uid, ratings_matrix, item_similar, filter_rule=None):    '''    预测全部评分，并可根据条件进行前置过滤    :param uid: 用户ID    :param ratings_matrix: 用户-物品打分矩阵    :param item_similar: 物品两两间的相似度    :param filter_rule: 过滤规则，只能是四选一，否则将抛异常："unhot","rated",["unhot","rated"],None    :return: 生成器，逐个返回预测评分    '''    if not filter_rule:        item_ids = ratings_matrix.columns    elif isinstance(filter_rule, str) and filter_rule == "unhot":        '''过滤非热门电影'''        # 统计每部电影的评分数        count = ratings_matrix.count()        # 过滤出评分数高于10的电影，作为热门电影        item_ids = count.where(count&gt;10).dropna().index    elif isinstance(filter_rule, str) and filter_rule == "rated":        '''过滤用户评分过的电影'''        # 获取用户对所有电影的评分记录        user_ratings = ratings_matrix.loc[uid]        # 评分范围是1-5，小于6的都是评分过的，除此以外的都是没有评分的        _ = user_ratings&lt;6        item_ids = _.where(_==False).dropna().index    elif isinstance(filter_rule, list) and set(filter_rule) == set(["unhot", "rated"]):        '''过滤非热门和用户已经评分过的电影'''        count = ratings_matrix.count()        ids1 = count.where(count &gt; 10).dropna().index        user_ratings = ratings_matrix.loc[uid]        _ = user_ratings &lt; 6        ids2 = _.where(_ == False).dropna().index        # 取二者交集        item_ids = set(ids1)&amp;set(ids2)    else:        raise Exception("无效的过滤参数")    yield from _predict_all(uid, item_ids, ratings_matrix, item_similar)if __name__ == '__main__':    ratings_matrix = load_data(DATA_PATH)    item_similar = compute_pearson_similarity(ratings_matrix, based="item")    for result in predict_all(1, ratings_matrix, item_similar, filter_rule=["unhot", "rated"]):        print(result)</code></pre></li><li><p>为指定用户推荐TOP-N结果</p><pre><code class="python"># ......def top_k_rs_result(k):    ratings_matrix = load_data(DATA_PATH)    item_similar = compute_pearson_similarity(ratings_matrix, based="item")    results = predict_all(1, ratings_matrix, item_similar, filter_rule=["unhot", "rated"])    return sorted(results, key=lambda x: x[2], reverse=True)[:k]if __name__ == '__main__':    from pprint import pprint    result = top_k_rs_result(20)    pprint(result)</code></pre></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;案例–算法实现：Item-Based-CF-预测评分&quot;&gt;&lt;a href=&quot;#案例–算法实现：Item-Based-CF-预测评分&quot; class=&quot;headerlink&quot; title=&quot;案例–算法实现：Item-Based CF 预测评分&quot;&gt;&lt;/a&gt;案例–算法实现：</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/07_%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%EF%BC%9AUser-Based%20CF%20%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/07_%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%EF%BC%9AUser-Based%20CF%20%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86/</id>
    <published>2023-02-13T05:36:25.630Z</published>
    <updated>2021-03-07T14:38:33.407Z</updated>
    
    <content type="html"><![CDATA[<h2 id="案例–算法实现：User-Based-CF-预测评分"><a href="#案例–算法实现：User-Based-CF-预测评分" class="headerlink" title="案例–算法实现：User-Based CF 预测评分"></a>案例–算法实现：User-Based CF 预测评分</h2><p><strong>评分预测公式：</strong><br>$$<br>pred(u,i)=\hat{r}<em>{ui}=\cfrac{\sum</em>{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}<br>$$</p><h4 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h4><ul><li><p>实现评分预测方法：<code>predict</code></p><pre><code class="python"># ......def predict(uid, iid, ratings_matrix, user_similar):    '''    预测给定用户对给定物品的评分值    :param uid: 用户ID    :param iid: 物品ID    :param ratings_matrix: 用户-物品评分矩阵    :param user_similar: 用户两两相似度矩阵    :return: 预测的评分值    '''    print("开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分..."%(uid, iid))    # 1. 找出uid用户的相似用户    similar_users = user_similar[uid].drop([uid]).dropna()    # 相似用户筛选规则：正相关的用户，负数就抛弃掉    similar_users = similar_users.where(similar_users&gt;0).dropna()    if similar_users.empty is True:        raise Exception("用户&lt;%d&gt;没有相似的用户" % uid)    # 2. 从uid用户的近邻相似用户中筛选出对iid物品有评分记录的近邻用户，也就是消费过iid物品的用户都拿出来    ids = set(ratings_matrix[iid].dropna().index)&amp;set(similar_users.index)    finally_similar_users = similar_users.ix[list(ids)]    # 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分    sum_up = 0    # 评分预测公式的分子部分的值    sum_down = 0    # 评分预测公式的分母部分的值    for sim_uid, similarity in finally_similar_users.iteritems():        # 近邻用户的评分数据        sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna()        # 近邻用户对iid物品的评分        sim_user_rating_for_item = sim_user_rated_movies[iid]        # 计算分子的值        sum_up += similarity * sim_user_rating_for_item        # 计算分母的值        sum_down += similarity    # 计算预测的评分值并返回    predict_rating = sum_up/sum_down    print("预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f" % (uid, iid, predict_rating))    return round(predict_rating, 2)if __name__ == '__main__':    ratings_matrix = load_data(DATA_PATH)    user_similar = compute_pearson_similarity(ratings_matrix, based="user")    # 预测用户1对物品1的评分    predict(1, 1, ratings_matrix, user_similar)    # 预测用户1对物品2的评分    predict(1, 2, ratings_matrix, user_similar)</code></pre></li><li><p>实现预测全部评分方法：<code>predict_all</code></p><pre><code class="python"># ......def predict_all(uid, ratings_matrix, user_similar):    '''    预测全部评分    :param uid: 用户id    :param ratings_matrix: 用户-物品打分矩阵    :param user_similar: 用户两两间的相似度    :return: 生成器，逐个返回预测评分    '''    # 准备要预测的物品的id列表    item_ids = ratings_matrix.columns    # 逐个预测    for iid in item_ids:        try:            rating = predict(uid, iid, ratings_matrix, user_similar)        except Exception as e:            print(e)        else:            yield uid, iid, ratingif __name__ == '__main__':    ratings_matrix = load_data(DATA_PATH)    user_similar = compute_pearson_similarity(ratings_matrix, based="user")    for i in predict_all(1, ratings_matrix, user_similar):        pass</code></pre></li><li><p>添加过滤规则</p><pre><code class="python">def _predict_all(uid, item_ids, ratings_matrix, user_similar):    '''    预测全部评分    :param uid: 用户id    :param item_ids: 要预测的物品id列表    :param ratings_matrix: 用户-物品打分矩阵    :param user_similar: 用户两两间的相似度    :return: 生成器，逐个返回预测评分    '''    # 逐个预测    for iid in item_ids:        try:            rating = predict(uid, iid, ratings_matrix, user_similar)        except Exception as e:            print(e)        else:            yield uid, iid, ratingdef predict_all(uid, ratings_matrix, user_similar, filter_rule=None):    '''    预测全部评分，并可根据条件进行前置过滤    :param uid: 用户ID    :param ratings_matrix: 用户-物品打分矩阵    :param user_similar: 用户两两间的相似度    :param filter_rule: 过滤规则，只能是四选一，否则将抛异常："unhot","rated",["unhot","rated"],None    :return: 生成器，逐个返回预测评分    '''    if not filter_rule:        item_ids = ratings_matrix.columns    elif isinstance(filter_rule, str) and filter_rule == "unhot":        '''过滤非热门电影'''        # 统计每部电影的评分数        count = ratings_matrix.count()        # 过滤出评分数高于10的电影，作为热门电影        item_ids = count.where(count&gt;10).dropna().index    elif isinstance(filter_rule, str) and filter_rule == "rated":        '''过滤用户评分过的电影'''        # 获取用户对所有电影的评分记录        user_ratings = ratings_matrix.loc[uid]        # 评分范围是1-5，小于6的都是评分过的，除此以外的都是没有评分的        _ = user_ratings&lt;6        item_ids = _.where(_==False).dropna().index    elif isinstance(filter_rule, list) and set(filter_rule) == set(["unhot", "rated"]):        '''过滤非热门和用户已经评分过的电影'''        count = ratings_matrix.count()        ids1 = count.where(count &gt; 10).dropna().index        user_ratings = ratings_matrix.loc[uid]        _ = user_ratings &lt; 6        ids2 = _.where(_ == False).dropna().index        # 取二者交集        item_ids = set(ids1)&amp;set(ids2)    else:        raise Exception("无效的过滤参数")    yield from _predict_all(uid, item_ids, ratings_matrix, user_similar)if __name__ == '__main__':    ratings_matrix = load_data(DATA_PATH)    user_similar = compute_pearson_similarity(ratings_matrix, based="user")    for result in predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"]):        print(result)</code></pre></li><li><p>根据预测评分为指定用户进行TOP-N推荐：</p><pre><code class="python"># ......def top_k_rs_result(k):    ratings_matrix = load_data(DATA_PATH)    user_similar = compute_pearson_similarity(ratings_matrix, based="user")    results = predict_all(1, ratings_matrix, user_similar, filter_rule=["unhot", "rated"])    return sorted(results, key=lambda x: x[2], reverse=True)[:k]if __name__ == '__main__':    from pprint import pprint    result = top_k_rs_result(20)    pprint(result)</code></pre></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;案例–算法实现：User-Based-CF-预测评分&quot;&gt;&lt;a href=&quot;#案例–算法实现：User-Based-CF-预测评分&quot; class=&quot;headerlink&quot; title=&quot;案例–算法实现：User-Based CF 预测评分&quot;&gt;&lt;/a&gt;案例–算法实现：</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/06_%E6%A1%88%E4%BE%8B--%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/06_%E6%A1%88%E4%BE%8B--%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90/</id>
    <published>2023-02-13T05:36:25.619Z</published>
    <updated>2021-03-07T14:38:33.531Z</updated>
    
    <content type="html"><![CDATA[<h2 id="案例–基于协同过滤的电影推荐"><a href="#案例–基于协同过滤的电影推荐" class="headerlink" title="案例–基于协同过滤的电影推荐"></a>案例–基于协同过滤的电影推荐</h2><p>前面我们已经基本掌握了协同过滤推荐算法，以及其中两种最基本的实现方案：User-Based CF和Item-Based CF，下面我们将利用真是的数据来进行实战演练。</p><p>案例需求 演示效果</p><p>分析案例</p><h4 id="数据集下载"><a href="#数据集下载" class="headerlink" title="数据集下载"></a>数据集下载</h4><p><a href="https://grouplens.org/datasets/movielens/latest/">MovieLens Latest Datasets Small</a></p><p>建议下载<a href="http://files.grouplens.org/datasets/movielens/ml-latest-small.zip">ml-latest-small.zip</a>，数据量小，便于我们单机使用和运行</p><p>目标：根据<code>ml-latest-small/ratings.csv</code>（用户-电影评分数据），分别实现User-Based CF和Item-Based CF，并进行电影评分的预测，然后为用户实现电影推荐</p><h4 id="数据集加载"><a href="#数据集加载" class="headerlink" title="数据集加载"></a>数据集加载</h4><ul><li><p>加载ratings.csv，并转换为用户-电影评分矩阵</p><pre><code class="python">import osimport pandas as pdimport numpy as npDATA_PATH = "./datasets/ml-latest-small/ratings.csv"CACHE_DIR = "./datasets/cache/"def load_data(data_path):    '''    加载数据    :param data_path: 数据集路径    :param cache_path: 数据集缓存路径    :return: 用户-物品评分矩阵    '''    # 数据集缓存地址    cache_path = os.path.join(CACHE_DIR, "ratings_matrix.cache")    print("开始加载数据集...")    if os.path.exists(cache_path):    # 判断是否存在缓存文件        print("加载缓存中...")        ratings_matrix = pd.read_pickle(cache_path)        print("从缓存加载数据集完毕")    else:        print("加载新数据中...")        # 设置要加载的数据字段的类型        dtype = {"userId": np.int32, "movieId": np.int32, "rating": np.float32}        # 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分        ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(3))        # 透视表，将电影ID转换为列名称，转换成为一个User-Movie的评分矩阵        ratings_matrix = ratings.pivot_table(index=["userId"], columns=["movieId"], values="rating")        # 存入缓存文件        ratings_matrix.to_pickle(cache_path)        print("数据集加载完毕")    return  ratings_matrixif __name__ == '__main__':    ratings_matrix = load_data(DATA_PATH)    print(ratings_matrix)</code></pre></li></ul><h4 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h4><ul><li><p>计算用户或物品两两相似度：</p><pre><code class="python"># ......def compute_pearson_similarity(ratings_matrix, based="user"):    '''    计算皮尔逊相关系数    :param ratings_matrix: 用户-物品评分矩阵    :param based: "user" or "item"    :return: 相似度矩阵    '''    user_similarity_cache_path = os.path.join(CACHE_DIR, "user_similarity.cache")    item_similarity_cache_path = os.path.join(CACHE_DIR, "item_similarity.cache")    # 基于皮尔逊相关系数计算相似度    # 用户相似度    if based == "user":        if os.path.exists(user_similarity_cache_path):            print("正从缓存加载用户相似度矩阵")            similarity = pd.read_pickle(user_similarity_cache_path)        else:            print("开始计算用户相似度矩阵")            similarity = ratings_matrix.T.corr()            similarity.to_pickle(user_similarity_cache_path)    elif based == "item":        if os.path.exists(item_similarity_cache_path):            print("正从缓存加载物品相似度矩阵")            similarity = pd.read_pickle(item_similarity_cache_path)        else:            print("开始计算物品相似度矩阵")            similarity = ratings_matrix.corr()            similarity.to_pickle(item_similarity_cache_path)    else:        raise Exception("Unhandled 'based' Value: %s"%based)    print("相似度矩阵计算/加载完毕")    return similarityif __name__ == '__main__':    ratings_matrix = load_data(DATA_PATH)    user_similar = compute_pearson_similarity(ratings_matrix, based="user")    print(user_similar)    item_similar = compute_pearson_similarity(ratings_matrix, based="item")    print(item_similar)</code></pre></li></ul><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>以上实现，仅用于实验阶段，因为工业上、或生产环境中，数据量是远超过我们本例中使用的数据量的，而pandas是无法支撑起大批量数据的运算的（上T的），因此工业上通常会使用spark、mapReduce等分布式计算框架来实现，我们后面的课程中也是建立在此基础上进行实践的。</p><p>但是正如前面所说，推荐算法的思想和理念都是统一的，不论使用什么平台工具、有多大的数据体量，其背后的实现原理都是不变的。</p><p>所以在本节，大家要深刻去学习的是推荐算法的业务流程，以及在具体的业务场景中，如本例的电影推荐，如何实现出推荐算法，并产生推荐结果。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;案例–基于协同过滤的电影推荐&quot;&gt;&lt;a href=&quot;#案例–基于协同过滤的电影推荐&quot; class=&quot;headerlink&quot; title=&quot;案例–基于协同过滤的电影推荐&quot;&gt;&lt;/a&gt;案例–基于协同过滤的电影推荐&lt;/h2&gt;&lt;p&gt;前面我们已经基本掌握了协同过滤推荐算法，以及</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/05_%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%86%B7%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/05_%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%86%B7%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98/</id>
    <published>2023-02-13T05:36:25.617Z</published>
    <updated>2021-08-06T12:09:59.832Z</updated>
    
    <content type="html"><![CDATA[<h3 id="推荐系统的冷启动问题"><a href="#推荐系统的冷启动问题" class="headerlink" title="推荐系统的冷启动问题"></a>推荐系统的冷启动问题</h3><ul><li><p>推荐系统冷启动概念</p><ul><li>⽤户冷启动：如何为新⽤户做个性化推荐</li><li>物品冷启动：如何将新物品推荐给⽤户（协同过滤）</li><li>系统冷启动：⽤户冷启动+物品冷启动</li><li>本质是推荐系统依赖历史数据，没有历史数据⽆法预测⽤户偏好</li></ul></li><li><p>用户冷启动</p><ul><li><p>1.收集⽤户特征</p><ul><li><p>⽤户注册信息：性别、年龄、地域（不能太多，不然用户不填写）</p></li><li><p>设备信息：定位、⼿机型号、app列表</p></li><li><p>社交信息、推⼴素材、安装来源</p><p><img src="/.com//recommend4.png"></p></li></ul></li><li><p>2 引导用户填写兴趣</p><p><img src="/.com//recommend5.png"></p></li><li><p>3 使用其它站点的行为数据, 例如腾讯视频&amp;QQ音乐 今日头条&amp;抖音</p></li><li><p>4 新老用户推荐策略的差异</p><ul><li>新⽤户在冷启动阶段更倾向于热门排⾏榜，⽼⽤户会更加需要长尾推荐</li><li>Explore Exploit⼒度（新用户探索是保守）</li><li>使⽤单独的特征和模型预估–新旧用户分离</li></ul></li><li><p>举例 性别与电视剧的关系</p></li></ul><p><img src="/.com//firststart.png"></p><p><img src="/.com//firststart1.png"></p><p><strong>用户冷启动</strong></p><ul><li>尽可能收集用户信息 构建用户画像（打标签）</li><li>根据用户的标签可以做人群聚类 用以有用户的行为做推荐</li><li>更多的使用流行度推荐</li></ul></li><li><p>物品冷启动</p><ul><li>给物品打标签</li><li>利用物品的内容信息，将新物品先投放给曾经喜欢过和它内容相似的其他物品的用户。</li></ul><p><img src="/.com//firststart2.png"></p><p>物品冷启动</p><ul><li>物品打标签 构建物品画像</li><li>基于内容的推荐</li></ul></li><li><p>系统冷启动</p><ul><li>基于内容的推荐 系统早期</li><li>基于内容的推荐逐渐过渡到协同过滤</li><li>基于内容的推荐和协同过滤的推荐结果都计算出来 加权求和得到最终推荐结果</li></ul></li></ul><h3 id="基于内容的推荐"><a href="#基于内容的推荐" class="headerlink" title="基于内容的推荐"></a>基于内容的推荐</h3><ul><li>给物品打标签<ul><li>系统自己提取从业务数据库中提取（商品的标题副标题等）</li><li>用户填写</li><li>中文分词 利用算法计算词的权重 <ul><li>tf-idf  tf term frequency 词频  5/100 *2 <ul><li>idf 逆文档频率 log 10   文本库篇数/出现关键词的文章篇数</li><li>1000 10python  1000/10 100   2</li><li>1000/1000 log(1) = 0</li></ul></li><li>textrank</li></ul></li></ul></li><li>利用标签的文字 转换成词向量<ul><li>word2Vec 词-&gt;向量</li><li>用向量来表示语义</li><li>如果两个词的词向量相似度比较高 认为这两个词的语义相近</li></ul></li><li>利用词向量 构建物品的向量<ul><li>一个物品有N个关键词 每一个关键词对应一个词向量</li><li>求和（权重*词向量）/N</li><li>利用N个关键词的词向量获取物品向量</li></ul></li><li>通过物品向量计算相似度<ul><li>皮尔逊 相关系数 计算物品向量的相似度</li></ul></li></ul><h3 id="基于内容的推荐-基于物品的协同过滤-区别"><a href="#基于内容的推荐-基于物品的协同过滤-区别" class="headerlink" title="基于内容的推荐 基于物品的协同过滤 区别"></a>基于内容的推荐 基于物品的协同过滤 区别</h3><ul><li>content_base ：词向量-&gt;物品向量-&gt;计算相似度</li><li>item_based cf :user-item matrix-&gt;物品向量-&gt;相似度</li><li>content_base  item_based cf 不一样<ul><li>物品向量构建过程有区别</li><li>基于内容的推荐<ul><li>物品向量 文本（物品描述信息，系统填标签，用户填标签）</li></ul></li><li>基于物品的协同过滤<ul><li>用户对物品的评分矩阵 用户的行为数据中来</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;推荐系统的冷启动问题&quot;&gt;&lt;a href=&quot;#推荐系统的冷启动问题&quot; class=&quot;headerlink&quot; title=&quot;推荐系统的冷启动问题&quot;&gt;&lt;/a&gt;推荐系统的冷启动问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;推荐系统冷启动概念&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;⽤户冷启动：</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/04_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/04_%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/</id>
    <published>2023-02-13T05:36:25.616Z</published>
    <updated>2021-08-06T08:14:18.344Z</updated>
    
    <content type="html"><![CDATA[<h2 id="推荐算法"><a href="#推荐算法" class="headerlink" title="推荐算法"></a>推荐算法</h2><ul><li>推荐模型构建流程</li><li>推荐算法概述</li><li>基于协同过滤的推荐算法</li><li>协同过滤实现</li></ul><h3 id="一-推荐模型构建流程"><a href="#一-推荐模型构建流程" class="headerlink" title="一 推荐模型构建流程"></a>一 推荐模型构建流程</h3><p>Data(数据)-&gt;Features(特征)-&gt;ML Algorithm(机器学习算法)-&gt;Prediction Output(预测输出)</p><ul><li>数据清洗/数据处理</li></ul><p><img src="/.com//algorithm1.png"></p><ul><li>数据来源<ul><li>显性数据<ul><li>Rating 打分</li><li>Comments 评论/评价</li></ul></li><li>隐形数据<ul><li> Order history 历史订单</li><li> Cart events    加购物车</li><li> Page views    页面浏览</li><li> Click-thru      点击</li><li> Search log     搜索记录</li></ul></li></ul></li><li>数据量/数据能否满足要求</li><li>特征工程</li></ul><p><img src="/.com//algorithm2.png"></p><ul><li>从数据中筛选特征<ul><li>一个给定的商品，可能被拥有类似品味或需求的用户购买</li><li>使用用户行为数据描述商品</li></ul></li></ul><p><img src="/.com//algorithm3.png"></p><ul><li><p>用数据表示特征</p><ul><li><p>将所有用户行为合并在一起 ，形成一个user-item 矩阵</p><p><img src="/.com//algorithm4.png" alt="1545452707102"></p></li></ul></li><li><p>选择合适的算法</p></li></ul><p><img src="/.com//algorithm5.png"></p><ul><li><p>产生推荐结果</p><p><img src="/.com//algorithm6.png"></p></li></ul><h3 id="二-最经典的推荐算法：协同过滤推荐算法（Collaborative-Filtering）"><a href="#二-最经典的推荐算法：协同过滤推荐算法（Collaborative-Filtering）" class="headerlink" title="二 最经典的推荐算法：协同过滤推荐算法（Collaborative Filtering）"></a>二 最经典的推荐算法：协同过滤推荐算法（Collaborative Filtering）</h3><p>算法思想：<strong>物以类聚，人以群分</strong></p><p>基本的协同过滤推荐算法基于以下假设：</p><ul><li>“跟你喜好<strong>相似的人</strong>喜欢的东西你也很有可能喜欢” ：基于用户的协同过滤推荐（User-based CF）</li><li>“跟你喜欢的东西<strong>相似的东西</strong>你也很有可能喜欢 ”：基于物品的协同过滤推荐（Item-based CF）</li></ul><p>实现协同过滤推荐有以下几个步骤：</p><ol><li><p><strong>找出最相似的人或物品：TOP-N相似的人或物品</strong></p><p>通过计算两两的相似度来进行排序，即可找出TOP-N相似的人或物品</p></li><li><p><strong>根据相似的人或物品产生推荐结果</strong></p><p>利用TOP-N结果生成初始推荐结果，然后过滤掉用户已经有过记录的物品或明确表示不感兴趣的物品</p></li></ol><p>以下是一个简单的示例，数据集相当于一个用户对物品的购买记录表：打勾表示用户对物品的有购买记录</p><ul><li><p>关于相似度计算这里先用一个简单的思想：如有两个同学X和Y，X同学爱好[足球、篮球、乒乓球]，Y同学爱好[网球、足球、篮球、羽毛球]，可见他们的共同爱好有2个，那么他们的相似度可以用：2/3 * 2/4 = 1/3 ≈ 0.33 来表示。</p><p>User-Based CF</p><p><img src="/.com//%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%901.png"></p><p>Item-Based CF</p><p><img src="/.com//%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%901.png"></p><p>通过前面两个demo，相信大家应该已经对协同过滤推荐算法的设计与实现有了比较清晰的认识。</p></li></ul><h3 id="三-相似度计算-Similarity-Calculation"><a href="#三-相似度计算-Similarity-Calculation" class="headerlink" title="三 相似度计算(Similarity Calculation)"></a>三 相似度计算(Similarity Calculation)</h3><p><img src="/.com//similarity_calc1.png"></p><ul><li><p>相似度的计算方法</p><ul><li>数据分类<ul><li>实数值(物品评分情况)</li><li>布尔值(用户的行为 是否点击 是否收藏)</li></ul></li><li>欧氏距离, 是一个欧式空间下度量距离的方法. 两个物体, 都在同一个空间下表示为两个点, 假如叫做p,q, 分别都是n个坐标, 那么欧式距离就是衡量这两个点之间的距离. <strong>欧氏距离不适用于布尔向量之间</strong></li></ul><p><img src="/.com//od.png" alt="1546159024305"></p><p>​欧氏距离的值是一个非负数, 最大值正无穷, 通常计算相似度的结果希望是[-1,1]或[0,1]之间,一般可以使用</p><p>​如下转化公式:<img src="/.com//od2.png"></p><p>​</p></li><li><p>杰卡德相似度&amp;余弦相似度&amp;皮尔逊相关系数</p><ul><li>余弦相似度<ul><li>度量的是两个向量之间的夹角, 用夹角的余弦值来度量相似的情况</li><li>两个向量的夹角为0是,余弦值为1, 当夹角为90度是余弦值为0,为180度是余弦值为-1</li><li>余弦相似度在度量文本相似度, 用户相似度 物品相似度的时候较为常用</li><li>余弦相似度的特点, 与向量长度无关,余弦相似度计算要对向量长度归一化, 两个向量只要方向一致,无论程度强弱, 都可以视为’相似’</li></ul></li><li>皮尔逊相关系数Pearson<ul><li>实际上也是一种余弦相似度, 不过先对向量做了中心化, 向量a b 各自<strong>减去向量的均值后, 再计算余弦相似度</strong></li><li>皮尔逊相似度计算结果在-1,1之间 -1表示负相关, 1表示正相关</li><li>度量两个变量是不是同增同减</li><li>皮尔逊相关系数度量的是两个变量的变化趋势是否一致, <strong>不适合计算布尔值向量之间的相关度</strong></li></ul></li><li>杰卡德相似度 Jaccard<ul><li>两个集合的交集元素个数在并集中所占的比例, 非常适用于布尔向量表示</li><li>分子是两个布尔向量做点积计算, 得到的就是交集元素的个数</li><li>分母是两个布尔向量做或运算, 再求元素和</li></ul></li><li>余弦相似度适合用户评分数据(实数值), 杰卡德相似度适用于隐式反馈数据(0,1布尔值)(是否收藏,是否点击,是否加购物车)</li></ul></li></ul><p><img src="/.com//similarity_calc2.png"></p><ul><li><p>余弦相似度</p><p><img src="/.com//similarity_calc5.png"></p></li><li><p>皮尔逊相关系数</p></li><li><p><img src="/.com//image-20210806161414511.png" alt="image-20210806161414511"></p></li></ul><p><img src="/.com//similarity_calc3.png"></p><p><img src="/.com//similarity_calc4.png"></p><ul><li>计算出用户1和其它用户之间的相似度</li></ul><p><img src="/.com//similarity_calc6.png"></p><ul><li>按照相似度大小排序, K近邻 如K取4:</li></ul><p><img src="/.com//similarity_calc7.png"></p><ul><li>取出近邻用户的购物清单</li></ul><p><img src="/.com//similarity_calc8.png"></p><ul><li>去除用户1已经购买过的商品</li></ul><p><img src="/.com//similarity_calc9.png"></p><ul><li>在剩余的物品中根据评分排序</li></ul><p><img src="/.com//similarity_calc10.png"></p><ul><li>物品相似度计算<ul><li>余弦相似度对绝对值大小不敏感带来的问题<ul><li>用户A对两部电影评分分别是1分和2分, 用户B对同样这两部电影进行评分是4分,5分 用余弦相似度计算,两个用户的相似度达到0.98    </li><li>可以采用改进的余弦相似度, 先计算向量每个维度上的均值, 然后每个向量在各个维度上都减去均值后,在计算余弦相似度, 用调整的余弦相似度计算得到的相似度是-0.1</li></ul></li></ul></li></ul><p><img src="/.com//similarity_calc11.png"></p><ul><li>物品相似度计算案例</li></ul><p><img src="/.com//similarity_calc12.png"></p><ul><li>找出物品1的相似商品</li></ul><p><img src="/.com//similarity_calc13.png"></p><ul><li>选择最近似的物品</li></ul><p><img src="/.com//similarity_calc14.png"></p><ul><li>基于用户与物品的协同过滤比较</li></ul><p><img src="/.com//similarity_calc15.png"><img src="/.com//similarity_calc16.png"></p><h3 id="协同过滤推荐算法代码实现："><a href="#协同过滤推荐算法代码实现：" class="headerlink" title="协同过滤推荐算法代码实现："></a>协同过滤推荐算法代码实现：</h3><ul><li><p>构建数据集：</p><pre><code class="python">users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 构建数据集datasets = [    ["buy",None,"buy","buy",None],    ["buy",None,None,"buy","buy"],    ["buy",None,"buy",None,None],    [None,"buy",None,"buy","buy"],    ["buy","buy","buy",None,"buy"],]</code></pre></li><li><p>计算时我们数据通常都需要对数据进行处理，或者编码，目的是为了便于我们对数据进行运算处理，比如这里是比较简单的情形，我们用1、0分别来表示用户的是否购买过该物品，则我们的数据集其实应该是这样的：</p><pre><code class="python">users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [    [1,0,1,1,0],    [1,0,0,1,1],    [1,0,1,0,0],    [0,1,0,1,1],    [1,1,1,0,1],]import pandas as pddf = pd.DataFrame(datasets,                  columns=items,                  index=users)print(df)</code></pre></li><li><p>有了数据集，接下来我们就可以进行相似度的计算，不过对于相似度的计算其实是有很多专门的相似度计算方法的，比如余弦相似度、皮尔逊相关系数、杰卡德相似度等等。这里我们选择使用杰卡德相似系数[0,1]</p><pre><code class="python"># 直接计算某两项的杰卡德相似系数from sklearn.metrics import jaccard_score# 计算Item A 和Item B的相似度print(jaccard_score(df["Item A"], df["Item B"]))# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算用户间相似度，1减去杰卡德距离就是杰卡德相识度user_similar = 1 - pairwise_distances(df.values, metric="jaccard")user_similar = pd.DataFrame(user_similar, columns=users, index=users)print("用户之间的两两相似度：")print(user_similar)# 计算物品间相似度item_similar = 1 - pairwise_distances(df.T.values, metric="jaccard")item_similar = pd.DataFrame(item_similar, columns=items, index=items)print("物品之间的两两相似度：")print(item_similar)</code></pre><p>有了两两的相似度，接下来就可以筛选TOP-N相似结果，并进行推荐了</p></li><li><p>User-Based CF</p><pre><code class="python">import pandas as pdimport numpy as npfrom pprint import pprintusers = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [    [1,0,1,1,0],    [1,0,0,1,1],    [1,0,1,0,0],    [0,1,0,1,1],    [1,1,1,0,1],]df = pd.DataFrame(datasets,                  columns=items,                  index=users)# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算用户间相似度user_similar = 1 - pairwise_distances(df, metric="jaccard")user_similar = pd.DataFrame(user_similar, columns=users, index=users)print("用户之间的两两相似度：")print(user_similar)topN_users = {}# 遍历每一行数据for i in user_similar.index:    # 取出每一列数据，并删除自身，然后排序数据    _df = user_similar.loc[i].drop([i])    _df_sorted = _df.sort_values(ascending=False)    top2 = list(_df_sorted.index[:2])    topN_users[i] = top2print("Top2相似用户：")pprint(topN_users)rs_results = {}# 构建推荐结果for user, sim_users in topN_users.items():    rs_result = set()    # 存储推荐结果    for sim_user in sim_users:        # 构建初始的推荐结果,要去重         rs_result = rs_result.union(set(df.loc[sim_user].replace(0,np.nan).dropna().index))    # 过滤掉已经购买过的物品    rs_result -= set(df.loc[user].replace(0,np.nan).dropna().index)    rs_results[user] = rs_resultprint("最终推荐结果：")pprint(rs_results)</code></pre></li><li><p>Item-Based CF</p><pre><code class="python">import pandas as pdimport numpy as npfrom pprint import pprintusers = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [    [1,0,1,1,0],    [1,0,0,1,1],    [1,0,1,0,0],    [0,1,0,1,1],    [1,1,1,0,1],]df = pd.DataFrame(datasets,                  columns=items,                  index=users)# 计算所有的数据两两的杰卡德相似系数from sklearn.metrics.pairwise import pairwise_distances# 计算物品间相似度item_similar = 1 - pairwise_distances(df.T, metric="jaccard")item_similar = pd.DataFrame(item_similar, columns=items, index=items)print("物品之间的两两相似度：")print(item_similar)topN_items = {}# 遍历每一行数据for i in item_similar.index:    # 取出每一列数据，并删除自身，然后排序数据    _df = item_similar.loc[i].drop([i])    _df_sorted = _df.sort_values(ascending=False)    top2 = list(_df_sorted.index[:2])    topN_items[i] = top2print("Top2相似物品：")pprint(topN_items)rs_results = {}# 构建推荐结果for user in df.index:    # 遍历所有用户    rs_result = set()    for item in df.ix[user].replace(0,np.nan).dropna().index:   # 取出每个用户当前已购物品列表        # 根据每个物品找出最相似的TOP-N物品，构建初始推荐结果        rs_result = rs_result.union(topN_items[item])    # 过滤掉用户已购的物品    rs_result -= set(df.ix[user].replace(0,np.nan).dropna().index)    # 添加到结果中    rs_results[user] = rs_resultprint("最终推荐结果：")pprint(rs_results)</code></pre></li></ul><p><strong>关于协同过滤推荐算法使用的数据集</strong></p><p>在前面的demo中，我们只是使用用户对物品的一个购买记录，类似也可以是比如浏览点击记录、收听记录等等。这样数据我们预测的结果其实相当于是在预测用户是否对某物品感兴趣，对于喜好程度不能很好的预测。</p><p>因此在协同过滤推荐算法中其实会更多的利用用户对物品的“评分”数据来进行预测，通过评分数据集，我们可以预测用户对于他没有评分过的物品的评分。其实现原理和思想和都是一样的，只是使用的数据集是用户-物品的评分数据。</p><p><strong>关于用户-物品评分矩阵</strong></p><p>用户-物品的评分矩阵，根据评分矩阵的稀疏程度会有不同的解决方案</p><ul><li><p>稠密评分矩阵</p><p><img src="/.com//%E7%A8%A0%E5%AF%86%E8%AF%84%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86.png"></p></li><li><p>稀疏评分矩阵</p><p><img src="/.com//%E7%A8%80%E7%96%8F%E8%AF%84%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86.png"></p></li></ul><p>这里先介绍稠密评分矩阵的处理，稀疏矩阵的处理相对会复杂一些，我们到后面再来介绍。</p><h4 id="使用协同过滤推荐算法对用户进行评分预测"><a href="#使用协同过滤推荐算法对用户进行评分预测" class="headerlink" title="使用协同过滤推荐算法对用户进行评分预测"></a>使用协同过滤推荐算法对用户进行评分预测</h4><ul><li><p>数据集：<img src="/.com//%E7%A8%A0%E5%AF%86%E8%AF%84%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86.png"></p><p><strong>目的：预测用户1对物品E的评分</strong></p></li><li><p>构建数据集：注意这里构建评分数据时，对于缺失的部分我们需要保留为None，如果设置为0那么会被当作评分值为0去对待</p><pre><code class="python">users = ["User1", "User2", "User3", "User4", "User5"]items = ["Item A", "Item B", "Item C", "Item D", "Item E"]# 用户购买记录数据集datasets = [    [5,3,4,4,None],    [3,1,2,3,3],    [4,3,4,3,5],    [3,3,1,5,4],    [1,5,5,2,1],]</code></pre></li><li><p>计算相似度：对于评分数据这里我们采用皮尔逊相关系数[-1,1]来计算，-1表示强负相关，+1表示强正相关</p><blockquote><p>pandas中corr方法可直接用于计算皮尔逊相关系数</p></blockquote><pre><code class="python">df = pd.DataFrame(datasets,                  columns=items,                  index=users)print("用户之间的两两相似度：")# 直接计算皮尔逊相关系数# 默认是按列进行计算，因此如果计算用户间的相似度，当前需要进行转置user_similar = df.T.corr()print(user_similar.round(4))print("物品之间的两两相似度：")item_similar = df.corr()print(item_similar.round(4))</code></pre><pre><code># 运行结果：用户之间的两两相似度：        User1   User2   User3   User4   User5User1  1.0000  0.8528  0.7071  0.0000 -0.7921User2  0.8528  1.0000  0.4677  0.4900 -0.9001User3  0.7071  0.4677  1.0000 -0.1612 -0.4666User4  0.0000  0.4900 -0.1612  1.0000 -0.6415User5 -0.7921 -0.9001 -0.4666 -0.6415  1.0000物品之间的两两相似度：        Item A  Item B  Item C  Item D  Item EItem A  1.0000 -0.4767 -0.1231  0.5322  0.9695Item B -0.4767  1.0000  0.6455 -0.3101 -0.4781Item C -0.1231  0.6455  1.0000 -0.7206 -0.4276Item D  0.5322 -0.3101 -0.7206  1.0000  0.5817Item E  0.9695 -0.4781 -0.4276  0.5817  1.0000</code></pre><p>可以看到与用户1最相似的是用户2和用户3；与物品A最相似的物品分别是物品E和物品D。</p><p><strong>注意：</strong>我们在预测评分时，往往是通过与其有正相关的用户或物品进行预测，如果不存在正相关的情况，那么将无法做出预测。这一点尤其是在稀疏评分矩阵中尤为常见，因为稀疏评分矩阵中很难得出正相关系数。</p></li><li><p><strong>评分预测：</strong></p><p><strong>User-Based CF 评分预测：使用用户间的相似度进行预测</strong></p><p>关于评分预测的方法也有比较多的方案，下面介绍一种效果比较好的方案，该方案考虑了用户本身的评分评分以及近邻用户的加权平均相似度打分来进行预测：<br>$$<br>pred(u,i)=\hat{r}<em>{ui}=\cfrac{\sum</em>{v\in U}sim(u,v)*r_{vi}}{\sum_{v\in U}|sim(u,v)|}<br>$$<br>我们要预测用户1对物品E的评分，那么可以根据与用户1最近邻的用户2和用户3进行预测，计算如下：</p><p>​<br>$$<br>pred(u_1, i_5) =\cfrac{0.85<em>3+0.71</em>5}{0.85+0.71} = 3.91<br>$$<br><strong>最终预测出用户1对物品5的评分为3.91</strong></p><p><strong>Item-Based CF 评分预测：使用物品间的相似度进行预测</strong></p><p>这里利用物品相似度预测的计算同上，同样考虑了用户自身的平均打分因素，结合预测物品与相似物品的加权平均相似度打分进行来进行预测<br>$$<br>pred(u,i)=\hat{r}<em>{ui}=\cfrac{\sum</em>{j\in I_{rated}}sim(i,j)<em>r_{uj}}{\sum_{j\in I_{rated}}sim(i,j)}<br>$$<br>我们要预测用户1对物品E的评分，那么可以根据与物品E最近邻的物品A和物品D进行预测，用相似度乘以用户1分别对物品A和物品D的评分，也就是5和4，计算如下：<br>$$<br>pred(u_1, i_5) = \cfrac {0.97</em>5+0.58*4}{0.97+0.58} = 4.63<br>$$<br>对比可见，User-Based CF预测评分和Item-Based CF的评分结果也是存在差异的，因为严格意义上他们其实应当属于两种不同的推荐算法，各自在不同的领域不同场景下，都会比另一种的效果更佳，但具体哪一种更佳，必须经过合理的效果评估，因此在实现推荐系统时这两种算法往往都是需要去实现的，然后对产生的推荐效果进行评估分析选出更优方案。</p><p>假如有几万种商品，而大部分商品用户都没有购买过怎么办？</p></li></ul><h3 id="基于模型的方法"><a href="#基于模型的方法" class="headerlink" title="基于模型的方法"></a>基于模型的方法</h3><ul><li><p>思想</p><ul><li>通过机器学习算法，在数据中找出模式，并将用户与物品间的互动方式模式化</li><li>基于模型的协同过滤方式是构建协同过滤更高级的算法</li></ul></li><li><p>近邻模型的问题</p><ul><li>物品之间存在相关性, 信息量并不随着向量维度增加而线性增加</li><li>矩阵元素稀疏, 计算结果不稳定,增减一个向量维度, 导致近邻结果差异很大的情况存在</li></ul></li><li><p>算法分类</p><ul><li>基于图的模型</li><li><strong>基于矩阵分解的方法</strong>（可以应用于稀疏）</li></ul></li><li><p>基于图的模型</p><ul><li>基于邻域的模型看做基于图的模型的简单形式</li></ul><p><img src="/.com//graph1.png"></p><ul><li>原理<ul><li>将用户的行为数据表示为二分图</li><li>基于二分图为用户进行推荐</li><li>根据两个顶点之间的路径数、路径长度和经过的顶点数来评价两个顶点的相关性</li></ul></li></ul></li><li><p>基于矩阵分解的模型</p><ul><li><p>原理</p><ul><li><p>根据用户与物品的潜在表现，我们就可以预测用户对未评分的物品的喜爱程度</p></li><li><p>把原来的大矩阵, 近似分解成两个小矩阵的乘积, 在实际推荐计算时不再使用大矩阵, 而是使用分解得到的两个小矩阵  </p></li><li><p>用户-物品评分矩阵A是M X N维, 即一共有M个用户, n个物品 我们选一个很小的数 K (K&lt;&lt; M, K&lt;&lt;N)<br><strong>K可以理解成会影响到用户对物品评分的特征</strong>，如果有30个特征会影响到，至少为30</p></li><li><p>通过计算得到两个矩阵U V  U是M * K矩阵 , 矩阵V是 N * K</p><p>$U_{m<em>k} V^{T}_{n</em>k} 约等于 A_{m*n}$</p><p>类似这样的计算过程就是矩阵分解</p></li></ul></li><li><p>基于矩阵分解的方法</p><ul><li>ALS交替最小二乘<ul><li>ALS-WR(加权正则化交替最小二乘法): alternating-least-squares with weighted-λ –regularization</li><li>将用户(user)对商品(item)的评分矩阵分解为两个矩阵：一个是用户对商品隐含特征的偏好矩阵，另一个是商品所包含的隐含特征的矩阵。<strong>在这个矩阵分解的过程中，评分缺失项得到了填充</strong>，也就是说我们可以基于这个填充的评分来给用户做商品推荐了。</li></ul></li><li>SVD奇异值分解矩阵</li></ul></li></ul></li><li><p>ALS方法</p><p><img src="/.com//als1.png"></p><ul><li><p>怎么去算，后面我们有实例</p></li><li><p>ALS的矩阵分解算法常应用于推荐系统中，将用户(user)对商品(item)的评分矩阵，分解为用户对商品隐含特征的偏好矩阵，和商品在隐含特征上的映射矩阵。</p></li><li><p>与传统的矩阵分解SVD方法来分解矩阵R(R∈ℝm×n)不同的是，ALS(alternating least squares)希望找到两个低维矩阵，以 R̃ =XY 来逼近矩阵R，其中 ，X∈ℝm×d，Y∈ℝd×n，这样，将问题的复杂度由O(m*n)转换为O((m+n)*d)。</p></li><li><p>计算X和Y过程：首先用一个小于1的随机数初始化Y，并根据公式求X，此时就可以得到初始的XY矩阵了，根据平方差和得到的X，重新计算并覆盖Y，计算差平方和，反复进行以上两步的计算，直到差平方和小于一个预设的数，或者迭代次数满足要求则停止</p><p>怎么做预测，从M*K矩阵中抽出一个用户，也就是一行，再去K*N矩阵中拿出1列，想乘就得到了一个评分</p></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;推荐算法&quot;&gt;&lt;a href=&quot;#推荐算法&quot; class=&quot;headerlink&quot; title=&quot;推荐算法&quot;&gt;&lt;/a&gt;推荐算法&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;推荐模型构建流程&lt;/li&gt;
&lt;li&gt;推荐算法概述&lt;/li&gt;
&lt;li&gt;基于协同过滤的推荐算法&lt;/li&gt;
&lt;li&gt;协</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/03_%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/03_%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0/</id>
    <published>2023-02-13T05:36:25.609Z</published>
    <updated>2021-08-06T02:50:06.890Z</updated>
    
    <content type="html"><![CDATA[<h2 id="二-推荐系统评估"><a href="#二-推荐系统评估" class="headerlink" title="二 推荐系统评估"></a>二 推荐系统评估</h2><ul><li>好的推荐系统可以实现用户, 服务提供方, 内容提供方的共赢</li></ul><p><img src="/.com//recommend2.png"></p><ul><li><p>显示反馈和隐式反馈</p><table>  <tbody><tr>    <th></th>    <th>显式反馈</th>    <th>隐式反馈</th>  </tr>  <tr> <td> 例子 </td> <td> 电影/书籍评分  是否喜欢这个推荐 </td> <td> 播放/点击 评论 下载 购买 </td>  </tr>  <tr>    <td> 准确性 </td>    <td> 高 </td>    <td> 低 </td>  </tr>  <tr>    <td> 数量 </td>    <td> 少 </td>    <td> 多 </td>  </tr>  <tr>    <td> 获取成本 </td>    <td> 高 </td>    <td> 低 </td>  </tr></tbody></table></li><li><p>常用评估指标</p><p>• 准确性  • 信任度<br>• 满意度  • 实时性<br>• 覆盖率  • 鲁棒性<br>• 多样性  • 可扩展性<br>• 新颖性  • 商业⽬标<br>• 惊喜度  • ⽤户留存</p><ul><li>准确性 (理论角度) Netflix 美国录像带租赁<ul><li>评分预测<ul><li>RMSE   MAE  点击率预估 精准率</li></ul></li><li>topN推荐<ul><li>召回率 精准率</li></ul></li></ul></li><li>准确性 (业务角度)</li></ul><p><img src="/.com//recommend3.png"></p><ul><li>覆盖度<ul><li>信息熵 对于推荐越大越好，覆盖的商品越多</li><li>覆盖率</li></ul></li><li>多样性&amp;新颖性&amp;惊喜性<ul><li>多样性：推荐列表中两两物品的不相似性。（相似性如何度量？</li><li>新颖性：未曾关注的类别、作者；推荐结果的平均流⾏度</li><li>惊喜性：历史不相似（惊）但很满意（喜）</li><li>往往需要牺牲准确性</li><li>使⽤历史⾏为预测⽤户对某个物品的喜爱程度</li><li>系统过度强调实时性</li></ul></li><li>Exploitation &amp; Exploration 探索与利用问题<ul><li>Exploitation(开发 利用)：选择现在可能最佳的⽅案</li><li>Exploration(探测 搜索)：选择现在不确定的⼀些⽅案，但未来可能会有⾼收益的⽅案</li><li>在做两类决策的过程中，不断更新对所有决策的不确定性的认知，优化<br>长期的⽬标</li></ul></li><li>EE问题实践<ul><li>兴趣扩展: 相似话题, 搭配推荐</li><li>人群算法: userCF 用户聚类</li><li>平衡个性化推荐和热门推荐比例</li><li>随机丢弃用户行为历史</li><li>随机扰动模型参数</li></ul></li><li>EE可能带来的问题<ul><li>探索伤害用户体验, 可能导致用户流失</li><li>探索带来的长期收益(留存率)评估周期长, KPI压力大</li><li>如何平衡实时兴趣和长期兴趣</li><li>如何平衡短期产品体验和长期系统生态</li><li>如何平衡大众口味和小众需求</li></ul></li><li>评估方法<ul><li>问卷调查: 成本高</li><li>离线评估:<ul><li>只能在用户看到过的候选集上做评估, 且跟线上真实效果存在偏差</li><li>只能评估少数指标</li><li>速度快, 不损害用户体验</li></ul></li><li>在线评估: 灰度发布 &amp; A/B测试 50% 全量上线</li><li>实践: 离线评估和在线评估结合, 定期做问卷调查</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;二-推荐系统评估&quot;&gt;&lt;a href=&quot;#二-推荐系统评估&quot; class=&quot;headerlink&quot; title=&quot;二 推荐系统评估&quot;&gt;&lt;/a&gt;二 推荐系统评估&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;好的推荐系统可以实现用户, 服务提供方, 内容提供方的共赢&lt;/li&gt;
&lt;/ul&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/02_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/02_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1/</id>
    <published>2023-02-13T05:36:25.607Z</published>
    <updated>2023-02-11T03:06:47.844Z</updated>
    
    <content type="html"><![CDATA[<h2 id="二-推荐系统设计"><a href="#二-推荐系统设计" class="headerlink" title="二 推荐系统设计"></a>二 推荐系统设计</h2><h3 id="2-1-推荐系统要素"><a href="#2-1-推荐系统要素" class="headerlink" title="2.1 推荐系统要素"></a>2.1 推荐系统要素</h3><ul><li>UI 和 UE(前端界面)</li><li>数据 (Lambda架构)</li><li>业务知识</li><li>算法</li></ul><h3 id="2-2-推荐系统架构"><a href="#2-2-推荐系统架构" class="headerlink" title="2.2 推荐系统架构"></a>2.2 推荐系统架构</h3><ul><li><p>推荐系统整体架构</p><p><img src="/.com//img/%E6%8E%A8%E8%8D%90%E6%B5%81%E7%A8%8B.png"></p></li><li><p>大数据Lambda架构</p><ul><li><p>由Twitter工程师Nathan Marz(storm项目发起人)提出</p></li><li><p>Lambda系统架构提供了一个结合实时数据和Hadoop预先计算的数据环境和混合平台, 提供一个实时的数据视图</p></li><li><p>分层架构</p><ul><li>批处理层<ul><li>数据不可变, 可进行任何计算, 可水平扩展</li><li>高延迟  几分钟~几小时(计算量和数据量不同)</li><li>日志收集 Flume</li><li>分布式存储 Hadoop hdfs</li><li>分布式计算 Hadoop MapReduce &amp; spark</li><li>视图存储数据库<ul><li>nosql(HBase/Cassandra)</li><li>Redis/memcache</li><li>MySQL</li></ul></li></ul></li><li>实时处理层<ul><li>流式处理, 持续计算</li><li>存储和分析某个窗口期内的数据</li><li>最终正确性(Eventual accuracy)</li><li>实时数据收集 flume &amp; kafka</li><li>实时数据分析  spark streaming/storm/flink</li></ul></li><li>服务层<ul><li>支持随机读</li><li>需要在非常短的时间内返回结果</li><li>读取批处理层和实时处理层结果并对其归并</li></ul></li></ul></li><li><p>Lambda架构图</p><p><img src="/.com//lambda3.png"></p></li></ul></li><li><p>推荐算法架构</p><ul><li>召回阶段(海选)<ul><li>召回决定了最终推荐结果的天花板</li><li>常用算法:<ul><li>协同过滤(基于用户 基于物品的)</li><li>基于内容 (根据用户行为总结出自己的偏好 根据偏好 通过文本挖掘技术找到内容上相似的商品)</li><li>基于隐语义</li></ul></li></ul></li><li>排序阶段<ul><li>召回决定了最终推荐结果的天花板, 排序逼近这个极限, 决定了最终的推荐效果</li><li>CTR预估 (点击率预估 使用LR算法)  估计用户是否会点击某个商品 需要用户的点击数据</li></ul></li><li>策略调整</li></ul></li></ul><p><img src="/.com//img/recommend7.jpeg"></p><ul><li><p>推荐系统的整体架构</p><p><img src="/.com//rs%E5%9F%BA%E7%A1%80%E4%B8%9A%E5%8A%A1%E6%9E%B6%E6%9E%84.png"></p><p><img src="/.com//img/rs%E5%9F%BA%E7%A1%80%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84.png"></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;二-推荐系统设计&quot;&gt;&lt;a href=&quot;#二-推荐系统设计&quot; class=&quot;headerlink&quot; title=&quot;二 推荐系统设计&quot;&gt;&lt;/a&gt;二 推荐系统设计&lt;/h2&gt;&lt;h3 id=&quot;2-1-推荐系统要素&quot;&gt;&lt;a href=&quot;#2-1-推荐系统要素&quot; class=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%80%E4%BB%8B/"/>
    <id>http://example.com/2023/02/13/day01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D/01_%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%80%E4%BB%8B/</id>
    <published>2023-02-13T05:36:25.605Z</published>
    <updated>2023-02-11T03:05:42.664Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-推荐系统简介"><a href="#一-推荐系统简介" class="headerlink" title="一 推荐系统简介"></a>一 推荐系统简介</h2><p>​        个性化推荐(推荐系统)经历了多年的发展，已经成为互联网产品的标配，也是AI成功落地的分支之一，在电商(淘宝/京东)、资讯(今日头条/微博)、音乐(网易云音乐/QQ音乐)、短视频(抖音/快手)等热门应用中,推荐系统都是核心组件之一。</p><ul><li><p>推荐系统产生背景</p><ul><li>信息过载 &amp; 用户需求不明确<ul><li>分类⽬录（1990s）：覆盖少量热门⽹站。Hao123 Yahoo</li><li>搜索引擎（2000s）：通过搜索词明确需求。Google Baidu</li><li>推荐系统（2010s）：不需要⽤户提供明确的需求，通过分析⽤<br>户的历史⾏为给⽤户的兴趣进⾏建模，从⽽主动给⽤户推荐能<br>够满⾜他们兴趣和需求的信息。</li></ul></li></ul></li><li><p>什么是推荐系统</p><ul><li>没有明确需求的用户访问了我们的服务, 且服务的物品对用户构成了信息过载, 系统通过一定的规则对物品进行排序,并将排在前面的物品展示给用户,这样的系统就是推荐系统</li></ul></li><li><p>推荐系统 V.S. 搜索引擎</p><table>  <tbody><tr>    <th></th>    <th>搜索</th>    <th>推荐</th>  </tr>  <tr>    <td> 行为方式 </td>    <td> 主动 </td>    <td> 被动 </td>  </tr>  <tr>    <td> 意图 </td>    <td> 明确 </td>    <td> 模糊 </td>  </tr>  <tr>    <td> 个性化 </td>    <td> 弱 </td>    <td> 强 </td>  </tr>  <tr>    <td> 流量分布 </td>    <td> 马太效应 </td>    <td> 长尾效应 </td>  </tr>  <tr>    <td> 目标 </td>    <td> 快速满足  </td>    <td> 持续服务 </td>  </tr>  <tr>    <td> 评估指标 </td>    <td> 简明 </td>    <td> 复杂 </td>  </tr></tbody></table></li><li><p>推荐系统的作用</p><ul><li>高效连接用户和物品, 发现长尾商品</li><li>留住用户和内容生产者, 实现商业目标</li></ul></li><li><p>推荐系统的工作原理</p><ul><li><strong>社会化推荐</strong> 向朋友咨询, 社会化推荐, 让好友给自己推荐物品</li><li><strong>基于内容的推荐</strong> 打开搜索引擎, 输入自己喜欢的演员的名字, 然后看看返回结果中还有什么电影是自己没看过的</li><li><strong>基于流行度的推荐</strong> 查看票房排行榜, </li><li><strong>基于协同过滤的推荐</strong> 找到和自己历史兴趣相似的用户, 看看他们最近在看什么电影</li></ul></li><li><p>推荐系统的应用场景 feed 流 信息流 </p><p><img src="/.com//recommend1.png"></p></li><li><p>推荐系统和Web项目的区别</p><ul><li>稳定的信息流通系统 V.S. 通过信息过滤实现目标提升 <ul><li>web项目: 处理复杂逻辑 处理高并发 实现高可用 为用户提供稳定服务, 构建一个稳定的信息流通的服务</li><li>推荐系统: 追求指标增长, 留存率/阅读时间/GMV (Gross Merchandise Volume电商网站成交金额)/视频网站VV (Video View)</li></ul></li><li>确定 V.S. 不确定思维<ul><li>web项目: 对结果有确定预期</li><li>推荐系统: 结果是概率问题</li></ul></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一-推荐系统简介&quot;&gt;&lt;a href=&quot;#一-推荐系统简介&quot; class=&quot;headerlink&quot; title=&quot;一 推荐系统简介&quot;&gt;&lt;/a&gt;一 推荐系统简介&lt;/h2&gt;&lt;p&gt;​        个性化推荐(推荐系统)经历了多年的发展，已经成为互联网产品的标配，也是AI</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>大数据框架简介</title>
    <link href="http://example.com/2023/02/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B/"/>
    <id>http://example.com/2023/02/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B/</id>
    <published>2023-02-11T05:44:39.000Z</published>
    <updated>2023-02-13T05:41:54.067Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop：分布式系统框架"><a href="#Hadoop：分布式系统框架" class="headerlink" title="Hadoop：分布式系统框架"></a>Hadoop：分布式系统框架</h1><p><img src="/.com//image-20230201202112535.png"></p><h2 id="HDFS：分布式文件系统"><a href="#HDFS：分布式文件系统" class="headerlink" title="HDFS：分布式文件系统"></a>HDFS：分布式文件系统</h2><p><img src="/.com//image-20230201203516037.png"></p><blockquote><p>TIP: HDFS中的block大小为64MB，默认保存3份</p></blockquote><h2 id="YARN：分布式资源调度器"><a href="#YARN：分布式资源调度器" class="headerlink" title="YARN：分布式资源调度器"></a>YARN：分布式资源调度器</h2><p>核心思想：JobTracker和TaskTacker进行分离<br><img src="/.com//image-20230201203912441.png"></p><p>包含ResourceManager，ApplicationMaster，Nodemanager，Container，Client</p><h2 id="MapReduce：分布式计算框架"><a href="#MapReduce：分布式计算框架" class="headerlink" title="MapReduce：分布式计算框架"></a>MapReduce：分布式计算框架</h2><p><img src="/.com//image-20230201202455615.png"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hadoop：分布式系统框架&quot;&gt;&lt;a href=&quot;#Hadoop：分布式系统框架&quot; class=&quot;headerlink&quot; title=&quot;Hadoop：分布式系统框架&quot;&gt;&lt;/a&gt;Hadoop：分布式系统框架&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/.com//image</summary>
      
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="BigData" scheme="http://example.com/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>投资的心态</title>
    <link href="http://example.com/2022/06/23/%E6%8A%95%E8%B5%84%E7%9A%84%E5%BF%83%E6%80%81/"/>
    <id>http://example.com/2022/06/23/%E6%8A%95%E8%B5%84%E7%9A%84%E5%BF%83%E6%80%81/</id>
    <published>2022-06-23T08:50:34.000Z</published>
    <updated>2023-02-13T05:45:32.316Z</updated>
    
    <content type="html"><![CDATA[<p>普通投资者把精力花在学习投资知识和策略上，有什么意义呢？什么是健康的投资心态？从投资这件事上如何历练正确的积极的心态，以对我们的人生有所帮助？</p><h2 id="避免的陷阱"><a href="#避免的陷阱" class="headerlink" title="避免的陷阱"></a>避免的陷阱</h2><ol><li>时间沉没成本<br>我已经花了多少时间在这上面？这部分投入换算成金钱，增加到我的亏损上，最终结果应当还是盈余。</li><li>未获得的和已经失去的</li><li>短线的痛苦</li></ol><h2 id="正确的心态"><a href="#正确的心态" class="headerlink" title="正确的心态"></a>正确的心态</h2><ol><li>长远眼光</li><li>市场知识的学习</li><li>人性的学习</li><li>避免生活中的陷阱</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;普通投资者把精力花在学习投资知识和策略上，有什么意义呢？什么是健康的投资心态？从投资这件事上如何历练正确的积极的心态，以对我们的人生有所帮助？&lt;/p&gt;
&lt;h2 id=&quot;避免的陷阱&quot;&gt;&lt;a href=&quot;#避免的陷阱&quot; class=&quot;headerlink&quot; title=&quot;避免的</summary>
      
    
    
    
    <category term="freetime" scheme="http://example.com/categories/freetime/"/>
    
    
    <category term="life" scheme="http://example.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>GNN概述</title>
    <link href="http://example.com/2022/04/29/GNN%E6%A6%82%E8%BF%B0/"/>
    <id>http://example.com/2022/04/29/GNN%E6%A6%82%E8%BF%B0/</id>
    <published>2022-04-29T10:19:16.000Z</published>
    <updated>2022-10-25T04:57:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Graph-embedding"><a href="#Graph-embedding" class="headerlink" title="Graph embedding"></a>Graph embedding</h1><h2 id="2-1-deepwalk"><a href="#2-1-deepwalk" class="headerlink" title="2.1 deepwalk"></a>2.1 deepwalk</h2><p>从某个节点出发随机游走，得到若干个序列，用skip-gram计算每个节点的embedding信息</p><h2 id="2-2-LINE"><a href="#2-2-LINE" class="headerlink" title="2.2 LINE"></a>2.2 LINE</h2><p>一阶相似性：直接相连的两节点相似</p><p>二阶相似性：有大量共同邻居的两个不相连节点相似</p><p>相似度高的节点会在embedding后有更短的欧氏距离</p><p>一阶和二阶直接拼接为节点的向量</p><span id="more"></span><h2 id="2-3-Node2vec"><a href="#2-3-Node2vec" class="headerlink" title="2.3 Node2vec"></a>2.3 Node2vec</h2><p>有策略的随机游走</p><p>深度优先得到节点的社团相似性（上图）</p><p>广度优先得到节点的结构等价性（下图）</p><p><img src="/.com//image-20210818171416928.png"></p><h2 id="2-4-Struc2vec"><a href="#2-4-Struc2vec" class="headerlink" title="2.4 Struc2vec"></a>2.4 Struc2vec</h2><ol><li><p>计算节点n阶邻居度的序列，</p></li><li><p>求两序列的相似度</p></li><li><p>得到两节点的结构相似性</p></li></ol><h2 id="2-5-SDNE"><a href="#2-5-SDNE" class="headerlink" title="2.5 SDNE"></a>2.5 SDNE</h2><p>采用多个非线性层捕获一阶二阶相似性</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="/.com//image-20210818173631288.png"></p><h1 id="两类学习"><a href="#两类学习" class="headerlink" title="两类学习"></a>两类学习</h1><ul><li><strong>归纳学习（Inductive Learning）：</strong> 先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的<a href="https://www.zhihu.com/search?q=%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%A8%A1%E5%9E%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:34232818%7D">贝叶斯模型</a>。</li><li><strong>直推学习（Transductive Learning）：</strong> 先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。</li></ul><h1 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h1><ul><li><strong>GCN的基本思想：</strong> 把一个节点在图中的高纬度邻接信息降维到一个低维的向量表示。</li><li><strong>GCN的优点：</strong> 可以捕捉graph的全局信息，从而很好地表示node的特征。</li><li><strong>GCN的缺点：</strong> Transductive learning的方式，需要把所有节点都参与训练才能得到<a href="https://www.zhihu.com/search?q=node+embedding&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:74242097%7D">node embedding</a>，无法快速得到新node的embedding。</li></ul><p>常用字母对应:</p><table><thead><tr><th>名称</th><th>字母</th></tr></thead><tbody><tr><td>度矩阵（对角阵）</td><td>D</td></tr><tr><td>邻接矩阵（01阵）</td><td>A</td></tr></tbody></table><p>$$<br>\tilde{A}=A+I_N<br>$$<br>$$<br>\tilde{D}=D+I_N<br>$$<br>$$<br>H^{(l+1)}=\sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})<br>$$</p><p>$\tilde{A}H$聚合了节点自身和邻居的信息，$\tilde{D}^{-\frac{1}{2}}$起到归一化的作用。</p><p>归一化之后的$\tilde{A}$ 可记为：<br>$$<br>\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}<br>$$</p><h2 id="训练方式"><a href="#训练方式" class="headerlink" title="训练方式"></a>训练方式</h2><h3 id="端到端"><a href="#端到端" class="headerlink" title="端到端"></a>端到端</h3><p>$$<br>Z=f(X,A)=softmax(\hat{A},ReLU(\hat{A}HW^{(0)})W^{(1)})<br>$$</p><h2 id="嵌入-MLP"><a href="#嵌入-MLP" class="headerlink" title="嵌入+MLP"></a>嵌入+MLP</h2><h1 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h1><p><strong>思想：学习一个节点的信息是怎么通过其邻居节点的特征聚合而来的</strong><br>$$<br>h^k_{N(v)}=Agg({h^{k-1}_u},\forall u\in N(v))<br>$$</p><p>$$<br>h_v^k=\sigma(W^k \cdot Concat(h^{k-1}<em>v,h^k</em>{N(v)}) )<br>$$</p><p>$N(v)$表示节点 $v$ 的邻居节点集合，$Agg$()表示某个聚合函数。$k$代表层数。每一轮结束后进行归一化：<br>$$<br>h_v^k=h_v^k/\Vert h_v^k \Vert _2<br>$$</p><h2 id="邻居采样"><a href="#邻居采样" class="headerlink" title="邻居采样"></a>邻居采样</h2><pre><code class="python">if len(neighbors) &gt;= self.max_degree:    neighbors=np.random.choice(neighbors, self.max_degree, replace=False)# 数量不足将有放回的采样elif len(neighbors) &lt; self.max_degree:    neighbors=np.random.choice(neighbors, self.max_degree, replace=True)</code></pre><p><img src="/.com//image-20211209161858990.png"></p><h2 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h2><p>需要满足输入排列不变性，如</p><ul><li>Mean()</li><li><strong>GCN aggregator</strong> 比mean()多算一个自身特征</li><li>LSTM</li><li>Pooling</li></ul><p>$$<br>Agg_k^{pool}=max({ \sigma(Wh_u^k+b)})<br>$$</p><h2 id="Minibatch"><a href="#Minibatch" class="headerlink" title="Minibatch"></a>Minibatch</h2><p>即先采样把不需要用到的节点删除</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>让临近的节点拥有相似的表示，反之应该表示大不相同</p><p><img src="/.com//v2-9c473f5e242f8db158854d4e5e036b9c_r.jpg"></p><h1 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h1><p>$$<br>\alpha_{ij}=\frac{exp(LeakyReLu(\vec a^T[\boldsymbol W\vec h_i\Vert \boldsymbol W\vec h_j ]))}{\sum _{k \in N_i}exp(LeakyReLu(\vec a^T[\boldsymbol W\vec h_i\Vert \boldsymbol  W \vec h_k ]))}<br>$$</p><blockquote><p>相当于在聚合前计算了权重。如果$\vec a$是全1向量，那么这个权重就由邻居节点各自的特征值大小决定。<br>$$<br>\vec h_i^\prime=\sigma\left(\sum_{j\in N_i}\alpha_{ij} \boldsymbol W\vec h_j\right)<br>$$</p></blockquote><h2 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h2><p>$$<br>\vec h_i^\prime=\sigma\left(\sum_{j\in N_i}\alpha_{ij}^k \boldsymbol W^k\vec h_j\right)<br>$$</p><h1 id="PATCH-SAN"><a href="#PATCH-SAN" class="headerlink" title="PATCH-SAN"></a>PATCH-SAN</h1><h1 id="HAN"><a href="#HAN" class="headerlink" title="HAN"></a>HAN</h1><p>异构图：节点和边的类型之和大于2<br>元路径：按照某种类型pattern的路径（有向），不同的元路径揭示了不同的<strong>语义信息</strong><br>基于元路径的邻居：以某种元路径连接到节点i的所有节点<br>所以，在根据元路径邻居分配注意力的时候，可以依据不同类型的元路径分配权重<br><img src="/.com//image-20211223171241963.png"></p><h2 id="Node-level-Attention"><a href="#Node-level-Attention" class="headerlink" title="Node-level Attention"></a>Node-level Attention</h2><ol><li>根据某一类型元路径，异构图转为同构图</li><li>类似于GAT<br>$$<br> e_{ij}^\Phi=att_{node}(h^\prime_i,h^\prime_j;\Phi)<br>$$<br>$$<br>\alpha_{ij}^\Phi=softmax(e_{ij}^\Phi)<br>$$<br>$$<br>z_{i}^\Phi=\sigma(\sum_{j \in{\mathcal {N}}<em>i^\Phi}\alpha</em>{ij}^\Phi \cdot  h^\prime_j)<br>$$</li></ol><h2 id="Semantic-level-Attention"><a href="#Semantic-level-Attention" class="headerlink" title="Semantic-level Attention"></a>Semantic-level Attention</h2><p>$$<br> w_{\Phi_i}=\frac{1}{|{\mathcal{V}}|} \sum_{i\in{\mathcal{V}}}q^T\cdot\tanh(W\cdot z_i^\Phi+b)<br>$$<br>$$<br> \beta_{\Phi_i}=\frac{exp(w_{\Phi_i})}{\sum_{i=1}^Pexp(w_{\Phi_i})}<br>$$<br>$$<br> Z=\sum_{i=1}^{P}\beta_{\Phi_i}\cdot Z_{\Phi_i}<br>$$</p><h1 id="GTN"><a href="#GTN" class="headerlink" title="GTN"></a>GTN</h1><p><img src="/.com//image-20211223194418292.png"></p><ol><li>用$D^{-1}Q_1Q_2$归一化</li><li>为了保留图本身的性质，给点边关系集增加一个单位阵$I$这样和A相乘时还是A。</li><li>矩阵相乘后去掉对角线</li></ol><h1 id="metapath2vec"><a href="#metapath2vec" class="headerlink" title="metapath2vec"></a>metapath2vec</h1><p>元路径随机游走<br>对称的元路径效果更好<br>$$<br>p(v^{i+1}|v_t^i,{\mathcal{P}})=<br>\begin{cases}<br>\frac{1}{|N_{t+1}(v_t^i)|}, &amp; \text{$(v^{i+1},v^i)\in E,\phi(v^{i+1})=t+1$}\[2ex]<br>0, &amp; \text{other situation}\<br>\end{cases}<br>$$<br>上述公式表明，在游走到某节点$v_t$的情况下，下一节点在满足一下两个条件的节点中等概率选择：</p><ol><li>和节点$v_t$有边连接</li><li>是元路径规定的下一个节点的类型</li></ol><blockquote><p>获得随机游走序列之后，和deepwalk一样输入skip-gram模型训练</p></blockquote><h1 id="GATNE"><a href="#GATNE" class="headerlink" title="GATNE"></a>GATNE</h1><ol><li>相比metapath2vec，增加了边类型不同的情况</li><li>base-embedding+edge-embedding<br><img src="/.com//image-20211226200105763.png"><blockquote><p>用函数逼近器得到归纳学习的效果<br><img src="/.com//image-20211226204804890.png"></p></blockquote></li></ol><h1 id="BiNE"><a href="#BiNE" class="headerlink" title="BiNE"></a>BiNE</h1><p> 给显式和隐式关系分别赋予不同的权重。<br> 显式关系：直接的连边，A-B或A-A<br> 隐式关系：共同邻居，类似于元路径的A-B-A</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a href="https://www.bilibili.com/video/BV1K5411H7EQ?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">B站某大佬</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;Graph-embedding&quot;&gt;&lt;a href=&quot;#Graph-embedding&quot; class=&quot;headerlink&quot; title=&quot;Graph embedding&quot;&gt;&lt;/a&gt;Graph embedding&lt;/h1&gt;&lt;h2 id=&quot;2-1-deepwalk&quot;&gt;&lt;a href=&quot;#2-1-deepwalk&quot; class=&quot;headerlink&quot; title=&quot;2.1 deepwalk&quot;&gt;&lt;/a&gt;2.1 deepwalk&lt;/h2&gt;&lt;p&gt;从某个节点出发随机游走，得到若干个序列，用skip-gram计算每个节点的embedding信息&lt;/p&gt;
&lt;h2 id=&quot;2-2-LINE&quot;&gt;&lt;a href=&quot;#2-2-LINE&quot; class=&quot;headerlink&quot; title=&quot;2.2 LINE&quot;&gt;&lt;/a&gt;2.2 LINE&lt;/h2&gt;&lt;p&gt;一阶相似性：直接相连的两节点相似&lt;/p&gt;
&lt;p&gt;二阶相似性：有大量共同邻居的两个不相连节点相似&lt;/p&gt;
&lt;p&gt;相似度高的节点会在embedding后有更短的欧氏距离&lt;/p&gt;
&lt;p&gt;一阶和二阶直接拼接为节点的向量&lt;/p&gt;</summary>
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="Graph" scheme="http://example.com/tags/Graph/"/>
    
  </entry>
  
  <entry>
    <title>网络挖掘基础</title>
    <link href="http://example.com/2022/03/08/%E7%BD%91%E7%BB%9C%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80/"/>
    <id>http://example.com/2022/03/08/%E7%BD%91%E7%BB%9C%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80/</id>
    <published>2022-03-08T11:56:36.000Z</published>
    <updated>2022-10-09T08:43:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图的性质"><a href="#图的性质" class="headerlink" title="图的性质"></a>图的性质</h1><h2 id="度中心性"><a href="#度中心性" class="headerlink" title="度中心性"></a>度中心性</h2><p>$$<br>D_i=\frac{N_i}{n-1}<br>$$</p><p>$D_i$: i 结点的度中心性</p><p>$N_i$: i 结点的度</p><p>n: 结点数量</p><h2 id="特征向量中心性"><a href="#特征向量中心性" class="headerlink" title="特征向量中心性"></a>特征向量中心性</h2><p>邻接矩阵的最大特征值对应的<strong>特征向量</strong>，给出了每个节点重要程度的度量</p><h2 id="中介中心性"><a href="#中介中心性" class="headerlink" title="中介中心性"></a>中介中心性</h2><p>$$<br>Betweenness=\frac{经过该节点的最短路径数目}{(n-2)(n-1)}<br>$$</p><p>如果两节点出现两条最短路径，经过目标节点的路径权值减半。</p><h2 id="连接中心性"><a href="#连接中心性" class="headerlink" title="连接中心性"></a>连接中心性</h2><p>$$<br>Closeness=\frac{n-1}{节点到所有其他节点最短路径长度之和}<br>$$</p><h2 id="clustering-coefficient-x2F-集聚系数-簇系数"><a href="#clustering-coefficient-x2F-集聚系数-簇系数" class="headerlink" title="clustering coefficient/集聚系数(簇系数)"></a>clustering coefficient/集聚系数(簇系数)</h2><span id="more"></span><p>$$<br>C_i=\frac{2e_i}{k_i(k_i-1)}<br>$$</p><p>某结点的邻居结点之间的边占这些结点之间最大边数的比例</p><h2 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h2><ol><li><p>指定某个节点的PR值</p></li><li><p>计算仅由它指向的节点的PR值，迭代到收敛</p><p>公式中，$N$是总节点数，所有节点都有$1-d$的保留概率被访问，$d$是阻尼系数，根据经验取0.85，阻尼系数乘上所有指向$p_i$的节点的PR值除以那些节点的出度</p></li></ol><p>$$<br>\displaystyle{ PR(p_{i}) = \frac{1-d}{N}+d\sum <em>{p</em>{j}\in B(p_{i})}\frac{PR({p_{j}})}{L({p_{j}})} }<br>$$</p><h2 id="Hits"><a href="#Hits" class="headerlink" title="Hits"></a>Hits</h2><ol><li><p>每一个网页的权威值$\displaystyle{  auth(p) = 1  }$，枢纽值$\displaystyle{  hub(p) = 1  }$</p></li><li><p>$\displaystyle{ auth(p) = \sum_{q\in P_{t_{0}}}hub(q) }$，$\displaystyle{ hub(p) = \sum_{q\in P_{from}}auth(q) }$</p><p>权威值更新为所有指向它的节点的枢纽值求和，枢纽值更新它指向的节点的权威值求和。</p></li><li><p>分别对权威值和枢纽值归一化</p></li><li><p>重复2-3直到收敛</p></li></ol><h2 id="案例分析-Zachary空手道俱乐部"><a href="#案例分析-Zachary空手道俱乐部" class="headerlink" title="案例分析-Zachary空手道俱乐部"></a>案例分析-Zachary空手道俱乐部</h2><p><code>nx.draw(nx.Graph())</code> 需要调用pyplot</p><p><img src="/.com//image-20220307160753170.png"></p><h2 id="度分布"><a href="#度分布" class="headerlink" title="度分布"></a>度分布</h2><pre><code class="python">degree_freq = nx.degree_histogram(G)degrees = range(len(degree_freq))plt.plot(degrees, degree_freq, 'ro-')plt.title('degree distribution of Zachary')plt.xlabel('Degree')plt.ylabel('Frequency')plt.show()</code></pre><p><img src="/.com//image-20220308163459883.png"></p><h2 id="幂律分布-无标度网络"><a href="#幂律分布-无标度网络" class="headerlink" title="幂律分布(无标度网络)"></a>幂律分布(无标度网络)</h2><pre><code class="python">G = nx.barabasi_albert_graph(10000, 10)degree_freq = nx.degree_histogram(G)degrees = range(len(degree_freq))plt.loglog(degrees[1:], degree_freq[1:],'ro') plt.title('degree distribution &amp; power law')plt.xlabel('Degree')plt.ylabel('Frequency')plt.show()</code></pre><p><img src="/.com//image-20220307161721628.png"></p><h2 id="簇系数（集聚系数）"><a href="#簇系数（集聚系数）" class="headerlink" title="簇系数（集聚系数）"></a>簇系数（集聚系数）</h2><blockquote><p>clustering coefficient</p></blockquote><p>$$<br>C_i=\frac{2e_i}{k_i(k_i-1)}<br>$$</p><p>某结点的邻居结点之间的边占这些结点之间最大边数的比例</p><pre><code class="python">clustering_coefs = list(nx.clustering(G).values())degrees = [n for _, n in G.degree()]data = pd.DataFrame({'clustering_coefs': clustering_coefs,                      'degrees': degrees})g = sns.lmplot(x='degrees', y='clustering_coefs', data=data)plt.title('degree-clutering')plt.show()</code></pre><p><img src="/.com//image-20220307162103343.png"></p><p>阴影表示置信区间</p><h2 id="度度相关性"><a href="#度度相关性" class="headerlink" title="度度相关性"></a>度度相关性</h2><blockquote><p>节点的度和邻居节点度的平均值的关系</p></blockquote><pre><code class="python">degree1 = []degree2 = []for v in G:    degree1.append(G.degree(v))    degree2.append(np.mean([G.degree(x) for x in G.neighbors(v)]))    data = pd.DataFrame({'degree1': degree1,                      'degree2': degree2})g = sns.lmplot(x='degree1', y='degree2', data=data)plt.title('degree-degree')plt.show()</code></pre><p><img src="/.com//image-20220307162937610.png"></p><pre><code class="Python"># 同配性系数nx.degree_assortativity_coefficient(G)# 平均距离nx.average_shortest_path_length(G)# 距离nx.distance_measures.diameter(G)# 最大连通分量largest_cc = max(nx.connected_components(G), key=len)Gcc = DG.subgraph(largest_cc)# 有向图的强连通print(nx.is_strongly_connected(Gcc))# 有向图的出度、入读DG.out_degree()DG.in_degree()</code></pre><h1 id="经典网络模型"><a href="#经典网络模型" class="headerlink" title="经典网络模型"></a>经典网络模型</h1><h2 id="随机网络"><a href="#随机网络" class="headerlink" title="随机网络"></a>随机网络</h2><pre><code class="python">n = 43m = 130p = 130G_random = nx.gnp_random_graph(n, p)G_random = nx.gnm_random_graph(n, m)print(nx.info(G_random))fig, axes = plt.subplots(figsize=(16, 8), nrows=1, ncols=1)pos = nx.circular_layout(G_random)nx.draw(G_random, pos=pos, with_labels=True, ax=axes[0])axes[0].set_title('Random network')plt.show()# 保证网络连通的 p 值大小？def plot_random_network(n, p, ax):    G_random = nx.gnp_random_graph(n, p)    is_connected = nx.is_connected(G_random)    pos = nx.circular_layout(G_random)    nx.draw(G_random, pos=pos, ax=ax)    ax.set_title('%.4f %s' %                 (p, 'Connected' if is_connected else 'Not connected'))</code></pre><p><img src="/.com//image-20220308110330811.png"></p><h3 id="配置网络"><a href="#配置网络" class="headerlink" title="配置网络"></a>配置网络</h3><pre><code class="python"># degree likedegrees = [d for v, d in G_friends.degree()]G_configuration = nx.configuration_model(degrees)</code></pre><h2 id="自我中心网络"><a href="#自我中心网络" class="headerlink" title="自我中心网络"></a>自我中心网络</h2><pre><code class="python">def plot_ego_network(G, n, ax, title):        # ndoes_ego = list(nx.ego_graph(G, n, radius=1))    # G_ego = G.subgraph(ndoes_ego)    G_ego = nx.ego_graph(G, n, radius=1)        cc = nx.clustering(G)[n]        pos=nx.fruchterman_reingold_layout(G_ego, seed=1028)    d = dict(G_ego.degree)    nx.draw(G_ego, pos=pos, node_size=[v * 100 for v in d.values()], with_labels=True, ax=ax)    ax.set_title('%s, cc=%.3f' % (title, cc))n = 19fig, axes = plt.subplots(figsize=(18, 6), nrows=1, ncols=3)plt.suptitle('Ego network of node %d' % n)plot_ego_network(G_random, n, axes[0], 'Random network')# plot_ego_network(nx.Graph(G_configuration), 19, axes[1], 'Configuration network')# plot_ego_network(G_friends, 19, axes[2], 'Friends network')plt.show()</code></pre><p><img src="/.com//image-20220308112019045.png"></p><h2 id="小世界网络"><a href="#小世界网络" class="headerlink" title="小世界网络"></a>小世界网络</h2><pre><code class="python">fig, axes = plt.subplots(figsize=(18, 6), nrows=1, ncols=3)G_regular = nx.watts_strogatz_graph(n = 16, k = 4, p = 0.0)pos = nx.circular_layout(G_regular)nx.draw(G_regular, pos=pos, with_labels=True, ax=axes[0])axes[0].set_title('Regular network')G_sm = nx.watts_strogatz_graph(n = 16, k = 4, p = 0.3)pos = nx.circular_layout(G_sm)nx.draw(G_sm, pos=pos, with_labels=True, ax=axes[1])axes[1].set_title('Small-world network')G_random = nx.gnp_random_graph(n = 16, p = 4/16)pos = nx.circular_layout(G_random)nx.draw(G_random, pos=pos, with_labels=True, ax=axes[2])axes[2].set_title('Random network')plt.show()</code></pre><p><img src="/.com//image-20220308112408375.png"></p><p><img src="/.com//image-20220308122754592.png"></p><p><img src="/.com//image-20220308123031740.png"></p><p>规律：换掉的边越多，聚类系数越小，越接近随机网络；平均距离越小</p><h2 id="无标度网络"><a href="#无标度网络" class="headerlink" title="无标度网络"></a>无标度网络</h2><p>学习资料</p><ul><li><a href="https://www.ellipsix.net/blog/2012/02/scale-invariance-and-the-power-law.html">无标度与幂律分布</a>  </li><li><a href="https://adamwierman.com/wp-content/uploads/2020/09/2013-SIGMETRICS-heavytails.pdf">The Fundamentals of Heavy Tails</a></li><li><a href="https://www.bilibili.com/video/BV1PJ411T7ey?from=search&amp;seid=16649474684175316499&amp;spm_id_from=333.337.0.0">mandelbrot 集合</a></li></ul><h3 id="BA模型生成无标度网络"><a href="#BA模型生成无标度网络" class="headerlink" title="BA模型生成无标度网络"></a>BA模型生成无标度网络</h3><pre><code class="python">G0 = nx.star_graph(4)nx.draw(G0, with_labels=True)i = 4# 生成一个节点i += 1color_map = ['tab:blue'] * icolor_map.append('tab:red')G1 = G0.copy()G1.add_node(i)nx.draw(G1, node_color=color_map, with_labels=True)# 按照度的大小概率连接possible_neighbors = list(G0.nodes)degrees = [G0.degree(n) for n in possible_neighbors]neighbors_to_choose = [[n] * G0.degree(n) for n in possible_neighbors]neighbors_to_choose = [i for l in neighbors_to_choose for i in l]neighbors_to_choosej = random.choices(neighbors_to_choose)[0]print('new neighbor is %d' % j)G0.add_edge(i, j)nx.draw(G0, node_color=color_map, with_labels=True)# 重复def get_SF(G):        i = len(G.nodes())        possible_neighbors = list(G.nodes)    degrees = [G.degree(n) for n in possible_neighbors]        neighbors_to_choose = [[n] * G.degree(n) for n in possible_neighbors]    neighbors_to_choose = [i for l in neighbors_to_choose for i in l]    j = random.choices(neighbors_to_choose)[0]    G.add_edge(i, j)    G = nx.star_graph(4)N = 100for i in range(N):    get_SF(G)    nx.draw(G)    </code></pre><p><img src="/.com//image-20220308165729644.png" alt="image-20220308165729644"></p><h3 id="对无标度网络的攻击"><a href="#对无标度网络的攻击" class="headerlink" title="对无标度网络的攻击"></a>对无标度网络的攻击</h3><blockquote><p>随机移除节点 vs 蓄意移除节点</p></blockquote><p><img src="/.com//image-20220308170355504.png"></p><h1 id="社团结构"><a href="#社团结构" class="headerlink" title="社团结构"></a>社团结构</h1><h2 id="手动划分"><a href="#手动划分" class="headerlink" title="手动划分"></a>手动划分</h2><pre><code class="python">G = nx.Graph()nx.add_cycle(G, [0, 1, 2])nx.add_cycle(G, [3, 4, 5])G.add_edge(2, 3)pos = nx.fruchterman_reingold_layout(G)nx.draw(G, with_labels=True, pos=pos)partition1 = [    {0, 1},    {2, 3},    {4, 5},]partition_map1 = {}for idx, cluster_nodes in enumerate(partition1):    for node in cluster_nodes:        partition_map1[node] = idx        </code></pre><h2 id="随机划分"><a href="#随机划分" class="headerlink" title="随机划分"></a>随机划分</h2><pre><code class="python">random_nodes = random.sample(K.nodes, 17)random_partition = [set(random_nodes),set(K.nodes) - set(random_nodes)] # 差集</code></pre><h2 id="模块度-Modularity"><a href="#模块度-Modularity" class="headerlink" title="模块度 Modularity"></a>模块度 Modularity</h2><p>一个好的社团发掘算法：社团内部连边多，社团之间连边少。</p><p>For weighted undirected networks, as described in the text, we have<br>$$<br>\begin{equation}<br>    Q_w=\frac{1}{W}\sum_C \left(W_C-\frac{s_C^2}{4W}\right),<br>    \label{eq:wmodul}<br>\end{equation}<br>$$</p><p>where </p><ul><li>$W$ is the total weight of the links of the network,</li><li>$W_C$ the total weight of the internal links of cluster $C$, and</li><li>$s_C$ the total strength of the nodes of $C$.</li></ul><p>值得注意的是：</p><ul><li>最优的划分， 0 &lt; Q &lt; 1；</li><li>若把所有节点当成同一个社团，则 Q = 0；</li><li>若把每一个节点当成不同的社团，则 Q &lt; 0；</li></ul><pre><code class="python">import matplotlib.pyplot as pltfig = plt.figure(figsize=(12, 4))plt.subplot(1, 2, 1)nx.draw(G, with_labels=True, node_color=node_colors1, pos=pos)plt.title('partition1')plt.subplot(1, 2, 2)nx.draw(G, with_labels=True, node_color=node_colors2, pos=pos)plt.title('partition2')plt.show()</code></pre><p><img src="/.com//image-20220308180939432.png"></p><h3 id="贪婪模块度划分"><a href="#贪婪模块度划分" class="headerlink" title="贪婪模块度划分"></a>贪婪模块度划分</h3><pre><code class="python">from networkx.algorithms.community import greedy_modularity_communitiesoptimal_partition = greedy_modularity_communities(K)nx.community.quality.modularity(K, optimal_partition)</code></pre><h3 id="Girvan-Newman划分算法"><a href="#Girvan-Newman划分算法" class="headerlink" title="Girvan-Newman划分算法"></a>Girvan-Newman划分算法</h3><p>不断的删除网络中边介数（Betweenness）最大的边，直到网络中每个节点都是一个社团为止。</p><ol><li>计算每一条边的边介数； </li><li>删除边介数最大的边； </li><li>重新计算网络中剩下的边的边阶数；</li><li>重复(3)和(4)步骤，直到网络中的任一顶点作为一个社区为止。</li></ol><p><img src="/.com//image-20220308182129101.png"></p><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><p><a href="https://www.bilibili.com/video/BV1Az4y1Q7v5?from=search&amp;seid=14361555497668642316&amp;spm_id_from=333.337.0.0">Gephi 中文教程</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;图的性质&quot;&gt;&lt;a href=&quot;#图的性质&quot; class=&quot;headerlink&quot; title=&quot;图的性质&quot;&gt;&lt;/a&gt;图的性质&lt;/h1&gt;&lt;h2 id=&quot;度中心性&quot;&gt;&lt;a href=&quot;#度中心性&quot; class=&quot;headerlink&quot; title=&quot;度中心性&quot;&gt;&lt;/a&gt;度中心性&lt;/h2&gt;&lt;p&gt;$$&lt;br&gt;D_i=\frac{N_i}{n-1}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;$D_i$: i 结点的度中心性&lt;/p&gt;
&lt;p&gt;$N_i$: i 结点的度&lt;/p&gt;
&lt;p&gt;n: 结点数量&lt;/p&gt;
&lt;h2 id=&quot;特征向量中心性&quot;&gt;&lt;a href=&quot;#特征向量中心性&quot; class=&quot;headerlink&quot; title=&quot;特征向量中心性&quot;&gt;&lt;/a&gt;特征向量中心性&lt;/h2&gt;&lt;p&gt;邻接矩阵的最大特征值对应的&lt;strong&gt;特征向量&lt;/strong&gt;，给出了每个节点重要程度的度量&lt;/p&gt;
&lt;h2 id=&quot;中介中心性&quot;&gt;&lt;a href=&quot;#中介中心性&quot; class=&quot;headerlink&quot; title=&quot;中介中心性&quot;&gt;&lt;/a&gt;中介中心性&lt;/h2&gt;&lt;p&gt;$$&lt;br&gt;Betweenness=\frac{经过该节点的最短路径数目}{(n-2)(n-1)}&lt;br&gt;$$&lt;/p&gt;
&lt;p&gt;如果两节点出现两条最短路径，经过目标节点的路径权值减半。&lt;/p&gt;
&lt;h2 id=&quot;连接中心性&quot;&gt;&lt;a href=&quot;#连接中心性&quot; class=&quot;headerlink&quot; title=&quot;连接中心性&quot;&gt;&lt;/a&gt;连接中心性&lt;/h2&gt;&lt;p&gt;$$&lt;br&gt;Closeness=\frac{n-1}{节点到所有其他节点最短路径长度之和}&lt;br&gt;$$&lt;/p&gt;
&lt;h2 id=&quot;clustering-coefficient-x2F-集聚系数-簇系数&quot;&gt;&lt;a href=&quot;#clustering-coefficient-x2F-集聚系数-簇系数&quot; class=&quot;headerlink&quot; title=&quot;clustering coefficient/集聚系数(簇系数)&quot;&gt;&lt;/a&gt;clustering coefficient/集聚系数(簇系数)&lt;/h2&gt;</summary>
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="Graph" scheme="http://example.com/tags/Graph/"/>
    
  </entry>
  
  <entry>
    <title>智能合约漏洞综述</title>
    <link href="http://example.com/2021/12/13/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E6%BC%8F%E6%B4%9E%E7%BB%BC%E8%BF%B0/"/>
    <id>http://example.com/2021/12/13/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E6%BC%8F%E6%B4%9E%E7%BB%BC%E8%BF%B0/</id>
    <published>2021-12-13T04:02:57.000Z</published>
    <updated>2021-12-13T06:02:10.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>404的是本地pdf，自己搜论文</p></blockquote><h1 id="paper"><a href="#paper" class="headerlink" title="paper"></a>paper</h1><p><a href="35-%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf">35-智能合约安全漏洞研究综述</a><br>#张超-清华<br><a href="%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf">智能合约安全漏洞检测技术研究综述</a><br>#钱鹏-浙工商<br><a href="%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E5%AE%89%E5%85%A8%E7%BB%BC%E8%BF%B0_%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90.pdf">智能合约安全综述_漏洞分析</a></p><h2 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h2><ul><li>股权众筹</li><li>游戏</li><li>保险</li><li>供应链</li><li>物联网</li></ul><span id="more"></span><h2 id="事件"><a href="#事件" class="headerlink" title="事件"></a>事件</h2><table><thead><tr><th>安全事件</th><th>安全漏洞</th><th>时间</th><th>危害</th></tr></thead><tbody><tr><td>The DAO</td><td>重入</td><td>2016.6</td><td>$5000w hard-fork</td></tr><tr><td>Parity钱包被盗+冻结</td><td>代码注入+权限不当</td><td>2017.7</td><td>$3000w 冻结</td></tr><tr><td>BEC/SMT</td><td>整数溢出</td><td>2018.4</td><td>攻击者大量抛售后市值归零</td></tr><tr><td>KotET合约拒绝服务</td><td>拒绝服务</td><td>2016.2</td><td>攻击者获得永久王座</td></tr></tbody></table><h3 id="第一次parity"><a href="#第一次parity" class="headerlink" title="第一次parity"></a>第一次parity</h3><pre><code class="solidity">contract WalletLibrary {     address owner;     // called by constructor     function initWallet(address _owner) {         owner = _owner;         // ... more setup ...     }     function changeOwner(address _new_owner) external {         if (msg.sender == owner) {             owner = _new_owner;         }     }     function () payable {         // ... receive money, log events, ...     }     function withdraw(uint amount) external returns (bool success) {         if (msg.sender == owner) {             return owner.send(amount);         } else {             return false;         }     }}</code></pre><pre><code class="solidity">contract Wallet {    address _walletLibrary;    address owner;    function Wallet(address _owner) {        // replace the following line with “_walletLibrary = new WalletLibrary();”        // if you want to try to exploit this contract in Remix.        _walletLibrary = &lt;address of pre-deployed WalletLibrary&gt;;        _walletLibrary.delegatecall(bytes4(sha3("initWallet(address)")), _owner);    }    function withdraw(uint amount) returns (bool success) {        return _walletLibrary.delegatecall(bytes4(sha3("withdraw(uint)")), amount);    }    // fallback function gets called if no other function matches call    function () payable {        _walletLibrary.delegatecall(msg.data);    }}</code></pre><p>wallet 合约将所有行为交给walletLibrary代理执行。首先同名构造函数 <code>Wallet(address _owner)</code> 在合约创建时执行，钱包拥有者为合约创建者。fallback函数将递交所有的数据给代理库，这里埋下了隐患。当攻击者调用<code>Wallet.initWallet(attacker)</code>时，不匹配Wallet 中的任何显式函数，所以进入fallback函数，<code>initWallet(attacker)</code>将被转发给代理。<code>initWallet(attacker)</code>命中 walletLibrary 中的函数跳转表，于是在wallet 的上下文中再次执行<code>initWallet()</code>从而使自己成为了钱包拥有者。</p><blockquote><p>msg.sender: 交易发起者</p><p>msg.value: 转账金额，大于0将触发收款方的fallback函数</p><p>msg.data: 调用函数，也可以随便写，当不命中跳转表时会调用fallback</p></blockquote><p><a href="https://hackingdistributed.com/2017/07/22/deep-dive-parity-bug/">An In-Depth Look at the Parity Multisig Bug</a></p><blockquote><p>At the EVM level, a contract is just a single program that takes a variable-length binary blob of data as input and produces a variable-length binary blob of data as its output. A transaction in EVM provides an address and some data. If the address holds code, this data is used in a “<a href="https://en.wikipedia.org/wiki/Branch_table">jump table</a>” like structure in the beginning of the contract’s code, with some of the data (the “function selector”) indexing jumps to different parts of contract code using the standard encodings described in the <a href="https://github.com/ethereum/wiki/wiki/Ethereum-Contract-ABI">Ethereum Contract ABI specification</a>. Above, the function selector for calling a function called initWallet that takes an address as its argument is the mysterious looking bytes4(sha3(“initWallet(address)”)).</p></blockquote><h3 id="第二次parity"><a href="#第二次parity" class="headerlink" title="第二次parity"></a>第二次parity</h3><p>Parity 钱包提供了多签钱包的库合约。当库合约的函数被 delegatecall 调用时，它是运行在调用方（即：用户多签合约）的上下文里，像 m_numOwners 这样的变量都来自于用户多签合约的上下文。另外，为了能被用户合约调用，这些库合约的初始化函数都是 public 的。</p><p>库合约本质上也不过是另外一个智能合约，这次攻击调用使用的是库合约本身的上下文。这源于大家忽视了库本身的独立性，其中也包括Parity的维护者。他们应该在库合约部署后立刻调用一次<code>initWallet()</code>以<strong>锁定</strong>所有权，结果等了一百多天，被黑客抢先。</p><p>攻击流程:</p><ol><li>攻击者直接调用库合约的<code>initWallet()</code>把自己设置为库合约的 owner。</li><li>攻击者调用<code>kill()</code>函数，把库合约删除，所有依赖于这个库的钱包被冻结了。</li></ol><blockquote><p>这一攻击在上述的第一次parity中不会发生，因为没写kill函数！</p></blockquote><p>解决办法：构造函数获得库合约所有权后，把库合约的m_numOwners变量置1</p><h3 id="REX"><a href="#REX" class="headerlink" title="REX"></a>REX</h3><ul><li><a href="https://cointelegraph.com/news/hacker-spends-1k-to-win-over-110k-in-eos-betting-game-using-rex">REX</a></li></ul><h2 id="生命周期"><a href="#生命周期" class="headerlink" title="生命周期"></a>生命周期</h2><ol><li>开发</li><li>编译<ul><li>高级语言编写的合约源代码都将被编译为统一规范的<strong>字节码</strong>(bytecode)才能在EVM上运行</li><li>同时，合约还会生成相应的合约调用接口(Application Binary Interface, ABI), 该接口定义了合约所有可以被调用的<strong>外部函数</strong>及其<strong>参数列表</strong>。</li></ul></li><li>部署<br>由一笔合约部署交易完成：<ul><li>合约的data字段将被设置为合约部署字节码，交易的接收方被设置0</li><li>矿工在交易打包时将根据合约部署者的地址和交易次数(nonce)来生成一个新的地址，并将合约的字节码部署到该地址</li><li>将执行构造函数，包括合约变量初始化和同名函数</li></ul></li><li>调用<ul><li>交易调用(transaction call): 由普通地址发起一笔合约调用</li><li>消息调用(message call): 由某个合约发起的对另一个合约中函数的调用</li></ul></li><li>销毁<br>在合约编写时加入销毁功能的合约才能在满足设定条件后销毁。销毁只是意味着不能调用，其storage和的代码都可以查看</li></ol><h2 id="程序特性"><a href="#程序特性" class="headerlink" title="程序特性"></a>程序特性</h2><h3 id="gas机制"><a href="#gas机制" class="headerlink" title="gas机制"></a>gas机制</h3><p>以太坊设计了 Gas 机制来为合约的执行计算费用。每一个以太坊字节码指令都<strong>根据其运算的复杂程度</strong>被标记了对应的需要消耗的 Gas 花费。合约调用方在发起一次合约调用时, 需要指定本次合约程序执行最高能花费的 Gas 数量, 并为这个最大数量先行付费。数量不足即回滚且不退费，数量充足则退剩下的费用。 #gas</p><h3 id="异常传递机制"><a href="#异常传递机制" class="headerlink" title="异常传递机制"></a>异常传递机制</h3><p>#exception</p><ul><li><strong>内部函数调用：</strong> 对于本合约或者父合约的内部函数的调用，只需要在以太坊虚拟机执行时进行指令跳转。</li><li><strong>外部函数调用：</strong> 对于指定地址的外部合约函数的调用，需要使用 CALL 指令向外部合约发送消息，这种调用也称为<strong>低级别的调用</strong>。对于所有的低级别调用来说, 如果被调用函数执行过程中出错而抛出异常，则异常并不会被沿着函数调用栈进行传递, 而是仅使用布尔类型的返回值来表示函数调用是否正常完成。 #low-level-call<ul><li><strong>委托调用：</strong> 外部函数调用的一种，本质上是对当前合约函数注入外部代码 #delegate</li></ul></li></ul><h3 id="合约代码无法修改"><a href="#合约代码无法修改" class="headerlink" title="合约代码无法修改"></a>合约代码无法修改</h3><p>一旦部署，无法修改。保证了唯一性，但是也不能修漏洞了。</p><h3 id="全局状态与调用序列"><a href="#全局状态与调用序列" class="headerlink" title="全局状态与调用序列"></a>全局状态与调用序列</h3><p>每个合约都有一个长期的 Storage 存储区域，为合约存储可跨函数使用的全局变量状态。由于合约的<strong>多入口调用方式</strong>，因此在不同函数内部的变量关系、约束结果会随着全局变量进行跨函数的传递，最直接的体现特征就是特定功能或者漏洞的触发需要<strong>多笔交易组成的调用序列</strong>来完成。</p><h2 id="按来源归类漏洞"><a href="#按来源归类漏洞" class="headerlink" title="按来源归类漏洞"></a>按来源归类漏洞</h2><p><img src="/.com//image-20211211201858358.png"></p><h3 id="高级语言"><a href="#高级语言" class="headerlink" title="高级语言"></a>高级语言</h3><ol><li>变量覆盖<br>在受影响的版本的solidity语言中，其声明的数组或者结构体类型变量会被编译器误用为 Storage 类型的变量。从而, 对这些变量的操作将导致对智能合约 Storage 存储区的非法覆盖。</li><li>整数溢出</li><li>未检验返回值，针对 #low-level-call </li><li>任意地址写入，针对storage的键值对</li><li>拒绝服务</li><li>资产冻结</li><li>未初始化变量</li><li>影子变量<ol><li>子合约中声明了与父合约中相同的storage变量</li><li>全局storage变量和函数内部的局部变量重名</li></ol></li><li>权限控制</li></ol><h3 id="虚拟机"><a href="#虚拟机" class="headerlink" title="虚拟机"></a>虚拟机</h3><p>三类存储空间：Stack, Memory, Storage </p><table><thead><tr><th>name</th><th>grain</th><th>description</th></tr></thead><tbody><tr><td>stack</td><td>32字节</td><td>作为程序运行时的必要组件, 用于保存程序运行时的各种临时数据</td></tr><tr><td>memory</td><td>单字节</td><td>保存数组、字符串等较大的临时数据</td></tr><tr><td>storage</td><td>32字节</td><td>消耗大量gas，作为区块链状态的一部分永久存储</td></tr></tbody></table><p><img src="/.com//image-20211211193109887.png"></p><h4 id="重入"><a href="#重入" class="headerlink" title="重入"></a>重入</h4><p>有典型重入漏洞的代码如下所示：</p><pre><code class="solidity">    contract ERCToken{    ...    function withdraw(uint amount) public returns    (bool){        if (credit[msg.sender]&gt;= amount) {            msg.sender.call.value(amount)();            credit[msg.sender]-=amount;            emit Withdraw(msg.sender, amount);            return true;            }        return false;        }    }</code></pre><p>如果收到转账的地址是一个<strong>合约地址</strong>，便会<strong>触发收款方的 fallback 函数</strong>，默认fallback函数为<code>function() external payable{}</code>。该机制本意是让任何一个合约不用写代码也能获得收款的能力，但它可能被恶意的攻击者用于发起<strong>重入攻击</strong>。 Attack 是一个攻击合约，其代码如下所示:</p><pre><code class="solidity">    contract Attack{    ...    function hack() public {        erctoken.withdraw(1);        }    function() public payable { //fallback 函数        erctoken.withdraw(1);        }    }</code></pre><p>攻击者只需要通过 hack 函数调用受害合约的 withdraw 函 数，受害合约 withdraw 函数中<code>msg.sender.call.value(amount)()</code>语句执行时，会触发攻击合约中的 fallback 函数，整个攻击过程如图所示：<br><img src="/.com//image-20211211221424451.png"><br>解决办法：</p><ol><li>先<code>credit[msg.sender]-=amount</code>再转账</li><li>用<code>transfer()</code>限制gas</li></ol><h4 id="代码注入"><a href="#代码注入" class="headerlink" title="代码注入"></a>代码注入</h4><p>委托调用所使用的 DELEGATECALL 字节码, 允许合约在自己的上下文环境中执行一段其他合约的代码片段。 #delegate </p><h4 id="短地址攻击"><a href="#短地址攻击" class="headerlink" title="短地址攻击"></a>短地址攻击</h4><p>是指攻击者通过构造末尾为零的地址进行合约调用, 并在调用参数中故意将地址末尾的零舍去, 从而利用虚拟机对于数据的自动补全机制来将<strong>第二个参数进行移位放大</strong>。而第二个参数往往是转账金额。</p><h4 id="不一致性攻击"><a href="#不一致性攻击" class="headerlink" title="不一致性攻击"></a>不一致性攻击</h4><h3 id="区块链"><a href="#区块链" class="headerlink" title="区块链"></a>区块链</h3><h4 id="时间戳依赖"><a href="#时间戳依赖" class="headerlink" title="时间戳依赖"></a>时间戳依赖</h4><p>区块时间戳是指当前的合约调用交易所属的区块被打包的时间戳。区块时间戳是可以被矿工在一 定的取值范围内操纵的，大约能有900秒的范围波动。</p><h4 id="条件竞争"><a href="#条件竞争" class="headerlink" title="条件竞争"></a>条件竞争</h4><p>仅仅通过交易顺序来作为决策条件是有问题的。例如, 某个悬赏合约承诺给第一个提交答案的账户奖励。在第一个人提交答案，但还没被打包时，攻击者可以在交易池中找到这个答案并通过<strong>提高汽油费</strong>来让自己“抄”的答案优先被打包。</p><h4 id="随机性不足"><a href="#随机性不足" class="headerlink" title="随机性不足"></a>随机性不足</h4><p>由于对于攻击合约和受害合约的调用都来自于攻击者发起的同一笔交易, 自然处在同一个区块中, 因此攻击合约中可以读取到和受害合约中使用的全部区块变量, 从而预测受害合约中的随机数。</p><h2 id="漏洞挖掘方法"><a href="#漏洞挖掘方法" class="headerlink" title="漏洞挖掘方法"></a>漏洞挖掘方法</h2><p><img src="/.com//image-20211212140200083.png"></p><h3 id="模糊测试-Fuzz-testing"><a href="#模糊测试-Fuzz-testing" class="headerlink" title="模糊测试 Fuzz testing"></a>模糊测试 Fuzz testing</h3><ul><li><strong>黑盒模糊测试</strong>不对程序内部结构进行分析, 而是通过生成随机输入触发程序的缺陷。</li><li><strong>白盒模糊测试</strong>则利用符号执行等程序分析技术对程序结构进行分析, 以提高覆盖率和漏洞挖掘能力。</li><li><strong>灰盒模糊测试</strong>不对程序进行分析但会根据程序反馈调整输入。<br>针对生成调用序列的优化：</li></ul><p><a href="Harvey.pdf">Harvey</a></p><blockquote><p>Harvey: 通过对在智能合约程序中条件判断语句之前进行插桩的方式, 来测量合约的每一次输入是否有利于产生新的路径。此外, Harvey 还尝试通过不同函数间对全局变量的读写依赖关系进行来生成简单的调用序列, 以提高序列对程序覆盖率的影响。</p></blockquote><p><a href="ILF.pdf">ILF</a></p><blockquote><p>ILF: 其首先选取部分的智能合约程序, 并使用符号执行引擎工具对这写合约进行多次的符号执行, 以产生出覆盖率足够高的调用序列。接下来, ILF 使用神经网络对这些调用序列特征进行训练, 并生成可以模仿该序列生成规则的模型以指导模糊测试。 </p><h3 id="符号执行-Symbolic-execution"><a href="#符号执行-Symbolic-execution" class="headerlink" title="符号执行  Symbolic execution"></a>符号执行  Symbolic execution</h3><p>主要思想是把程序执行过程中不确定的输入转换为<strong>符号值</strong>，以推动程序执行与分析。<br>智能合约与传统程序的差异主要在于合约中<strong>全局变量</strong>的取值是不确定的，因此智能合约的全局变量也需要被处理为符号值。<br><a href="https://github.com/ConsenSys/mythril">Mythril</a>: 多次符号执行增加全局变量的真实性<br><a href="Manticore.pdf">Manticore</a>: 变量值具体化+多合约分析<br><img src="/.com//image-20211213101229185.png"></p><h3 id="形式化验证-Formal-verification"><a href="#形式化验证-Formal-verification" class="headerlink" title="形式化验证  Formal verification"></a>形式化验证  Formal verification</h3><p>形式化验证技术通常使用形式化的描述语言来描述一个系统的属性和特点, 为其构造出形式化规范, 再运用严格的数学逻辑证明来对其实际的运行时行为进行推理。<br><a href="sp20-verx.pdf">sp20-verx</a>: 延迟谓词抽象技术将交易执行期间的符号执行信息与交易之间的抽象信息相结合</p></blockquote><h3 id="静态程序分析"><a href="#静态程序分析" class="headerlink" title="静态程序分析"></a>静态程序分析</h3><p>SASC、SmartCheck、Slither</p><h3 id="污点分析"><a href="#污点分析" class="headerlink" title="污点分析"></a>污点分析</h3><p>通过影子栈等方式实现更加精确的数据流分析<br><img src="/.com//image-20211212211555358.png"></p><h2 id="攻防"><a href="#攻防" class="headerlink" title="攻防"></a>攻防</h2><h3 id="自动化漏洞利用（自动攻击）"><a href="#自动化漏洞利用（自动攻击）" class="headerlink" title="自动化漏洞利用（自动攻击）"></a>自动化漏洞利用（自动攻击）</h3><p><a href="teEther.pdf">teEther</a></p><h3 id="攻击阻断"><a href="#攻击阻断" class="headerlink" title="攻击阻断"></a>攻击阻断</h3><p>Sereum、EVM*提出了有效的实时攻击检测方案, ÆGIS则进一步提出了如何改进区块链中的<strong>共识算法</strong>以让全网节点对恶意交易的阻断达成共识, 但是其算法设计较为粗糙, 难以被真实应用</p><h2 id="展望"><a href="#展望" class="headerlink" title="展望"></a>展望</h2><p>Libra公司开发了面向资产的中间语言Move, 通过<strong>静态类型绑定</strong>、<strong>强制类型检查</strong>、<strong>不支持无限循环递归</strong>等方式增加安全性。将不同的高级语言编译为统一的中间语言，来提供统一的安全保障。</p><ol><li>构建统一且规范的智能合约漏洞数据集</li><li>构建基于深度学习的动静态分析综合模型</li><li>构建统一的漏洞检测工具性能评估体系</li></ol><h1 id="idea"><a href="#idea" class="headerlink" title="idea"></a>idea</h1><p>#idea</p><ol><li>十五种漏洞中，大多通过编译器就能检测出。</li><li>有安全模板和库已经大量应用</li><li>针对新功能、罕用功能的新漏洞</li><li>公共库的漏洞</li></ol><h2 id="深度学习的应用"><a href="#深度学习的应用" class="headerlink" title="深度学习的应用"></a>深度学习的应用</h2><ol><li>模糊测试的输入序列</li><li>静态分析的构图+GNN</li><li>符号分析的强化学习方法</li></ol><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ol><li>数据集手工分析，提取特征：storage、函数调用、逻辑跳转</li><li>复现和修改构图方式，比如基于符号分析构图</li><li>看基于深度学习方法挖掘漏洞的论文</li></ol>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;404的是本地pdf，自己搜论文&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;paper&quot;&gt;&lt;a href=&quot;#paper&quot; class=&quot;headerlink&quot; title=&quot;paper&quot;&gt;&lt;/a&gt;paper&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;35-%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf&quot;&gt;35-智能合约安全漏洞研究综述&lt;/a&gt;&lt;br&gt;#张超-清华&lt;br&gt;&lt;a href=&quot;%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E5%AE%89%E5%85%A8%E6%BC%8F%E6%B4%9E%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0.pdf&quot;&gt;智能合约安全漏洞检测技术研究综述&lt;/a&gt;&lt;br&gt;#钱鹏-浙工商&lt;br&gt;&lt;a href=&quot;%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%E5%AE%89%E5%85%A8%E7%BB%BC%E8%BF%B0_%E6%BC%8F%E6%B4%9E%E5%88%86%E6%9E%90.pdf&quot;&gt;智能合约安全综述_漏洞分析&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;应用领域&quot;&gt;&lt;a href=&quot;#应用领域&quot; class=&quot;headerlink&quot; title=&quot;应用领域&quot;&gt;&lt;/a&gt;应用领域&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;股权众筹&lt;/li&gt;
&lt;li&gt;游戏&lt;/li&gt;
&lt;li&gt;保险&lt;/li&gt;
&lt;li&gt;供应链&lt;/li&gt;
&lt;li&gt;物联网&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="Blockchain" scheme="http://example.com/tags/Blockchain/"/>
    
  </entry>
  
  <entry>
    <title>北大肖臻区块链技术与应用</title>
    <link href="http://example.com/2021/12/04/%E5%8C%97%E5%A4%A7%E8%82%96%E8%87%BB%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <id>http://example.com/2021/12/04/%E5%8C%97%E5%A4%A7%E8%82%96%E8%87%BB%E5%8C%BA%E5%9D%97%E9%93%BE%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%BA%94%E7%94%A8/</id>
    <published>2021-12-04T07:34:45.000Z</published>
    <updated>2021-12-11T03:26:05.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>记得比较琐碎，以太坊部分待补充完整</p></blockquote><h1 id="BTC-密码学原理"><a href="#BTC-密码学原理" class="headerlink" title="BTC-密码学原理"></a>BTC-密码学原理</h1><p>比特币最小单位satoshi</p><p>crypto-currency加密货币</p><p>cryptographic hash function密码哈希函数</p><h2 id="哈希函数的性质"><a href="#哈希函数的性质" class="headerlink" title="哈希函数的性质"></a>哈希函数的性质</h2><ol><li><strong>collision resistance</strong>冲突阻碍，指会发生哈希冲突的两个输入难以用某个高效算法找到</li></ol><blockquote><p>由实践经验得出某些哈希函数无法人为制造哈希碰撞，这一点无法在数学上证明</p><p>另一些算法，如MD5 已经被破解</p></blockquote><p>​brute-force暴力破解，指用穷举法破解</p><p>​message digest信息摘要/指纹，指利用冲突阻碍原理，信息被篡改后哈希值不改变的情况难以找到</p><span id="more"></span><ol start="2"><li><strong>hiding</strong>隐匿性</li></ol><p>​H(x)难以反推x，只能用暴力破解</p><p>​<strong>条件：</strong>x的样本空间足够大且分布均匀</p><ol start="3"><li><strong>puzzle friendly</strong></li></ol><p>$$<br>H(block header) &lt;= target<br>$$<br>$$<br>difficulty=\frac{挖矿难度=1,时对应的,target}{当前,target}<br>$$<br>$$<br>target=target<em>\frac{挖出最近,2016,个块的实际时间}{2016</em>10min}<br>$$</p><p>​target 设定目标，前面都是0，nBits</p><p>​新的target由上面公式算出，但必须处于在上一个的1/4～4倍之间，超过这个范围按边界值</p><p>​这一性质说明，完成这个目标，一定是进行了大量的穷举，即 <strong>proof of work工作量证明</strong></p><p>​difficult to solve, but  easy to verify </p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>对于股票预测的提前公布会影响股价，所以要提前一天放到封好的信封里</p><p>预测者仅仅提前公布自己的哈希值，第二天收盘时，再公布自己的预测文件</p><ul><li>预测者无法修改这个文件</li><li>别人无法提前反推这个文件</li></ul><p><strong>问题：</strong>股票的输入空间不足，这会导致隐匿性丧失；但预测者的修改还是很难实现</p><p><strong>解决：</strong>通过拼接随机数(nonce)扩大输入空间</p><h2 id="比特币用的哈希算法"><a href="#比特币用的哈希算法" class="headerlink" title="比特币用的哈希算法"></a>比特币用的哈希算法</h2><p>SHA-256</p><p>Secure Hash Algorithm</p><h2 id="签名"><a href="#签名" class="headerlink" title="签名"></a>签名</h2><p>如何开帐户？创立公钥私钥对</p><p>asymmetric encryption algorithm 非对称加密</p><p>加密用公钥，解密用对应的私钥</p><p>签名用私钥，验证用公钥 </p><h1 id="BTC-数据结构"><a href="#BTC-数据结构" class="headerlink" title="BTC-数据结构"></a>BTC-数据结构</h1><h2 id="hash-pointers"><a href="#hash-pointers" class="headerlink" title="hash pointers"></a>hash pointers</h2><p><strong>哈希指针</strong>：前一个区块的位置+H(前一个区块包含其哈希指针的所有数据)结构体</p><p>genesis block创世区块</p><p>most recent block最近区块</p><p>tamper-evident log  篡改事件日志，指中间任意一个哈希修改都会导致之后所有区块哈希值的改变</p><h2 id="Merkle-Tree"><a href="#Merkle-Tree" class="headerlink" title="Merkle Tree"></a>Merkle Tree</h2><p>默克尔树</p><p>由每个区块包含的交易构建</p><p>merkle proof</p><h2 id="block-header-block-body"><a href="#block-header-block-body" class="headerlink" title="block header + block body"></a>block header + block body</h2><p>block header：</p><ul><li><p>version</p></li><li><p>hash of previous block header 只取块头哈希</p></li><li><p>merkle root hash 存储了和当前区块交易所有相关区块的根哈希值</p></li><li><p>target</p></li><li><p>nonce</p></li></ul><p>blcok body：</p><ul><li>交易列表</li></ul><p><strong>问题</strong>：只保存了header的轻节点如何验证一笔交易是否写入了区块链中？</p><p><img src="/.com//image-20211125141800039.png" alt="image-20211125141800039"></p><blockquote><p>只要验证当前交易所在的分支，就可以证明交易tx的存在，这称之为proof of membership/inclusion</p><p>那么如何验证某个交易不存在呢？这需要获取整棵树挨个计算。更好的方案是用叶子结点按哈希值排好序。</p></blockquote><h2 id="全节点和轻节点"><a href="#全节点和轻节点" class="headerlink" title="全节点和轻节点"></a>全节点和轻节点</h2><p><strong>fully validationg node</strong></p><p><img src="/.com//image-20211126112329965.png"></p><p><strong>light node</strong></p><p><img src="/.com//image-20211126112314323.png"></p><h1 id="BTC-协议"><a href="#BTC-协议" class="headerlink" title="BTC-协议"></a>BTC-协议</h1><p>double spending attack: 一张数字货币很容易通过复制操作变两张</p><p><img src="/.com//image-20211125193750904.png"></p><p><strong>问题</strong>：B的同伙B’知道A的公钥/地址，为什么不能冒充A</p><ol><li>一方面，B’如果用自己的公钥作为输入，并声明这是A的公钥。这一公钥和其来源，即A收款的输出，即A的地址，对不上</li><li>另一方面，B’如果用A的公钥作为输入，别人会用这个去解密。但你不知道A的私钥，不能用A的私钥签名，别人用A的公钥解密的时候出错</li></ol><blockquote><p>交易的输入：币的来源交易的输出+付款人的公钥</p><p>交易的输出：收款人的地址</p></blockquote><h2 id="分布式共识"><a href="#分布式共识" class="headerlink" title="分布式共识"></a>分布式共识</h2><blockquote><p>membership机制：筛选优质会员参与 比如hyperledger 联盟链</p></blockquote><blockquote><p>如果仅仅采用投票机制，sybil attack 机器产生大量账户投票</p></blockquote><ol><li>BTC的共识：算出nonce的节点获得记账权</li></ol><p>​获得记账权的好处： </p><ul><li>出块奖励 block reward  挖出新区块的将获得铸币</li><li>帮别人记账的交易费 transaction fee</li></ul><p>​mining miner</p><ol start="2"><li>longest valid chain</li></ol><h1 id="BTC-实现"><a href="#BTC-实现" class="headerlink" title="BTC-实现"></a>BTC-实现</h1><p>UTXO: unspent TX output</p><p><img src="/.com//image-20211125211205209.png"></p><p>给定任何一个区块，计算当前所有的UXTO金额之和，等同于自创世区块到给定区块的挖矿奖励之和。</p><p>在钱包程序中，钱包管理的是一组私钥，对应的是一组公钥和地址。钱包程序必须从创世区块开始扫描每一笔交易，如果：</p><ol><li>遇到某笔交易的某个Output是钱包管理的地址之一，则钱包余额增加；</li><li>遇到某笔交易的某个Input是钱包管理的地址之一，则钱包余额减少。</li></ol><p>UXTO是存储在本地的，其正确性由遍历整个区块链验证</p><p><strong>transaction-based ledger</strong></p><p>irrevocable ledger不可篡改交易</p><h2 id="zero-confirmation-amp-six-confirmation"><a href="#zero-confirmation-amp-six-confirmation" class="headerlink" title="zero-confirmation &amp; six-confirmation"></a>zero-confirmation &amp; six-confirmation</h2><p>需要回滚某笔交易，在该比交易所在区块的前一个区块开始分叉，只要比原链更长即可</p><p>所以电商需要等待六个区块，使得分叉攻击几乎不可能</p><p>零确认指的是还未挖出新区块，电商自行验证签名和UXTO后确认收款：</p><ul><li>比特币网络中诚实的节点只会接收最早的交易</li><li>发货还需要时间，发现用户回滚交易就不发货。这时候这笔交易后的区块已经很多了</li></ul><h2 id="selfish-mining"><a href="#selfish-mining" class="headerlink" title="selfish mining"></a>selfish mining</h2><p>挖出区块但不立刻发布有什么好处？</p><p>如果比其他人快一步，别人的算力都浪费在竞争前一个区块上</p><p>但如果运气不好被别人赶上了，自己的算力都浪费了</p><h1 id="BTC-网络"><a href="#BTC-网络" class="headerlink" title="BTC-网络"></a>BTC-网络</h1><p>appication layer: BitCoin Block chain</p><p>network layer: P2P Overlay Network</p><p>simple, robust, but not efficient</p><p>flooding的方式消息传播 best effort</p><h1 id="BTC-挖矿"><a href="#BTC-挖矿" class="headerlink" title="BTC-挖矿"></a>BTC-挖矿</h1><p>调整nonce 4bytes输入空间不够，可能全部尝试了还是到不了target</p><p>所以实际计算还有coinbase交易的coinbase field可以调整，这将影响header里的merkle tree root</p><p><strong>问题</strong>：矿主是如何避免矿工挖到区块自行发布，同时又获得almost valid block的奖励？</p><p><strong>解决</strong>：coinbase的收款地址必须是矿主</p><ul><li><p>这保证了挖出来的奖励只能给矿主。如果填自己的那就没人一起挖，就和矿池没关系了。</p></li><li><p>矿工提交的almost valid block一定是填了矿主的地址。如果他填了自己的地址，也就是不按照矿主发给他的挖，那矿主那边验证不通过，他就拿不到部分奖励。</p></li></ul><h2 id="矿池的优劣"><a href="#矿池的优劣" class="headerlink" title="矿池的优劣"></a>矿池的优劣</h2><p>好处：能让矿工的收入稳定</p><p>坏处：会导致51%攻击，比如回滚和联合抵制（boycott）</p><h1 id="BTC-分叉"><a href="#BTC-分叉" class="headerlink" title="BTC-分叉"></a>BTC-分叉</h1><p>state fork: 随时会产生的</p><ul><li>forking attack or deliberate fork</li></ul><p>protocol fork: 软件/协议升级产生的</p><ul><li>hard fork:新版本认可老版本<ul><li>旧节点不更新软件，那么他将永远无法认可新版本的区块，即使含有新版本的链更长。社区会产生分裂，从而产生两种币</li><li>更新软件的新节点仍然认可老版本的区块，按照最长链原则</li><li>防范重放攻击，带chain ID</li></ul></li><li>soft fork： 新版本不兼容老版本<ul><li>新版本节点掌握大多数算力，老版本节点被迫更新</li></ul></li></ul><h1 id="BTC-匿名性-anonymity"><a href="#BTC-匿名性-anonymity" class="headerlink" title="BTC-匿名性 anonymity"></a>BTC-匿名性 anonymity</h1><p>pseudonymity 别名</p><p><strong>问题</strong>：不同的地址如何关联在一起？</p><ul><li><p>一笔交易如果你的某个地址余额不足，可以有多个输入，多个输入是同一个人</p></li><li><p>一笔交易如果有多个输出，一个是商家收款地址，另一个多出来的找零给自己的新地址</p></li></ul><p><strong>解决</strong>：零币和零钞</p><p><strong>问题</strong>：什么情况能对应现实中的某人？</p><ul><li>大额兑现法币，或者用法币买入</li><li>比特币线下支付，消费记录人人可查</li></ul><p>silk road 黑店</p><h2 id="提高匿名性"><a href="#提高匿名性" class="headerlink" title="提高匿名性"></a>提高匿名性</h2><p>应用层:</p><ul><li>coin mixing</li></ul><p>网络层:</p><ul><li>TOR 洋葱路由</li></ul><h2 id="Zero-knowledge-proof"><a href="#Zero-knowledge-proof" class="headerlink" title="Zero-knowledge proof"></a>Zero-knowledge proof</h2><p>A 向 B 证明一个陈述是正确的，而不必透露正确之外的内容。</p><p><img src="/.com//image-20211126183519831.png"></p><h1 id="Ethereum-以太坊"><a href="#Ethereum-以太坊" class="headerlink" title="Ethereum 以太坊"></a>Ethereum 以太坊</h1><p>ETH最小单位wei</p><p>创始人 Vitalik</p><ol><li>出块时间</li><li>基于ghost协议的共识机制</li><li>memory hard mining puzzle 计算密集型</li><li>pow -&gt; proof of stake</li><li>智能合约，记录历史状态，为了支持智能合约的回滚</li></ol><h2 id="账户"><a href="#账户" class="headerlink" title="账户"></a>账户</h2><ol><li><p>externally owned account</p><ol><li>balance余额</li><li>nonce 其实是counter，记录该账户交易次数，用来避免重放攻击</li></ol></li><li><p>smart contract account</p><ol><li>code</li><li>storage</li></ol></li></ol><h2 id="状态树、交易树、收据树"><a href="#状态树、交易树、收据树" class="headerlink" title="状态树、交易树、收据树"></a>状态树、交易树、收据树</h2><p>modified Merkle Patricia trie</p><p>recursive length profix</p><p><img src="/.com//image-20211127122117738.png"> </p><h2 id="GHOST"><a href="#GHOST" class="headerlink" title="GHOST"></a>GHOST</h2><p>叔父区块 7/8*3</p><p>叔父区块最多有两个</p><p>最长合法链如果包含叔链的交易，那么总共会获得2*1/32*3+3</p><p>最多6代 7/8～2/8</p><h2 id="权益证明"><a href="#权益证明" class="headerlink" title="权益证明"></a>权益证明</h2><p>虚拟挖矿</p><p>pow的系统不是一个闭环，刚诞生的会面临AltCoin Infanticide，也就是外界的美元通过挖矿转化成币，从而让这一币种的价格暴跌</p><p>pos系统是一个闭环，挖矿难度由持有的币决定</p><p>以太坊中准备使用Casper the Friendly Finality Gadget</p><ol><li>对某个epoch是否是finality用保证金进行两轮投票</li><li>对验证者进行奖励和处罚，发现对两条链下注的就没收保证金</li></ol><h2 id="智能合约"><a href="#智能合约" class="headerlink" title="智能合约"></a>智能合约</h2><p>solidity</p><p><img src="/.com//image-20211127161431761.png"> </p><blockquote><p>不支持hash表遍历，用一个数组保存</p><p>hash表所有值初始为0</p><p>不支持多线程</p></blockquote><p> <img src="/.com//image-20211127163017087.png"></p><p> <img src="/.com//image-20211127163621440.png"></p><p><strong>问题</strong>：以太坊中所有全节点都要在本地回滚自己的三棵树，并独立验证别人新发布的区块的正确性，这样得不到任何好处。他们不去验证直接认为某个新区块是正确的怎么办？</p><p><strong>解决</strong>：本地必须保证三棵树的正确性才能继续往下挖，所以一定会更新</p><p><img src="/.com//image-20211127213749740.png"></p><h2 id="拍卖auction"><a href="#拍卖auction" class="headerlink" title="拍卖auction"></a>拍卖auction</h2><p><img src="/.com//image-20211127214840557.png"></p><p>code is law</p><p>irrecocable trust不可撤消的信托</p><p>智能合约一定要反复测试再发布</p><p>Irrevocability is a double edged sword.</p><p>Nothing is irrevocable.</p><h2 id="DAO-decentralized-autonomous-organization"><a href="#DAO-decentralized-autonomous-organization" class="headerlink" title="DAO-decentralized autonomous organization"></a>DAO-decentralized autonomous organization</h2><p>The DAO 众筹的智能合约</p><p>too big to fail</p><p>硬分叉之后，旧链改名为ETC，新链沿用ETH</p><h2 id="反思"><a href="#反思" class="headerlink" title="反思"></a>反思</h2><ol><li><p>Is solidity the right programming language?</p></li><li><p>formal verification 是否应该研究？能否实现？</p></li><li><p>专门的模板和编写机构</p></li><li><p>开源也会存在安全漏洞，many eyeball fallacy虽然看的人很多，但看懂的很少，都以为别人看过</p></li><li><p>规则修改用去中心化的方法完成</p></li><li><p>在互不信任的实体建立共识才需要智能合约，其他分布式场景用不到</p></li></ol><h1 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h1><p>不是哪都能用，不应该和已有的支付方式竞争</p><ol><li>Information can flow freely on the Internet, but payment cannot.</li><li>民主不是最好的方案，只是最不坏的方案</li></ol>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;记得比较琐碎，以太坊部分待补充完整&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;BTC-密码学原理&quot;&gt;&lt;a href=&quot;#BTC-密码学原理&quot; class=&quot;headerlink&quot; title=&quot;BTC-密码学原理&quot;&gt;&lt;/a&gt;BTC-密码学原理&lt;/h1&gt;&lt;p&gt;比特币最小单位satoshi&lt;/p&gt;
&lt;p&gt;crypto-currency	加密货币&lt;/p&gt;
&lt;p&gt;cryptographic hash function	密码哈希函数&lt;/p&gt;
&lt;h2 id=&quot;哈希函数的性质&quot;&gt;&lt;a href=&quot;#哈希函数的性质&quot; class=&quot;headerlink&quot; title=&quot;哈希函数的性质&quot;&gt;&lt;/a&gt;哈希函数的性质&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;collision resistance&lt;/strong&gt;	冲突阻碍，指会发生哈希冲突的两个输入难以用某个高效算法找到&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;由实践经验得出某些哈希函数无法人为制造哈希碰撞，这一点无法在数学上证明&lt;/p&gt;
&lt;p&gt;另一些算法，如MD5 已经被破解&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;​	brute-force	暴力破解，指用穷举法破解&lt;/p&gt;
&lt;p&gt;​	message digest	信息摘要/指纹，指利用冲突阻碍原理，信息被篡改后哈希值不改变的情况难以找到&lt;/p&gt;</summary>
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="Blockchain" scheme="http://example.com/tags/Blockchain/"/>
    
  </entry>
  
  <entry>
    <title>A Peek Into the Reasoning of Neural Networks Interpreting with Structural Visual Concepts</title>
    <link href="http://example.com/2021/11/15/A-Peek-Into-the-Reasoning-of-Neural-Networks-Interpreting-with-Structural-Visual-Concepts/"/>
    <id>http://example.com/2021/11/15/A-Peek-Into-the-Reasoning-of-Neural-Networks-Interpreting-with-Structural-Visual-Concepts/</id>
    <published>2021-11-15T09:05:32.000Z</published>
    <updated>2022-10-09T08:43:51.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文为《两小时读论文系列第一篇》，完成于2021-11-10</p><p>该论文发表于CVPR2021</p><p>难度：<span class="github-emoji"><span>⭐</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></p><p>文章地址：<a href="https://arxiv.org/abs/2105.00290">https://arxiv.org/abs/2105.00290</a></p><p>PPT展示：<a href="https://www.youtube.com/watch?v=ZzkpUrK-cRA">https://www.youtube.com/watch?v=ZzkpUrK-cRA</a></p><p>code: <a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation">https://github.com/gyhandy/Visual-Reasoning-eXplanation</a></p></blockquote><h1 id="Idea"><a href="#Idea" class="headerlink" title="Idea"></a>Idea</h1><p>框架名：VRX, visual reasoning explanation</p><ol><li>提取比特征更高阶的：视觉概念</li><li>按概念之间的关系组成有向有权图</li><li>用GNN聚合图结构信息，给出为什么这样预测和为什么不预测成别的标签的解释</li><li>副产品：可以给出预测错误的原因</li></ol><span id="more"></span><h1 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h1><p><img src="/.com//image-20211110153323457.png"></p><ol><li>VCE (Visual Concept Extractor) : Grad-Cam的attention map过滤背景，用ACE (SLIC分割 -&gt; patch resize -&gt; patch2vec -&gt; 聚类 -&gt; TCAV) 得到最重要的四个视觉概念</li><li>SCG (Structural Concept Graph) : 按照人脑的思维方式，四个概念的空间相对关系应该是确定的。对四个视觉概念按空间结构构图。</li><li>GRN (Graph Reasoning Network) :  传统的卷积+全连接的模型已经能达到80%以上的正确率。但是全连接部分很难解耦，信息流动不透明，这部分难以解释；GRN 由图卷积和一个简单的 MLP 组成；用知识蒸馏使得 GRN 和原模型决策一致；因为 GRN 是解耦的、易追踪的、可解释的模型，那么就可以用 GRN 以及之前的 VCE 和 SCG 来解释传统的 CNN 模型。</li><li>VDI (Visual Decision Interpreter) : 该研究提出了基于梯度的贡献度分配算法，为每个参与决策的点（视觉概念）和边（概念之间的关系）计算其对于特定决策的贡献值，贡献值的高低代表了其肯定还是否定了该决策。</li></ol><h1 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h1><ol><li><p>SCG: 节点表示最重要的四个视觉概念，检测不足四个，则用黑色的节点补满。边是有向有权的，由空间距离和依赖系数组成，后者是可训练参数，作者认为这个参数能在训练完成后揭示概念之间的隐含关系。</p><blockquote><p>SCG是如何部分删减的？原图中写的是选择重要的边，文中找不到，个人认为是根据计算完成后的权重离0的距离选的</p></blockquote></li><li><p>训练中，所有类别的结构概念图共享一套图卷积的参数，但是每个类别在消息传递中有专属的注意力权重参数 $e_{ji}$，类别专属的注意力权重参数是为了学习每个类别独特的视觉概念之间的空间和依赖关系。</p></li><li><p>为了提升模拟的鲁棒性，该研究还用 <strong>mask out 视觉概念添加扰动</strong>的方法使得概念图推理网络与被解释的原网络在面对扰动时决策一致。</p></li></ol><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><ol><li><p>研究者用 VRX 对错误原因背后推理逻辑的解释作为修改建议，通过<strong>视觉概念（节点）的替换和删除</strong>，以及<strong>空间关系（边）的多样性扩充</strong>，原网络的错分可以被纠正。最终，VRX 将其错误总结为三种类别：</p><p><img src="/.com//image-20211110185737339.png"></p></li></ol><h1 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h1><p><a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation">https://github.com/gyhandy/Visual-Reasoning-eXplanation</a></p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文为《两小时读论文系列第一篇》，完成于2021-11-10&lt;/p&gt;
&lt;p&gt;该论文发表于CVPR2021&lt;/p&gt;
&lt;p&gt;难度：&lt;span class=&quot;github-emoji&quot;&gt;&lt;span&gt;⭐&lt;/span&gt;&lt;img src=&quot;https://github.githubassets.com/images/icons/emoji/unicode/2b50.png?v8&quot; aria-hidden=&quot;true&quot; onerror=&quot;this.parent.classList.add(&#39;github-emoji-fallback&#39;)&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;文章地址：&lt;a href=&quot;https://arxiv.org/abs/2105.00290&quot;&gt;https://arxiv.org/abs/2105.00290&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;PPT展示：&lt;a href=&quot;https://www.youtube.com/watch?v=ZzkpUrK-cRA&quot;&gt;https://www.youtube.com/watch?v=ZzkpUrK-cRA&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code: &lt;a href=&quot;https://github.com/gyhandy/Visual-Reasoning-eXplanation&quot;&gt;https://github.com/gyhandy/Visual-Reasoning-eXplanation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;Idea&quot;&gt;&lt;a href=&quot;#Idea&quot; class=&quot;headerlink&quot; title=&quot;Idea&quot;&gt;&lt;/a&gt;Idea&lt;/h1&gt;&lt;p&gt;框架名：VRX, visual reasoning explanation&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提取比特征更高阶的：视觉概念&lt;/li&gt;
&lt;li&gt;按概念之间的关系组成有向有权图&lt;/li&gt;
&lt;li&gt;用GNN聚合图结构信息，给出为什么这样预测和为什么不预测成别的标签的解释&lt;/li&gt;
&lt;li&gt;副产品：可以给出预测错误的原因&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="paper-reading" scheme="http://example.com/categories/paper-reading/"/>
    
    
    <category term="CV" scheme="http://example.com/tags/CV/"/>
    
    <category term="GNN" scheme="http://example.com/tags/GNN/"/>
    
    <category term="Interpretability" scheme="http://example.com/tags/Interpretability/"/>
    
  </entry>
  
  <entry>
    <title>GAN初识</title>
    <link href="http://example.com/2021/11/02/GAN%E5%88%9D%E8%AF%86/"/>
    <id>http://example.com/2021/11/02/GAN%E5%88%9D%E8%AF%86/</id>
    <published>2021-11-02T14:25:35.000Z</published>
    <updated>2022-10-14T04:53:49.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>生成对抗网络，Generative Adversarial Network (GAN) , 发明于2014年，是一种无监督学习的方法。最初的GAN由一个生成器和一个判别器组成：生成器把噪声作为输入，尽可能生成以假乱真的样本；判别器以真实样本和生成样本作为输入，输出样本真假判断的结果。最终能从训练好的生成器中输出高质量的，甚至超过真实样本的结果。生成器学习到的是鉴别器认同的数据的分布，如果从某个通常的隐空间（标准正态分布）冷启动，那么它学习的是分布变换的过程。</p><blockquote><p>Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua. Generative Adversarial Networks. 2014. <a href="https://arxiv.org/abs/1406.2661">arXiv:1406.2661</a></p></blockquote><span id="more"></span><p><img src="/.com//2019-07-15-124947.jpg" alt="MNIST手写数字+ GAN架构"></p><h1 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h1><p><img src="/.com//image-20211204155049112.png"></p><h1 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h1><p>最基础的GAN实现</p><pre><code class="python"># Ubuntu 16.04# tensorflow 2.5, CUDA 11.2import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltimport PILimport timeimport osfrom tensorflow.keras import layers(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()# 加了一维:通道数=1train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float')train_images = (train_images - 127.5) / 127.5BUFFER_SIZE = 60000BATCH_SIZE = 256EPOCHS = 50noise_dim = 100num_examples_to_generate = 16train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)# 生成器，输入N*100def make_generator_model():    model = tf.keras.Sequential()    model.add(layers.Dense(7 * 7 * 256, use_bias=False, input_shape=(100,)))    model.add(layers.BatchNormalization())    model.add(layers.LeakyReLU())    model.add(layers.Reshape((7, 7, 256)))    assert model.output_shape == (None, 7, 7, 256)    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))    assert model.output_shape == (None, 7, 7, 128)    model.add(layers.BatchNormalization())    model.add(layers.LeakyReLU())    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))    assert model.output_shape == (None, 14, 14, 64)    model.add(layers.BatchNormalization())    model.add(layers.LeakyReLU())    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))    assert model.output_shape == (None, 28, 28, 1)    return modelgenerator = make_generator_model()# generate 1*100 normal distribution# noise = tf.random.normal([1, 100])# generated_image = generator(noise, training=False)# discriminator,input N*28*28*1def make_discriminator_model():    model = tf.keras.Sequential()    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',                            input_shape=[28, 28, 1]))    model.add(layers.LeakyReLU())    model.add(layers.Dropout(0.3))    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))    model.add(layers.LeakyReLU())    model.add(layers.Dropout(0.3))    model.add(layers.Flatten())    model.add(layers.Dense(1))    return modeldiscriminator = make_discriminator_model()cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)# the more ones in real_output and zeros in fake_output, the better the discriminator isdef discriminator_loss(real_output, fake_output):    real_loss = cross_entropy(tf.ones_like(real_output), real_output)    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)    total_loss = real_loss + fake_loss    return total_loss# the more zeros in fake_output, the worse generator isdef generator_loss(fake_output):    return cross_entropy(tf.ones_like(fake_output), fake_output)# same learning rategenerator_optimizer = tf.keras.optimizers.Adam(1e-4)discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)@tf.functiondef train_step(images):    noise = tf.random.normal([BATCH_SIZE, noise_dim])    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:        generated_images = generator(noise, training=True)        real_output = discriminator(images, training=True)        fake_output = discriminator(generated_images, training=True)        gen_loss = generator_loss(fake_output)        disc_loss = discriminator_loss(real_output, fake_output)    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))checkpoint_dir = './training_checkpoints'checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt")checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,                                 discriminator_optimizer=discriminator_optimizer,                                 generator=generator,                                 discriminator=discriminator)seed = tf.random.normal([num_examples_to_generate, noise_dim])# for every epochs, for every batch, 256 real image + 256 fake image# after one epochs, generate 16 fake image to show the growth of generatordef train(dataset, epochs):    for epoch in range(epochs):        start = time.time()        for image_batch in dataset:            train_step(image_batch)        generate_and_save_images(generator,                                 epoch + 1,                                 seed)        # Save the model every 15 epochs        if (epoch + 1) % 15 == 0:            checkpoint.save(file_prefix=checkpoint_prefix)        print('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))    # Generate after the final epoch    generate_and_save_images(generator,                             epochs,                             seed)def generate_and_save_images(model, epoch, test_input):    predictions = model(test_input, training=False)    fig = plt.figure(figsize=(4, 4))    for i in range(predictions.shape[0]):        plt.subplot(4, 4, i + 1)        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')        plt.axis('off')    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))if __name__ == '__main__':    train(train_dataset,EPOCHS)</code></pre><p><a href="https://www.tensorflow.org/tutorials/generative/dcgan?hl=zh-cn">代码参考</a></p><h1 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h1><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><ul><li><p>DCGAN 去掉池化层、全连接层，使用BN层</p></li><li><p>PGAN 能处理1024*1024的图片</p></li><li><p>SAGAN 加入自注意力机制</p><p><a href="https://www.paperweekly.site/papers/notes/414">SAGAN论文解读</a></p></li><li><p>StyleGAN 生成器各层都输入噪声，噪声由多个全连接层生成</p><p><a href="https://zhuanlan.zhihu.com/p/263554045">StyleGAN 和 StyleGAN2 的深度理解</a></p></li><li><p>BigGAN <img src="/.com//v2-914466c51bb79895c857360a33be982b_720w.jpg"></p><blockquote><p>噪声向量 z 的块和条件标签 c 在残差块下是通过 concat 操作后送入 BatchNorm 层</p></blockquote><ul><li>发现增大 batchsize 能提高生成效果</li><li>对先验分布 z 设置阈值来对样本<strong>多样性</strong>和<strong>保真度</strong>进行控制</li></ul><p><a href="https://zhuanlan.zhihu.com/p/46581611">深度解读DeepMind新作：史上最强GAN图像生成器—BigGAN</a></p></li><li><p>U-Net 判别器输出全局和局部像素决策 </p><p><a href="https://blog.csdn.net/u014546828/article/details/111244358">MyDLNote-Network: 2020 CVPR 基于 U-Net 判别器的生成对抗网络 A U-Net Based Discriminator for Generative Adversari</a></p></li><li><p>multi-modal</p></li></ul><p><img src="/.com//image-20211101163841571.png"></p><h2 id="条件输入"><a href="#条件输入" class="headerlink" title="条件输入"></a>条件输入</h2><ul><li><p>原始的 GAN 基于噪声作为生成器的输入，一次生成所有类别的图像，无法生成指定图像。</p></li><li><p>CGAN 将类别标签和噪声拼接输入生成器</p></li><li><p>ACGAN 判别器输出真假和类别</p></li><li><p>CGAN with Projection Discriminator 判别器的中间feature和条件信息进行点乘</p></li></ul><h2 id="正则化方法"><a href="#正则化方法" class="headerlink" title="正则化方法"></a>正则化方法</h2><ul><li>BN -&gt; Pixelwise Normalization/equalized learning rate -&gt; Spectral Normalization -&gt; Wasserstein Distance</li></ul><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><ul><li><p>stochastic discriminator augmentation</p></li><li><p>Adaptive discriminator augmentation</p></li></ul><h1 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h1><ul><li><p>分类：CatGAN</p></li><li><p>生成：风格迁移、去雨去雾、去马赛克、超分辨率、虚拟换衣、换脸、表情、年龄、妆容迁移</p></li><li><p>生成人脸解决数据来源的隐私问题</p></li><li><p>生成图片用于数据增强</p></li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p><a href="https://mp.weixin.qq.com/s?__biz=MzU5MTgzNzE0MA==&amp;mid=2247483674&amp;idx=1&amp;sn=4370e8bb00f456bd1f3ad09e4adb7c56&amp;scene=21#wechat_redirect">GAN整整6年了！是时候要来捋捋了！</a></p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;p&gt;生成对抗网络，Generative Adversarial Network (GAN) , 发明于2014年，是一种无监督学习的方法。最初的GAN由一个生成器和一个判别器组成：生成器把噪声作为输入，尽可能生成以假乱真的样本；判别器以真实样本和生成样本作为输入，输出样本真假判断的结果。最终能从训练好的生成器中输出高质量的，甚至超过真实样本的结果。生成器学习到的是鉴别器认同的数据的分布，如果从某个通常的隐空间（标准正态分布）冷启动，那么它学习的是分布变换的过程。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Goodfellow, Ian J.; Pouget-Abadie, Jean; Mirza, Mehdi; Xu, Bing; Warde-Farley, David; Ozair, Sherjil; Courville, Aaron; Bengio, Yoshua. Generative Adversarial Networks. 2014. &lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot;&gt;arXiv:1406.2661&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;</summary>
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="GAN" scheme="http://example.com/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>muxViz 使用指南</title>
    <link href="http://example.com/2021/10/18/muxViz-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <id>http://example.com/2021/10/18/muxViz-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/</id>
    <published>2021-10-18T05:44:39.000Z</published>
    <updated>2022-10-09T08:44:30.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><ol><li>安装R-studio、R</li><li>muxViz-3.1（无GUI）：<a href="https://github.com/manlius/muxViz">https://github.com/manlius/muxViz</a></li><li>muxViz-2.0（GUI）：<a href="https://github.com/wjj0301/Multiplex-Networks">https://github.com/wjj0301/Multiplex-Networks</a></li></ol><h1 id="输入数据格式"><a href="#输入数据格式" class="headerlink" title="输入数据格式"></a>输入数据格式</h1><pre><code class="shell"># config.txtpath_multilayer; path_to_layers_info; path_to_layers_layout# 或者每层对应一个文件path_layer_X; label_layer_X; path_to_layout_layer_X...</code></pre><pre><code class="shell"># layers.txt# 层信息layerID layerLabel1 L12 L23 L3...</code></pre><pre><code class="shell"># .edges# 层内连边关系，无headline# 依次为 源节点ID、层ID、目标节点ID、层ID、边权值1 1 53 1 3                                                                     2 1 94 1 42 1 165 1 7...# 层间连边关系，对于multiplex，节点ID相同1 1 1 22 1 2 2...</code></pre><pre><code class="shell"># layout.txt# 节点信息，可选，加入此文件后，edges文件nodeID可直接换成nodeLabelnodeID nodeLabel1 n12 n23 n3...</code></pre><span id="more"></span><h2 id="时间轴文件格式"><a href="#时间轴文件格式" class="headerlink" title="时间轴文件格式"></a>时间轴文件格式</h2><p>这个模块允许在多层网络的顶部建立与动态过程相对应的动画，未尝识</p><h2 id="导入R"><a href="#导入R" class="headerlink" title="导入R"></a>导入R</h2><pre><code class="R">buildMultilayerNetworkFromMuxvizFiles(  config.file,  isDirected,  isWeighted,  MultisliceType,  LayerCouplingStrength = 1,  format = "muxviz edge-colored", # or "muxviz general"  verbose = T)</code></pre><h1 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h1><h2 id="调色板"><a href="#调色板" class="headerlink" title="调色板"></a>调色板</h2><pre><code class="R">library(RColorBrewer)library(viridis)</code></pre><h2 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a>绘图</h2><pre><code class="R">library(muxViz)library(igraph)library(rgl)set.seed(1)# Network setupLayers &lt;- 3Nodes &lt;- 200layerCouplingStrength &lt;- 1networkOfLayersType &lt;- "categorical"isDirected &lt;- Flayer.colors &lt;- brewer.pal(8, "Set2")pathInfomap &lt;- "infomap-0.x/Infomap"nodeTensor &lt;- list()g.list &lt;- list()# 每层社团数plantedGroupsPerLayer &lt;- 4# matrix of the stochastic block modelblock.matrix &lt;- matrix(0.1 / Nodes, plantedGroupsPerLayer,                       plantedGroupsPerLayer)diag(block.matrix) &lt;- 2 * log(Nodes) / Nodes# 总节点数平均分给四个社团block.sizes &lt;- rep(floor(Nodes / plantedGroupsPerLayer), plantedGroupsPerLayer)for (l in 1:Layers) {  #Generate the layers  #双中括号提取修改元素  g.list[[l]] &lt;- sample_sbm(Nodes, pref.matrix=block.matrix,                            block.sizes=block.sizes)    #Get the list of adjacency matrices which build the multiplex  nodeTensor[[l]] &lt;- get.adjacency(g.list[[l]])  #定义布局方式 "fr","drl", "auto", "kk", "comp", "dh"  lay &lt;- layoutMultiplex(g.list, layout="fr", ggplot.format=F, box=T)    # Show the multiplex network  plot_multiplex3D(g.list, layer.layout=lay, layer.colors=layer.colors,                   layer.shift.x=0.5, layer.space=2,                   layer.labels="auto", layer.labels.cex=1.5,                   node.size.values="auto", node.size.scale=0.8,                   show.aggregate=T)  # 导出，有问题  snapshot3d("../man/figures/multi_sbm.png", fmt = "png", width = 1024, height = 1024)}</code></pre><p><a href="https://manlius.github.io/muxViz/reference/plot_multiplex3D.html">https://manlius.github.io/muxViz/reference/plot_multiplex3D.html</a></p><h2 id="图像操作"><a href="#图像操作" class="headerlink" title="图像操作"></a>图像操作</h2><p>左键拖动整体旋转，右键按拖动方向改变各层散射角度，中键拖动缩放</p><h2 id="GUI"><a href="#GUI" class="headerlink" title="GUI"></a>GUI</h2><p>linux下有问题，win下操作</p><ul><li><p>multiplex 要画出层间连线只能用GUI，muxViz未提供接口，GUI源码仍然使用igraph，只能从sever.R中剥离此函数</p></li><li><p>GUI中给出不同节点或边指定颜色的接口，使用后闪退；可以按GUI的社团检测结果给出颜色</p></li><li><p>多层社团检测使用后闪退</p></li></ul><h2 id="导出"><a href="#导出" class="headerlink" title="导出"></a>导出</h2><p>win和linux下都有问题，建议直接截图</p><h1 id="图数据分析"><a href="#图数据分析" class="headerlink" title="图数据分析"></a>图数据分析</h1><h2 id="单层指标"><a href="#单层指标" class="headerlink" title="单层指标"></a>单层指标</h2><h2 id="多层指标"><a href="#多层指标" class="headerlink" title="多层指标"></a>多层指标</h2>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;安装R-studio、R&lt;/li&gt;
&lt;li&gt;muxViz-3.1（无GUI）：&lt;a href=&quot;https://github.com/manlius/muxViz&quot;&gt;https://github.com/manlius/muxViz&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;muxViz-2.0（GUI）：&lt;a href=&quot;https://github.com/wjj0301/Multiplex-Networks&quot;&gt;https://github.com/wjj0301/Multiplex-Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&quot;输入数据格式&quot;&gt;&lt;a href=&quot;#输入数据格式&quot; class=&quot;headerlink&quot; title=&quot;输入数据格式&quot;&gt;&lt;/a&gt;输入数据格式&lt;/h1&gt;&lt;pre&gt;&lt;code class=&quot;shell&quot;&gt;# config.txt

path_multilayer; path_to_layers_info; path_to_layers_layout
# 或者每层对应一个文件
path_layer_X; label_layer_X; path_to_layout_layer_X
...
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;shell&quot;&gt;# layers.txt
# 层信息

layerID layerLabel
1 L1
2 L2
3 L3
...
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;shell&quot;&gt;# .edges
# 层内连边关系，无headline
# 依次为 源节点ID、层ID、目标节点ID、层ID、边权值
1 1 53 1 3                                                                     
2 1 94 1 4
2 1 165 1 7
...

# 层间连边关系，对于multiplex，节点ID相同
1 1 1 2
2 1 2 2
...
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&quot;shell&quot;&gt;# layout.txt
# 节点信息，可选，加入此文件后，edges文件nodeID可直接换成nodeLabel

nodeID nodeLabel
1 n1
2 n2
3 n3
...
&lt;/code&gt;&lt;/pre&gt;</summary>
    
    
    
    <category term="tech-article" scheme="http://example.com/categories/tech-article/"/>
    
    
    <category term="R" scheme="http://example.com/tags/R/"/>
    
    <category term="interpretable" scheme="http://example.com/tags/interpretable/"/>
    
  </entry>
  
</feed>
