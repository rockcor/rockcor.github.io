<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>GNN概述</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Graph embedding2.1 deepwalk从某个节点出发随机游走，得到若干个序列，用skip-gram计算每个节点的embedding信息
2.2 LINE一阶相似性：直接相连的两节点相似
二阶相似性：有大量共同邻居的两个不相连节点相似
相似度高的节点会在embedding后有更短的欧氏距离
一阶和二阶直接拼接为节点的向量
2.3 Node2vec有策略的随机游走
深度优先得到节点的社团相似性（上图）
广度优先得到节点的结构等价性（下图）

2.4 Struc2vec
计算节点n阶邻居度的序列，

求两序列的相似度

得到两节点的结构相似性


2.5 SDNE采用多个非线性层捕获一阶二阶相似性
总结
两类学习
归纳学习（Inductive Learning）： 先从训练样本中学习到一定的模式，.."><meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Rockcor's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">GNN概述</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Graph-embedding"><span class="toc-text">Graph embedding</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-deepwalk"><span class="toc-text">2.1 deepwalk</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-LINE"><span class="toc-text">2.2 LINE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Node2vec"><span class="toc-text">2.3 Node2vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-Struc2vec"><span class="toc-text">2.4 Struc2vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-SDNE"><span class="toc-text">2.5 SDNE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%A4%E7%B1%BB%E5%AD%A6%E4%B9%A0"><span class="toc-text">两类学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GCN"><span class="toc-text">GCN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F"><span class="toc-text">训练方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF"><span class="toc-text">端到端</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5-MLP"><span class="toc-text">嵌入+MLP</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GraphSAGE"><span class="toc-text">GraphSAGE</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%82%BB%E5%B1%85%E9%87%87%E6%A0%B7"><span class="toc-text">邻居采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0"><span class="toc-text">聚合函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Minibatch"><span class="toc-text">Minibatch</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GAT"><span class="toc-text">GAT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-head-attention"><span class="toc-text">multi-head attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#PATCH-SAN"><span class="toc-text">PATCH-SAN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#HAN"><span class="toc-text">HAN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Node-level-Attention"><span class="toc-text">Node-level Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Semantic-level-Attention"><span class="toc-text">Semantic-level Attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GTN"><span class="toc-text">GTN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#metapath2vec"><span class="toc-text">metapath2vec</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#GATNE"><span class="toc-text">GATNE</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BiNE"><span class="toc-text">BiNE</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reference"><span class="toc-text">Reference</span></a></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Graph"><i class="tag post-item-tag">Graph</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">GNN概述</h1><time class="has-text-grey" datetime="2022-04-29T10:19:16.000Z">2022-04-29</time><article class="mt-2 post-content"><h1 id="Graph-embedding"><a href="#Graph-embedding" class="headerlink" title="Graph embedding"></a>Graph embedding</h1><h2 id="2-1-deepwalk"><a href="#2-1-deepwalk" class="headerlink" title="2.1 deepwalk"></a>2.1 deepwalk</h2><p>从某个节点出发随机游走，得到若干个序列，用skip-gram计算每个节点的embedding信息</p>
<h2 id="2-2-LINE"><a href="#2-2-LINE" class="headerlink" title="2.2 LINE"></a>2.2 LINE</h2><p>一阶相似性：直接相连的两节点相似</p>
<p>二阶相似性：有大量共同邻居的两个不相连节点相似</p>
<p>相似度高的节点会在embedding后有更短的欧氏距离</p>
<p>一阶和二阶直接拼接为节点的向量</p>
<h2 id="2-3-Node2vec"><a href="#2-3-Node2vec" class="headerlink" title="2.3 Node2vec"></a>2.3 Node2vec</h2><p>有策略的随机游走</p>
<p>深度优先得到节点的社团相似性（上图）</p>
<p>广度优先得到节点的结构等价性（下图）</p>
<p><img src="image-20210818171416928.png" alt=""></p>
<h2 id="2-4-Struc2vec"><a href="#2-4-Struc2vec" class="headerlink" title="2.4 Struc2vec"></a>2.4 Struc2vec</h2><ol>
<li><p>计算节点n阶邻居度的序列，</p>
</li>
<li><p>求两序列的相似度</p>
</li>
<li><p>得到两节点的结构相似性</p>
</li>
</ol>
<h2 id="2-5-SDNE"><a href="#2-5-SDNE" class="headerlink" title="2.5 SDNE"></a>2.5 SDNE</h2><p>采用多个非线性层捕获一阶二阶相似性</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="image-20210818173631288.png" alt=""></p>
<h1 id="两类学习"><a href="#两类学习" class="headerlink" title="两类学习"></a>两类学习</h1><ul>
<li><strong>归纳学习（Inductive Learning）：</strong> 先从训练样本中学习到一定的模式，然后利用其对测试样本进行预测（即首先从特殊到一般，然后再从一般到特殊），这类模型如常见的<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=贝叶斯模型&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={" sourcetype"%3a"article"%2c"sourceid"%3a34232818}"="">贝叶斯模型</a>。</li>
<li><strong>直推学习（Transductive Learning）：</strong> 先观察特定的训练样本，然后对特定的测试样本做出预测（从特殊到特殊），这类模型如k近邻、SVM等。</li>
</ul>
<h1 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h1><ul>
<li><strong>GCN的基本思想：</strong> 把一个节点在图中的高纬度邻接信息降维到一个低维的向量表示。</li>
<li><strong>GCN的优点：</strong> 可以捕捉graph的全局信息，从而很好地表示node的特征。</li>
<li><strong>GCN的缺点：</strong> Transductive learning的方式，需要把所有节点都参与训练才能得到<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=node+embedding&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra={" sourcetype"%3a"article"%2c"sourceid"%3a74242097}"="">node embedding</a>，无法快速得到新node的embedding。</li>
</ul>
<p>常用字母对应:</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>字母</th>
</tr>
</thead>
<tbody><tr>
<td>度矩阵（对角阵）</td>
<td>D</td>
</tr>
<tr>
<td>邻接矩阵（01阵）</td>
<td>A</td>
</tr>
</tbody></table>
<p>$$<br>\tilde{A}=A+I_N<br>$$<br>$$<br>\tilde{D}=D+I_N<br>$$<br>$$<br>H^{(l+1)}=\sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})<br>$$</p>
<p>$\tilde{A}H$聚合了节点自身和邻居的信息，$\tilde{D}^{-\frac{1}{2}}$起到归一化的作用。</p>
<p>归一化之后的$\tilde{A}$ 可记为：<br>$$<br>\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}<br>$$</p>
<h2 id="训练方式"><a href="#训练方式" class="headerlink" title="训练方式"></a>训练方式</h2><h3 id="端到端"><a href="#端到端" class="headerlink" title="端到端"></a>端到端</h3><p>$$<br>Z=f(X,A)=softmax(\hat{A},ReLU(\hat{A}HW^{(0)})W^{(1)})<br>$$</p>
<h2 id="嵌入-MLP"><a href="#嵌入-MLP" class="headerlink" title="嵌入+MLP"></a>嵌入+MLP</h2><h1 id="GraphSAGE"><a href="#GraphSAGE" class="headerlink" title="GraphSAGE"></a>GraphSAGE</h1><p><strong>思想：学习一个节点的信息是怎么通过其邻居节点的特征聚合而来的</strong><br>$$<br>h^k_{N(v)}=Agg({h^{k-1}_u},\forall u\in N(v))<br>$$</p>
<p>$$<br>h_v^k=\sigma(W^k \cdot Concat(h^{k-1}<em>v,h^k</em>{N(v)}) )<br>$$</p>
<p>$N(v)$表示节点 $v$ 的邻居节点集合，$Agg$()表示某个聚合函数。$k$代表层数。每一轮结束后进行归一化：<br>$$<br>h_v^k=h_v^k/\Vert h_v^k \Vert _2<br>$$</p>
<h2 id="邻居采样"><a href="#邻居采样" class="headerlink" title="邻居采样"></a>邻居采样</h2><pre><code class="python">if len(neighbors) &gt;= self.max_degree:
    neighbors=np.random.choice(neighbors, self.max_degree, replace=False)
# 数量不足将有放回的采样
elif len(neighbors) &lt; self.max_degree:
    neighbors=np.random.choice(neighbors, self.max_degree, replace=True)</code></pre>
<p><img src="image-20211209161858990.png" alt=""></p>
<h2 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h2><p>需要满足输入排列不变性，如</p>
<ul>
<li>Mean()</li>
<li><strong>GCN aggregator</strong> 比mean()多算一个自身特征</li>
<li>LSTM</li>
<li>Pooling</li>
</ul>
<p>$$<br>Agg_k^{pool}=max({ \sigma(Wh_u^k+b)})<br>$$</p>
<h2 id="Minibatch"><a href="#Minibatch" class="headerlink" title="Minibatch"></a>Minibatch</h2><p>即先采样把不需要用到的节点删除</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>让临近的节点拥有相似的表示，反之应该表示大不相同</p>
<p><img src="v2-9c473f5e242f8db158854d4e5e036b9c_r.jpg" alt=""></p>
<h1 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h1><p>$$<br>\alpha_{ij}=\frac{exp(LeakyReLu(\vec a^T[\boldsymbol W\vec h_i\Vert \boldsymbol W\vec h_j ]))}{\sum _{k \in N_i}exp(LeakyReLu(\vec a^T[\boldsymbol W\vec h_i\Vert \boldsymbol  W \vec h_k ]))}<br>$$</p>
<blockquote>
<p>相当于在聚合前计算了权重。如果$\vec a$是全1向量，那么这个权重就由邻居节点各自的特征值大小决定。<br>$$<br>\vec h_i^\prime=\sigma\left(\sum_{j\in N_i}\alpha_{ij} \boldsymbol W\vec h_j\right)<br>$$</p>
</blockquote>
<h2 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi-head attention"></a>multi-head attention</h2><p>$$<br>\vec h_i^\prime=\sigma\left(\sum_{j\in N_i}\alpha_{ij}^k \boldsymbol W^k\vec h_j\right)<br>$$</p>
<h1 id="PATCH-SAN"><a href="#PATCH-SAN" class="headerlink" title="PATCH-SAN"></a>PATCH-SAN</h1><h1 id="HAN"><a href="#HAN" class="headerlink" title="HAN"></a>HAN</h1><p>异构图：节点和边的类型之和大于2<br>元路径：按照某种类型pattern的路径（有向），不同的元路径揭示了不同的<strong>语义信息</strong><br>基于元路径的邻居：以某种元路径连接到节点i的所有节点<br>所以，在根据元路径邻居分配注意力的时候，可以依据不同类型的元路径分配权重<br><img src="image-20211223171241963.png" alt=""></p>
<h2 id="Node-level-Attention"><a href="#Node-level-Attention" class="headerlink" title="Node-level Attention"></a>Node-level Attention</h2><ol>
<li><p>根据某一类型元路径，异构图转为同构图</p>
</li>
<li><p>类似于GAT<br>$$<br> e_{ij}^\Phi=att_{node}(h^\prime_i,h^\prime_j;\Phi)<br>$$<br>$$<br>\alpha_{ij}^\Phi=softmax(e_{ij}^\Phi)<br>$$<br>$$<br>z_{i}^\Phi=\sigma(\sum_{j \in{\mathcal {N}}<em>i^\Phi}\alpha</em>{ij}^\Phi \cdot  h^\prime_j)<br>$$</p>
<h2 id="Semantic-level-Attention"><a href="#Semantic-level-Attention" class="headerlink" title="Semantic-level Attention"></a>Semantic-level Attention</h2><p>$$<br>w_{\Phi_i}=\frac{1}{|{\mathcal{V}}|} \sum_{i\in{\mathcal{V}}}q^T\cdot\tanh(W\cdot z_i^\Phi+b)<br>$$<br>$$<br>\beta_{\Phi_i}=\frac{exp(w_{\Phi_i})}{\sum_{i=1}^Pexp(w_{\Phi_i})}<br>$$<br>$$<br>Z=\sum_{i=1}^{P}\beta_{\Phi_i}\cdot Z_{\Phi_i}<br>$$</p>
<h1 id="GTN"><a href="#GTN" class="headerlink" title="GTN"></a>GTN</h1><p><img src="image-20211223194418292.png" alt=""></p>
</li>
<li><p>用$D^{-1}Q_1Q_2$归一化</p>
</li>
<li><p>为了保留图本身的性质，给点边关系集增加一个单位阵$I$这样和A相乘时还是A。</p>
</li>
<li><p>矩阵相乘后去掉对角线</p>
<h1 id="metapath2vec"><a href="#metapath2vec" class="headerlink" title="metapath2vec"></a>metapath2vec</h1><p>元路径随机游走<br>对称的元路径效果更好<br>$$<br>p(v^{i+1}|v_t^i,{\mathcal{P}})=<br>\begin{cases}<br>\frac{1}{|N_{t+1}(v_t^i)|}, &amp; \text{$(v^{i+1},v^i)\in E,\phi(v^{i+1})=t+1$}\[2ex]<br>0, &amp; \text{other situation}\<br>\end{cases}<br>$$<br>上述公式表明，在游走到某节点$v_t$的情况下，下一节点在满足一下两个条件的节点中等概率选择：</p>
</li>
<li><p>和节点$v_t$有边连接</p>
</li>
<li><p>是元路径规定的下一个节点的类型 </p>
</li>
</ol>
<blockquote>
<p>获得随机游走序列之后，和deepwalk一样输入skip-gram模型训练</p>
</blockquote>
<h1 id="GATNE"><a href="#GATNE" class="headerlink" title="GATNE"></a>GATNE</h1><ol>
<li>相比metapath2vec，增加了边类型不同的情况</li>
<li>base-embedding+edge-embedding<br><img src="image-20211226200105763.png" alt=""><blockquote>
<p>用函数逼近器得到归纳学习的效果<br><img src="image-20211226204804890.png" alt=""></p>
</blockquote>
<h1 id="BiNE"><a href="#BiNE" class="headerlink" title="BiNE"></a>BiNE</h1>给显式和隐式关系分别赋予不同的权重。<br>显式关系：直接的连边，A-B或A-A<br>隐式关系：共同邻居，类似于元路径的A-B-A</li>
</ol>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1K5411H7EQ?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click">B站某大佬</a></p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2022/06/23/%E6%8A%95%E8%B5%84%E7%9A%84%E5%BF%83%E6%80%81/" title="投资的心态"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: 投资的心态</span></a><a class="button is-default" href="/2022/03/08/%E7%BD%91%E7%BB%9C%E6%8C%96%E6%8E%98%E5%9F%BA%E7%A1%80/" title="网络挖掘基础"><span class="has-text-weight-semibold">Next: 网络挖掘基础</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="rockcor/blog-comment" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/rockcor"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> Rockcor 2023</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>