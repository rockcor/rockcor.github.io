<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>头条大数据推荐项目</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="

环境配置启动 hadoop、hive(连接 mysql)#~/hadoop_code/start_hive.sh
start-all.sh
service docker start
docker start mysql
hive --service metastore &amp;amp;

#查看mysql
docker exec -it mysql bash
mysql -uroot -p
#密码: password
ctrl+P+Q 退出

启动 hbase、spark、thriftservercd ~/bigdata
start-hbase.sh
./spark/sbin/start-all.sh
hbase thrift start

检查jps

10948 ThriftServer
3816 Res.."><meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Rockcor's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">头条大数据推荐项目</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-text">环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8-hadoop%E3%80%81hive-%E8%BF%9E%E6%8E%A5-mysql"><span class="toc-text">启动 hadoop、hive(连接 mysql)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8-hbase%E3%80%81spark%E3%80%81thriftserver"><span class="toc-text">启动 hbase、spark、thriftserver</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5"><span class="toc-text">检查</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF%E8%AE%A1%E7%AE%97%E6%9B%B4%E6%96%B0%E7%89%A9%E5%93%81%E7%94%BB%E5%83%8F"><span class="toc-text">离线计算更新物品画像</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8-Sqoop-%E8%BF%81%E7%A7%BB%E5%92%8C%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">用 Sqoop 迁移和同步数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%9F%8B%E7%82%B9%E6%94%B6%E9%9B%86"><span class="toc-text">用户行为埋点收集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%8B%E7%82%B9%E8%AE%BE%E7%BD%AE"><span class="toc-text">埋点设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8-flume-%E6%94%B6%E9%9B%86%E5%88%B0-hive-%E4%B8%AD"><span class="toc-text">用 flume 收集到 hive 中</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-supervisor-%E7%AE%A1%E7%90%86-flume-%E8%BF%9B%E7%A8%8B"><span class="toc-text">使用 supervisor 管理 flume 进程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-text">测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF%E6%96%87%E7%AB%A0%E7%94%BB%E5%83%8F%E8%AE%A1%E7%AE%97"><span class="toc-text">离线文章画像计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%9F%E5%A7%8B%E6%96%87%E7%AB%A0%E6%95%B0%E6%8D%AE%E5%90%88%E5%B9%B6"><span class="toc-text">原始文章数据合并</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%86%E5%8F%B2%E6%96%87%E7%AB%A0-tfidf-%E8%AE%A1%E7%AE%97"><span class="toc-text">历史文章 tfidf 计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%86%E5%8F%B2%E6%96%87%E7%AB%A0-textrank-%E8%AE%A1%E7%AE%97"><span class="toc-text">历史文章 textrank 计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B-word2vec-%E5%92%8C%E5%A2%9E%E9%87%8F%E6%96%87%E7%AB%A0%E7%BC%96%E7%A0%81"><span class="toc-text">训练词向量模型 word2vec 和增量文章编码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8-Apscheduler-%E5%AE%9A%E6%97%B6%E6%9B%B4%E6%96%B0%E6%96%87%E7%AB%A0%E7%94%BB%E5%83%8F"><span class="toc-text">用 Apscheduler 定时更新文章画像</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF%E7%94%A8%E6%88%B7%E5%8F%AC%E5%9B%9E%E9%9B%86%E4%B8%8E%E6%8E%92%E5%BA%8F%E8%AE%A1%E7%AE%97"><span class="toc-text">离线用户召回集与排序计算</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E7%94%BB%E5%83%8F%E5%AD%98%E5%82%A8%E4%B8%8E%E8%8E%B7%E5%8F%96"><span class="toc-text">用户画像存储与获取</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E6%9D%83%E9%87%8D%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="toc-text">用户权重计算公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E%E6%8E%92%E5%BA%8F"><span class="toc-text">召回排序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-ALS-%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="toc-text">基于 ALS 模型的召回</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="toc-text">基于内容的召回</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B-CTR"><span class="toc-text">离线排序模型 CTR</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97%E4%B8%9A%E5%8A%A1"><span class="toc-text">实时计算业务</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E7%AE%80%E4%BB%8B"><span class="toc-text">Kafka 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#flume-%E6%94%B6%E9%9B%86%E6%97%A5%E5%BF%97%E5%88%B0-kafka"><span class="toc-text">flume 收集日志到 kafka</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E6%97%B6%E5%8F%AC%E5%9B%9E%E9%9B%86%E4%B8%9A%E5%8A%A1"><span class="toc-text">实时召回集业务</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E4%B8%9A%E5%8A%A1%E6%B5%81%E5%AE%9E%E7%8E%B0%E4%B8%8E-AB-%E6%B5%8B%E8%AF%95"><span class="toc-text">推荐业务流实现与 AB 测试</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#gRPC"><span class="toc-text">gRPC</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAuser-reco-proto%E5%8D%8F%E8%AE%AE%E6%96%87%E4%BB%B6"><span class="toc-text">创建user_reco.proto协议文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%8D%E5%8A%A1%E7%AB%AF%E7%BC%96%E5%86%99"><span class="toc-text">服务端编写</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E5%93%88%E5%B8%8C%E5%88%86%E6%A1%B6%E8%BF%9B%E8%A1%8C%E6%B5%81%E9%87%8F%E5%88%87%E5%88%86"><span class="toc-text">通过哈希分桶进行流量切分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E6%9C%8D%E5%8A%A1%E4%B8%AD%E5%BF%83"><span class="toc-text">推荐服务中心</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E4%B8%AD%E5%BF%83%E4%B8%9A%E5%8A%A1%E9%80%BB%E8%BE%91"><span class="toc-text">推荐中心业务逻辑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E7%94%A8%E6%88%B7%E5%8F%AC%E5%9B%9E%E7%BB%93%E6%9E%9C"><span class="toc-text">获取用户召回结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E7%BA%BF%E9%A2%84%E6%B5%8B"><span class="toc-text">在线预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E8%B7%AF%E5%8F%AC%E5%9B%9E"><span class="toc-text">多路召回</span></a></li></ol></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/BigData"><i class="tag post-item-tag">BigData</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">头条大数据推荐项目</h1><time class="has-text-grey" datetime="2023-02-13T05:44:39.000Z">2023-02-13</time><article class="mt-2 post-content"><img src="/2023/02/13/%E5%A4%B4%E6%9D%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/image-20230211193156286.png" class="">

<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><h2 id="启动-hadoop、hive-连接-mysql"><a href="#启动-hadoop、hive-连接-mysql" class="headerlink" title="启动 hadoop、hive(连接 mysql)"></a>启动 hadoop、hive(连接 mysql)</h2><pre><code class="shell">#~/hadoop_code/start_hive.sh
start-all.sh
service docker start
docker start mysql
hive --service metastore &amp;
</code></pre>
<pre><code class="shell">#查看mysql
docker exec -it mysql bash
mysql -uroot -p
#密码: password
ctrl+P+Q 退出
</code></pre>
<h2 id="启动-hbase、spark、thriftserver"><a href="#启动-hbase、spark、thriftserver" class="headerlink" title="启动 hbase、spark、thriftserver"></a>启动 hbase、spark、thriftserver</h2><pre><code class="shell">cd ~/bigdata
start-hbase.sh
./spark/sbin/start-all.sh
hbase thrift start
</code></pre>
<h2 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h2><pre><code class="shell">jps

10948 ThriftServer
3816 ResourceManager
3145 DataNode
6571 HMaster
4813 RunJar
7667 Master
13557 Jps
6998 HRegionServer
9691 Worker
9948 RunJar
3645 SecondaryNameNode
2751 NameNode
4223 NodeManager
6463 HQuorumPeer
</code></pre>
<h1 id="离线计算更新物品画像"><a href="#离线计算更新物品画像" class="headerlink" title="离线计算更新物品画像"></a>离线计算更新物品画像</h1><h2 id="用-Sqoop-迁移和同步数据库"><a href="#用-Sqoop-迁移和同步数据库" class="headerlink" title="用 Sqoop 迁移和同步数据库"></a>用 Sqoop 迁移和同步数据库</h2><p>业务数据通常存放在 mysql 数据库中，我们需要把它定期同步到 hadoop 的 hive 数据仓库中。</p>
<pre><code class="sql">create database if not exists toutiao comment "user,news information of 136 mysql" location '/user/hive/warehouse/toutiao.db/';
</code></pre>
<pre><code class="shell">sqoop list-databases --connect jdbc:mysql://192.168.19.137:3306/ --username root -P
</code></pre>
<p>密码：<strong>password</strong><br>会显示连接到的数据库:</p>
<pre><code>information_schema
hive
mysql
performance_schema
sys
toutiao
</code></pre>
<p>写增量导入的 Sqoop 脚本</p>
<pre><code class="shell">#/root/toutiao_project/scripts/import_incremental.sh
time=`date +"%Y-%m-%d" -d "-1day"`
declare -A check
check=([user_profile]=update_time [user_basic]=last_login [news_channel]=update_time)
declare -A merge
merge=([user_profile]=user_id [user_basic]=user_id [news_channel]=channel_id)

for k in ${!check[@]}
do
    sqoop import \
        --connect jdbc:mysql://192.168.19.137/toutiao \
        --username root \
        --password password \
        --table $k \
        --m 4 \
        --target-dir /user/hive/warehouse/toutiao.db/$k \
        --incremental lastmodified \
        --check-column ${check[$k]} \
        --merge-key ${merge[$k]} \
        --last-value ${time}
done
</code></pre>
<p>写 crontab-shell 脚本让 Sqoop 定时运行</p>
<pre><code class="shell">crontab -e
#每30分钟运行一次
*/30 * * * * /root/toutiao_project/scripts/import_incremental.sh
service crond start
</code></pre>
<blockquote>
<p>这里 MySQL 里面没有创建好，实际会报错，不管。</p>
</blockquote>
<h2 id="用户行为埋点收集"><a href="#用户行为埋点收集" class="headerlink" title="用户行为埋点收集"></a>用户行为埋点收集</h2><h3 id="埋点设置"><a href="#埋点设置" class="headerlink" title="埋点设置"></a>埋点设置</h3><pre><code class="json"># 曝光的参数，
{"actionTime":"2019-04-10 18:15:35","readTime":"","channelId":0,"param":{"action": "exposure", "userId": "2", "articleId": "[18577, 14299]", "algorithmCombine": "C2"}}

# 对文章发生行为的参数
{"actionTime":"2019-04-10 18:12:11","readTime":"2886","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:32","readTime":"","channelId":18,"param":{"action": "click", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:34","readTime":"1053","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
...
</code></pre>
<h3 id="用-flume-收集到-hive-中"><a href="#用-flume-收集到-hive-中" class="headerlink" title="用 flume 收集到 hive 中"></a>用 flume 收集到 hive 中</h3><p>创建 flume 配置文件</p>
<pre><code class="shell">#/root/bigdata/flume/collect_click.conf
a1.sources = s1
a1.sinks = k1
a1.channels = c1

# 实时查看日志文件尾
a1.sources.s1.channels= c1
a1.sources.s1.type = exec
a1.sources.s1.command = tail -F /root/logs/userClick.log
# 设置两个拦截器 1.格式过滤 2.附加时间戳
a1.sources.s1.interceptors=i1 i2
a1.sources.s1.interceptors.i1.type=regex_filter
a1.sources.s1.interceptors.i1.regex=\\{.*\\}
a1.sources.r1.interceptors.i1.excludeEvents = false
a1.sources.s1.interceptors.i2.type=timestamp

# 指定缓冲区和batchdata
a1.channels.c1.type=memory
a1.channels.c1.capacity=30000
a1.channels.c1.transactionCapacity=1000

# 连接hdfs
a1.sinks.k1.type=hdfs
a1.sinks.k1.channel=c1
a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.writeFormat=Text
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.idleTimeout=60
</code></pre>
<p>hive 中创建数据库和表</p>
<pre><code class="sql">create database if not exists profile comment "user action" location '/user/hive/warehouse/profile.db/';

create table user_action(
actionTime STRING comment "user actions time",
readTime STRING comment "user reading time",
channelId INT comment "article channel id",
param map comment "action parameter")
COMMENT "user primitive action"
PARTITIONED BY(dt STRING)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
LOCATION '/user/hive/warehouse/profile.db/user_action';
</code></pre>
<p><code>ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'</code>:添加 json 格式匹配功能</p>
<p>flume 会自动生成目录，在 hive 内部表上直接同步。但是如果想要通过 spark sql 获取内容，每天还是要主动关联：</p>
<pre><code class="sql">alter table user_action add partition (dt='2023-02-11') location "/user/hive/warehouse/profile.db/user_action/2023-02-11/"
</code></pre>
<h3 id="使用-supervisor-管理-flume-进程"><a href="#使用-supervisor-管理-flume-进程" class="headerlink" title="使用 supervisor 管理 flume 进程"></a>使用 supervisor 管理 flume 进程</h3><p>flume 及其依赖写入脚本/root/toutiao_project/scripts/collect-click.sh</p>
<pre><code class="shell">#!/usr/bin/env bash

export JAVA_HOME=/root/bigdata/jdk
export HADOOP_HOME=/root/bigdata/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin

/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1
</code></pre>
<p>在/etc/supervisor 的 reco.conf 添加</p>
<pre><code class="shell">[program:collect-click]
command=/bin/bash /root/toutiao_project/scripts/collect_click.sh
user=root
autorestart=true
redirect_stderr=true
stdout_logfile=/root/logs/collect.log
loglevel=info
stopsignal=KILL
stopasgroup=true
killasgroup=true
</code></pre>
<p>最后用 supervisord 启动收集</p>
<pre><code class="shell">pip install supervisor
supervisord -c /etc/supervisord.conf
supervisorctl status
</code></pre>
<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><pre><code class="shell">echo {\"actionTime\":\"2023-02-11 21:04:39\",\"readTime\":\"\",\"channelId\":18,\"param\":{\"action\": \"click\", \"userId\": \"2\", \"articleId\": \"14299\", \"algorithmCombine\": \"C2\"}} &gt;&gt; userClick.log
</code></pre>
<p>在 <a target="_blank" rel="noopener" href="http://192.168.19.137:50070/explorer.html#/user/hive/warehouse/profile.db/user_action/">前端页面</a> 和 hive 中应当看到结果。</p>
<h2 id="离线文章画像计算"><a href="#离线文章画像计算" class="headerlink" title="离线文章画像计算"></a>离线文章画像计算</h2><h3 id="原始文章数据合并"><a href="#原始文章数据合并" class="headerlink" title="原始文章数据合并"></a>原始文章数据合并</h3><ol>
<li>创建 spark 基类</li>
<li>启动 jupyter</li>
</ol>
<pre><code class="shell">source activate py365
jupyter notebook --allow-root --ip=192.168.19.137
# 密码：123
</code></pre>
<ol start="3">
<li>运行 full_call/merge_data</li>
</ol>
<h3 id="历史文章-tfidf-计算"><a href="#历史文章-tfidf-计算" class="headerlink" title="历史文章 tfidf 计算"></a>历史文章 tfidf 计算</h3><ol>
<li>jieba 分词，去除停用词，保留名词、英文和自定义词库中的词</li>
<li>使用 spark ML 中 CountVectorizer 包进行词频统计，得到词袋模型/字典<br><img src="/2023/02/13/%E5%A4%B4%E6%9D%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/image-20230211214707213.png"></li>
<li>使用 spark ML 中 IDF 包进一步计算每个单词的权重</li>
<li>根据索引和权重排序得到可以每篇文章权重最高的 20 个词</li>
</ol>
<h3 id="历史文章-textrank-计算"><a href="#历史文章-textrank-计算" class="headerlink" title="历史文章 textrank 计算"></a>历史文章 textrank 计算</h3><pre><code class="python">def textrank(partition):
    import os

    import jieba
    import jieba.analyse
    import jieba.posseg as pseg
    import codecs

    abspath = "/root/words"

    # 结巴加载用户词典
    userDict_path = os.path.join(abspath, "ITKeywords.txt")
    jieba.load_userdict(userDict_path)

    # 停用词文本
    stopwords_path = os.path.join(abspath, "stopwords.txt")

    def get_stopwords_list():
        """返回stopwords列表"""
        stopwords_list = [i.strip()
                          for i in codecs.open(stopwords_path).readlines()]
        return stopwords_list

    # 所有的停用词列表
    stopwords_list = get_stopwords_list()

    class TextRank(jieba.analyse.TextRank):
        def __init__(self, window=20, word_min_len=2):
            super(TextRank, self).__init__()
            self.span = window  # 窗口大小
            self.word_min_len = word_min_len  # 单词的最小长度
            # 要保留的词性，根据jieba github ，具体参见https://github.com/baidu/lac
            self.pos_filt = frozenset(
                ('n', 'x', 'eng', 'f', 's', 't', 'nr', 'ns', 'nt', "nw", "nz", "PER", "LOC", "ORG"))

        def pairfilter(self, wp):
            """过滤条件，返回True或者False"""

            if wp.flag == "eng":
                if len(wp.word) &lt;= 2:
                    return False

            if wp.flag in self.pos_filt and len(wp.word.strip()) &gt;= self.word_min_len \
                    and wp.word.lower() not in stopwords_list:
                return True
    # TextRank过滤窗口大小为5，单词最小为2
    textrank_model = TextRank(window=5, word_min_len=2)
    allowPOS = ('n', "x", 'eng', 'nr', 'ns', 'nt', "nw", "nz", "c")
</code></pre>
<p>同样可以给出 20 个关键词。但是最终结果由 Textank * IDF 再取前 20 给出<br><img src="/2023/02/13/%E5%A4%B4%E6%9D%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/image-20230211220415129.png"></p>
<h3 id="训练词向量模型-word2vec-和增量文章编码"><a href="#训练词向量模型-word2vec-和增量文章编码" class="headerlink" title="训练词向量模型 word2vec 和增量文章编码"></a>训练词向量模型 word2vec 和增量文章编码</h3><pre><code class="python">from pyspark.ml.feature import Word2Vec

# minCount忽略总频率低于此频率的所有单词
w2v = Word2Vec(vectorSize=100, inputCol='words', outputCol='model', minCount=3)
w2v_model = w2v.fit(words_df)
w2v_model.write().overwrite().save("hdfs://hadoop-master:9000/headlines/models/test.word2vec")

from pyspark.ml.feature import Word2VecModel

word_vec = Word2VecModel.load("hdfs://hadoop-master:9000/headlines/models/test.word2vec")
vectors = word_vec.getVectors()
</code></pre>
<p>编码后和每个单词权重相乘，最终得到每篇文章的特征向量（文章画像）</p>
<h3 id="用-Apscheduler-定时更新文章画像"><a href="#用-Apscheduler-定时更新文章画像" class="headerlink" title="用 Apscheduler 定时更新文章画像"></a>用 Apscheduler 定时更新文章画像</h3><ol>
<li>增量更新文章编码，包括 hive 里的 article_profile<blockquote>
<p>新词可以用平均值填充</p>
</blockquote>
</li>
<li>定期重新计算 tfidf、textrank 和 word2vec 模型</li>
<li>Apsheduler 是 crontab 升级版</li>
</ol>
<pre><code class="python">import sys
import os
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))
sys.path.insert(0, os.path.join(BASE_DIR, 'reco_sys'))
from apscheduler.schedulers.blocking import BlockingScheduler
from apscheduler.executors.pool import ProcessPoolExecutor
from scheduler.update import update_article_profile


# 创建scheduler，多进程执行
executors = {
    'default': ProcessPoolExecutor(3)
}

scheduler = BlockingScheduler(executors=executors)

# 添加定时更新任务更新文章画像,每隔一小时更新
scheduler.add_job(update_article_profile, trigger='interval', hours=1)


scheduler.start()
</code></pre>
<h1 id="离线用户召回集与排序计算"><a href="#离线用户召回集与排序计算" class="headerlink" title="离线用户召回集与排序计算"></a>离线用户召回集与排序计算</h1><h2 id="用户画像存储与获取"><a href="#用户画像存储与获取" class="headerlink" title="用户画像存储与获取"></a>用户画像存储与获取</h2><p>用户画像需要快速迭代，方便读取，选择存储在 hbase 中。这里我们从 hbase 关联到 hive。</p>
<pre><code class="sql">create external table user_profile_hbase(
user_id STRING comment "userID",
information map&lt;string, DOUBLE&gt; comment "user basic information",
article_partial map&lt;string, DOUBLE&gt; comment "article partial",
env map&lt;string, INT&gt; comment "user env")
COMMENT "user profile table"
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,basic:,partial:,env:")
TBLPROPERTIES ("hbase.table.name" = "user_profile");
</code></pre>
<p>读取 user_article_basic 表，<strong>合并行为表</strong>与<strong>文章画像中的主题词</strong></p>
<pre><code class="python"># 获取基本用户行为信息，然后进行文章画像的主题词合并
uup.spark.sql("use profile")
# 取出日志中的channel_id
user_article_ = uup.spark.sql("select * from user_article_basic").drop('channel_id')
uup.spark.sql('use article')
article_label = uup.spark.sql("select article_id, channel_id, topics from article_profile")
# 合并使用文章中正确的channel_id
click_article_res = user_article_.join(article_label, how='left', on=['article_id'])
</code></pre>
<h3 id="用户权重计算公式"><a href="#用户权重计算公式" class="headerlink" title="用户权重计算公式"></a>用户权重计算公式</h3><p><strong>用户标签权重 =( 行为类型权重之和) × 时间衰减</strong></p>
<table>
<thead>
<tr>
<th>行为</th>
<th>分值</th>
</tr>
</thead>
<tbody><tr>
<td>阅读时间(&lt;1000)</td>
<td>1</td>
</tr>
<tr>
<td>阅读时间(&gt;=1000)</td>
<td>2</td>
</tr>
<tr>
<td>收藏</td>
<td>2</td>
</tr>
<tr>
<td>分享</td>
<td>3</td>
</tr>
<tr>
<td>点击</td>
<td>5</td>
</tr>
<tr>
<td><strong>时间衰减</strong>=1/(log(t)+1) ,t 为时间发生时间距离当前时间的大小</td>
<td></td>
</tr>
</tbody></table>
<p>使用 happybase 关联文章表，统计每个词的标签权重，得到用户的关键词喜好 top10</p>
<pre><code class="python">import happybase
#  用于读取hbase缓存结果配置
pool = happybase.ConnectionPool(size=10, host='192.168.19.137', port=9090)

with pool.connection() as conn:
    table = conn.table('user_profile')
    # 获取每个键 对应的所有列的结果
    data = table.row(b'user:2', columns=[b'partial'])
    conn.close()
</code></pre>
<p>完善代码后，添加到 Apscheduler 中</p>
<pre><code class="python">scheduler.add_job(update_user_profile, trigger='interval', hours=2)
</code></pre>
<h2 id="召回排序"><a href="#召回排序" class="headerlink" title="召回排序"></a>召回排序</h2><ul>
<li>用户冷启动（前期点击行为较少情况）<ul>
<li>非个性化推荐<ul>
<li><strong>热门召回</strong>：自定义热门规则，根据当前时间段热点定期更新维护<em>热点文章库</em></li>
<li><strong>新文章召回</strong>：为了提高新文章的曝光率，建立<em>新文章库</em>，进行推荐</li>
</ul>
</li>
<li>个性化推荐：<ul>
<li><strong>基于内容的协同过滤在线召回</strong>：基于用户实时兴趣画像相似的召回结果用于首页的个性化推荐</li>
</ul>
</li>
</ul>
</li>
<li>后期离线部分（用户点击行为较多，用户画像完善）<ul>
<li>建立用户长期兴趣画像（详细）：包括用户各个维度的兴趣特征</li>
<li>训练排序模型<ul>
<li><strong>LR 模型、FTRL、Wide&amp;Deep</strong></li>
</ul>
</li>
<li>离线部分的召回：<ul>
<li><strong>基于模型协同过滤推荐离线召回</strong>：ALS</li>
<li><strong>基于内容的离线召回</strong>：或者称基于用户画像的召回</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="基于-ALS-模型的召回"><a href="#基于-ALS-模型的召回" class="headerlink" title="基于 ALS 模型的召回"></a>基于 ALS 模型的召回</h3><pre><code class="python">from pyspark.ml.recommendation import ALS
# 模型训练和推荐默认每个用户固定文章个数
als = ALS(userCol='als_user_id', itemCol='als_article_id', ratingCol='clicked', checkpointInterval=1)
model = als.fit(als_user_article_click)
recall_res = model.recommendForAllUsers(100)
</code></pre>
<p>召回结果存储</p>
<pre><code class="python">def save_offline_recall_hbase(partition):
    """离线模型召回结果存储
    """
    import happybase
    pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
    for row in partition:
        with pool.connection() as conn:
            # 获取历史看过的该频道文章
            history_table = conn.table('history_recall')
            # 多个版本
            data = history_table.cells('reco:his:{}'.format(row.user_id).encode(),
                                       'channel:{}'.format(row.channel_id).encode())

            history = []
            if len(data) &gt;= 2:
                for l in data[:-1]:
                    history.extend(eval(l))
            else:
                history = []

            # 过滤reco_article与history
            reco_res = list(set(row.article_list) - set(history))

            if reco_res:

                table = conn.table('cb_recall')
                # 默认放在推荐频道
                table.put('recall:user:{}'.format(row.user_id).encode(),
                          {'als:{}'.format(row.channel_id).encode(): str(reco_res).encode()})
                conn.close()

                # 放入历史推荐过文章
                history_table.put("reco:his:{}".format(row.user_id).encode(),
                                  {'channel:{}'.format(row.channel_id): str(reco_res).encode()})
            conn.close()

als_recall.foreachPartition(save_offline_recall_hbase)
</code></pre>
<h3 id="基于内容的召回"><a href="#基于内容的召回" class="headerlink" title="基于内容的召回"></a>基于内容的召回</h3><p>即根据 LHS 等算法，快速得到用户当前点击文章的相似文章集，进行推荐。</p>
<pre><code class="python"># 循环partition
for row in partition:
    # 获取相似文章结果表
    similar_article = similar_table.row(str(row.article_id).encode(),
                                        columns=[b'similar'])
    # 相似文章相似度排序过滤，召回不需要太大的数据， 百个，千
    _srt = sorted(similar_article.items(), key=lambda item: item[1], reverse=True)
    if _srt:
        # 每次行为推荐若干篇文章
        reco_article = [int(i[0].split(b':')[1]) for i in _srt][:10]
</code></pre>
<blockquote>
<p>基于内容和基于模型的结果存入同一张 hbase 表</p>
</blockquote>
<pre><code class="sql">create external table cb_recall_hbase(
user_id STRING comment "userID",
als map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment "als recall",
content map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment "content recall",
online map&lt;string, ARRAY&lt;BIGINT&gt;&gt; comment "online recall")
COMMENT "user recall table"
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,als:,content:,online:")
TBLPROPERTIES ("hbase.table.name" = "cb_recall");
</code></pre>
<h3 id="离线排序模型-CTR"><a href="#离线排序模型-CTR" class="headerlink" title="离线排序模型 CTR"></a>离线排序模型 CTR</h3><p>CTR（Click-Through Rate）预估：给定一个 Item，预测该 Item 会被点击的概率<br>最基础的模型目前都是基于 LR 的点击率预估策略，目前在工业使用模型做预估的有这么几种类型</p>
<ul>
<li>宽模型 + 特征⼯程<ul>
<li>LR/MLR + 非 ID 类特征(⼈⼯离散/GBDT/FM)</li>
<li>spark 中可以直接使用</li>
</ul>
</li>
<li>宽模型 + 深模型<ul>
<li>wide&amp;deep,DeepFM</li>
<li>使用 TensorFlow 进行训练</li>
</ul>
</li>
<li>深模型：<ul>
<li>DNN + 特征 embedding</li>
<li>使用 TensorFlow 进行训练</li>
</ul>
</li>
</ul>
<p>特征包含：用户画像关键词 10+文章画像关键词 10+channel_id(25, onehot)+文章主题词向量(concat, 100)</p>
<pre><code class="python">cols = ['article_id', 'user_id', 'channel_id', 'articlevector', 'weights', 'article_weights', 'clicked']

train_version_two = VectorAssembler().setInputCols(cols[2:6]).setOutputCol("features").transform(train)
</code></pre>
<p><img src="/2023/02/13/%E5%A4%B4%E6%9D%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/image-20230212102837399.png"><br>训练线性回归模型，可以服务于在线召回</p>
<pre><code class="python">lr = LogisticRegression()
model = lr.setLabelCol("clicked").setFeaturesCol("features").fit(train_version_two)
model.save("hdfs://hadoop-master:9000/headlines/models/lr.obj")
</code></pre>
<p>定期重新训练</p>
<h1 id="实时计算业务"><a href="#实时计算业务" class="headerlink" title="实时计算业务"></a>实时计算业务</h1><p>实时（在线）计算：</p>
<ul>
<li>解决用户冷启动问题</li>
<li>实时计算能够根据用户的点击实时反馈，快速跟踪用户的喜好</li>
</ul>
<p>日志数据我们已经收集到 hadoop 中，但是做实时分析的时候，我们需要将每个时刻用户产生的点击行为收集到 KAFKA 当中，等待 spark streaming 程序去消费。</p>
<h2 id="Kafka-简介"><a href="#Kafka-简介" class="headerlink" title="Kafka 简介"></a>Kafka 简介</h2><p><strong>Kafka</strong>是由<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Apache%E8%BD%AF%E4%BB%B6%E5%9F%BA%E9%87%91%E4%BC%9A" title="Apache软件基金会">Apache 软件基金会</a>开发的一个<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%BC%80%E6%BA%90" title="开源">开源</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%B5%81%E5%A4%84%E7%90%86" title="流处理">流处理</a>平台，由<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Scala" title="Scala">Scala</a>和<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/Java" title="Java">Java</a>编写。该项目的目标是为处理实时数据提供一个统一、高吞吐、低延迟的平台。其持久化层本质上是一个“按照分布式事务日志架构的大规模发布/订阅消息队列”，这使它作为企业级基础设施来处理流式数据非常有价值。</p>
<h2 id="flume-收集日志到-kafka"><a href="#flume-收集日志到-kafka" class="headerlink" title="flume 收集日志到 kafka"></a>flume 收集日志到 kafka</h2><p>开启 zookeeper,需要在一直在服务器端实时运行，以守护进程运行</p>
<pre><code class="shell">/root/bigdata/kafka/bin/zookeeper-server-start.sh -daemon /root/bigdata/kafka/config/zookeeper.properties
</code></pre>
<p>以及 kafka</p>
<pre><code>/root/bigdata/kafka/bin/kafka-server-start.sh /root/bigdata/kafka/config/server.properties
</code></pre>
<p>测试</p>
<pre><code class="shell"> #开启消息生产者
/root/bigdata/kafka/bin/kafka-console-producer.sh --broker-list 192.168.19.137:9092 --sync --topic click-trace
 #开启消费者
/root/bigdata/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.19.137:9092 --topic  click-trace
#在生产者窗口输入任意内容测试
</code></pre>
<p>修改原来收集日志的文件，添加 flume 收集日志行为到 kafka 的 source, channel, sink</p>
<pre><code>a1.sources = s1
a1.sinks = k1 k2
a1.channels = c1 c2

a1.sources.s1.channels= c1 c2
a1.sources.s1.type = exec
a1.sources.s1.command = tail -F /root/logs/userClick.log
a1.sources.s1.interceptors=i1 i2
a1.sources.s1.interceptors.i1.type=regex_filter
a1.sources.r1.interceptors.i1.excludeEvents = false
a1.sources.s1.interceptors.i1.regex=\\{.*\\}
a1.sources.s1.interceptors.i2.type=timestamp

# channel1
a1.channels.c1.type=memory
a1.channels.c1.capacity=30000
a1.channels.c1.transactionCapacity=1000

# channel2
a1.channels.c2.type=memory
a1.channels.c2.capacity=30000
a1.channels.c2.transactionCapacity=1000

# k1
a1.sinks.k1.type=hdfs
a1.sinks.k1.channel=c1
a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.writeFormat=Text
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.idleTimeout=60

# k2
a1.sinks.k2.channel=c2
a1.sinks.k2.type=org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k2.kafka.bootstrap.servers=192.168.19.137:9092
a1.sinks.k2.kafka.topic=click-trace
a1.sinks.k2.kafka.batchSize=20
a1.sinks.k2.kafka.producer.requiredAcks=1
</code></pre>
<p>添加 supervisor 配置</p>
<pre><code>[program:kafka]
command=/bin/bash /root/toutiao_project/scripts/start_kafka.sh
user=root
autorestart=true
redirect_stderr=true
stdout_logfile=/root/logs/kafka.log
loglevel=info
stopsignal=KILL
stopasgroup=true
killasgroup=true
</code></pre>
<p>用 supervisorctl 启动后测试</p>
<h2 id="实时召回集业务"><a href="#实时召回集业务" class="headerlink" title="实时召回集业务"></a>实时召回集业务</h2><p>实时召回基于相似度的文章推荐</p>
<p>创建 online 文件夹，建立在线实时处理程序</p>
<ul>
<li>目的：对用户日志进行处理，实时达到求出相似文章，放入用户召回集合中</li>
<li>步骤：<ul>
<li>1、配置 spark streaming 信息</li>
<li>2、读取点击行为日志数据，获取相似文章列表</li>
<li>3、过滤历史文章集合</li>
<li>4、存入召回结果以及历史记录结果</li>
</ul>
</li>
</ul>
<p>happybase 和 kafka 对接 spark streaming 的配置</p>
<pre><code class="python"># 增加spark online 启动配置
class DefaultConfig(object):
    """默认的一些配置信息
    """
    SPARK_ONLINE_CONFIG = (
        ("spark.app.name", "onlineUpdate"),  # 设置启动的spark的app名称，没有提供，将随机产生一个名称
        ("spark.master", "yarn"),
        ("spark.executor.instances", 4)
    )
# 添加sparkstreaming启动对接kafka的配置

from pyspark import SparkConf
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from setting.default import DefaultConfig

import happybase

#  用于读取hbase缓存结果配置
pool = happybase.ConnectionPool(size=10, host='hadoop-master', port=9090)
# 1、创建conf
conf = SparkConf()
conf.setAll(DefaultConfig.SPARK_ONLINE_CONFIG)
# 建立spark session以及spark streaming context
sc = SparkContext(conf=conf)
# 创建Streaming Context
stream_c = StreamingContext(sc, 60)

# KAFKA配置
KAFKA_SERVER = "192.168.19.137:9092"
# 基于内容召回配置，用于收集用户行为，获取相似文章实时推荐
similar_kafkaParams = {"metadata.broker.list": DefaultConfig.KAFKA_SERVER, "group.id": 'similar'}
SIMILAR_DS = KafkaUtils.createDirectStream(stream_c, ['click-trace'], similar_kafkaParams)
</code></pre>
<p>主代码</p>
<pre><code class="python">class OnlineRecall(object):
"""在线处理计算平台
"""
    def __init__(self):
        pass
    
    def _update_online_cb(self):
            """
            通过点击行为更新用户的cb召回表中的online召回结果
            :return:
            """
            def foreachFunc(rdd):
    
                for data in rdd.collect():
                    logger.info(
                        "{}, INFO: rdd filter".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
                    # 判断日志行为类型，只处理点击流日志
                    if data["param"]["action"] in ["click", "collect", "share"]:
                        # print(data)
                        with pool.connection() as conn:
                            try:
                                # 相似文章表
                                sim_table = conn.table("article_similar")
    
                                # 根据用户点击流日志涉及文章找出与之最相似文章(基于内容的相似)，选取TOP-k相似的作为召回推荐结果
                                _dic = sim_table.row(str(data["param"]["articleId"]).encode(), columns=[b"similar"])
                                _srt = sorted(_dic.items(), key=lambda obj: obj[1], reverse=True)  # 按相似度排序
                                if _srt:
    
                                    topKSimIds = [int(i[0].split(b":")[1]) for i in _srt[:self.k]]
    
                                    # 根据历史推荐集过滤，已经给用户推荐过的文章
                                    history_table = conn.table("history_recall")
    
                                    _history_data = history_table.cells(
                                        b"reco:his:%s" % data["param"]["userId"].encode(),
                                        b"channel:%d" % data["channelId"]
                                    )
                                    # print("_history_data: ", _history_data)
    
                                    history = []
                                    if len(data) &gt;= 2:
                                        for l in data[:-1]:
                                            history.extend(eval(l))
                                    else:
                                        history = []
    
                                    # 根据历史召回记录，过滤召回结果
                                    recall_list = list(set(topKSimIds) - set(history_data))
    
                                    # print("recall_list: ", recall_list)
                                    logger.info("{}, INFO: store user:{} cb_recall data".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), data["param"]["userId"]))
                                    if recall_list:
                                        # 如果有推荐结果集，那么将数据添加到cb_recall表中，同时记录到历史记录表中
                                        logger.info(
                                            "{}, INFO: get online-recall data".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
                                        recall_table = conn.table("cb_recall")
    
                                        recall_table.put(
                                            b"recall:user:%s" % data["param"]["userId"].encode(),
                                            {b"online:%d" % data["channelId"]: str(recall_list).encode()}
                                        )
    
                                        history_table.put(
                                            b"reco:his:%s" % data["param"]["userId"].encode(),
                                            {b"channel:%d" % data["channelId"]: str(recall_list).encode()}
                                        )
                            except Exception as e:
                                logger.info("{}, WARN: {}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))
                            finally:
                                conn.close()
    
            SIMILAR_DS.map(lambda x: json.loads(x[1])).foreachRDD(foreachFunc)
    
            return None
</code></pre>
<h1 id="推荐业务流实现与-AB-测试"><a href="#推荐业务流实现与-AB-测试" class="headerlink" title="推荐业务流实现与 AB 测试"></a>推荐业务流实现与 AB 测试</h1><ul>
<li><p>逻辑流程</p>
<ul>
<li>1、后端发送推荐请求，实时推荐系统拿到请求参数<ul>
<li>grpc对接</li>
</ul>
</li>
<li>2、根据用户进行ABTest分流<ul>
<li>ABTest实验中心，用于进行分流任务，方便测试调整不同的模型上线</li>
</ul>
</li>
<li>3、推荐中心服务<ul>
<li>根据用户在ABTest分配的算法进行召回服务和排序服务读取返回结果</li>
</ul>
</li>
<li>4、返回推荐结果和埋点参数封装</li>
</ul>
</li>
</ul>
<p><img src="/2023/02/13/%E5%A4%B4%E6%9D%A1%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E9%A1%B9%E7%9B%AE/image-20230212160039176.png"></p>
<h2 id="gRPC"><a href="#gRPC" class="headerlink" title="gRPC"></a>gRPC</h2><ul>
<li>gRPC是由Google公司开源的高性能RPC框架。</li>
<li>gRPC支持多语言<br>gRPC原生使用C、Java、Go进行了三种实现，而C语言实现的版本进行封装后又支持C++、C#、Node、ObjC、 Python、Ruby、PHP等开发语言</li>
<li>gRPC支持多平台<br>支持的平台包括：Linux、Android、iOS、MacOS、Windows</li>
<li>gRPC的消息协议使用Google自家开源的Protocol Buffers协议机制（proto3） 序列化</li>
<li>gRPC的传输使用HTTP/2标准，支持双向流和连接多路复用</li>
</ul>
<h3 id="创建user-reco-proto协议文件"><a href="#创建user-reco-proto协议文件" class="headerlink" title="创建user_reco.proto协议文件"></a>创建user_reco.proto协议文件</h3><ul>
<li>用户刷新feed流接口<ul>
<li>user_recommend(User) returns (Track)</li>
</ul>
</li>
<li>文章相似(猜你喜欢)接口<ul>
<li>article_recommend(Article) returns(Similar)</li>
</ul>
</li>
</ul>
<p>编写grpc_tools.protoc</p>
<pre><code class="shell">syntax = "proto3";

message User {

    string user_id = 1;
    int32 channel_id = 2;
    int32 article_num = 3;
    int64 time_stamp = 4;
}
// int32 ---&gt; int64 article_id
message Article {

    int64 article_id = 1;
    int32 article_num = 2;

}

message param2 {
    string click = 1;
    string collect = 2;
    string share = 3;
    string read = 4;
}

message param1 {
    int64 article_id = 1;
    param2 params = 2;
}

message Track {
    string exposure = 1;
    repeated param1 recommends = 2;
    int64 time_stamp = 3;
}

message Similar {
    repeated int64 article_id = 1;
}

service UserRecommend {
    // feed recommend
    rpc user_recommend(User) returns (Track) {}
    rpc article_recommend(Article) returns(Similar) {}
}
</code></pre>
<p>通过命令生成</p>
<pre><code class="shell">python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. user_reco.proto
</code></pre>
<h3 id="服务端编写"><a href="#服务端编写" class="headerlink" title="服务端编写"></a>服务端编写</h3><pre><code class="python"># route.py
# 基于用户推荐的rpc服务推荐
# 定义指定的rpc服务输入输出参数格式proto
RPC_SERVER = '192.168.19.137:9999'
class UserRecommendServicer(user_reco_pb2_grpc.UserRecommendServicer):
    """
    对用户进行技术文章推荐
    """
    def user_recommend(self, request, context):
        """
        用户feed流推荐
        :param request:
        :param context:
        :return:
        """
        # 选择C4组合
        user_id = request.user_id
        channel_id = request.channel_id
        article_num = request.article_num
        time_stamp = request.time_stamp

        # 解析参数，并进行推荐中心推荐(暂时使用假数据替代)
        class Temp(object):
            user_id = -10
            algo = 'test'
            time_stamp = -10

        tp = Temp()
        tp.user_id = user_id
        tp.time_stamp = time_stamp
        _track = add_track([], tp)

        # 解析返回参数到rpc结果参数
        # 参数如下
        # [       {"article_id": 1, "param": {"click": "", "collect": "", "share": "", 'detentionTime':''}},
        #         {"article_id": 2, "param": {"click": "", "collect": "", "share": "", 'detentionTime':''}},
        #         {"article_id": 3, "param": {"click": "", "collect": "", "share": "", 'detentionTime':''}},
        #         {"article_id": 4, "param": {"click": "", "collect": "", "share": "", 'detentionTime':''}}
        #     ]
        # 第二个rpc参数
        _param1 = []
        for _ in _track['recommends']:
            # param的封装
            _params = user_reco_pb2.param2(click=_['param']['click'],
                                           collect=_['param']['collect'],
                                           share=_['param']['share'],
                                           read=_['param']['read'])
            _p2 = user_reco_pb2.param1(article_id=_['article_id'], params=_params)
            _param1.append(_p2)
        # param
        return user_reco_pb2.Track(exposure=_track['param'], recommends=_param1, time_stamp=_track['timestamp'])

#    def article_recommend(self, request, context):
#        """
#       文章相似推荐
#       :param request:
#       :param context:
#       :return:
#       """
#       # 获取web参数
#       article_id = request.article_id
#       article_num = request.article_num
#
#        # 进行文章相似推荐,调用推荐中心的文章相似
#       _article_list = article_reco_list(article_id, article_num, 105)
#
#       # rpc参数封装
#       return user_reco_pb2.Similar(article_id=_article_list)

def add_track(res, temp):
    """
    封装埋点参数
    :param res: 推荐文章id列表
    :param cb: 合并参数
    :param rpc_param: rpc参数
    :return: 埋点参数
        文章列表参数
        单文章参数
    """
    # 添加埋点参数
    track = {}

    # 准备曝光参数
    # 全部字符串形式提供，在hive端不会解析问题
    _exposure = {"action": "exposure", "userId": temp.user_id, "articleId": json.dumps(res),
                 "algorithmCombine": temp.algo}

    track['param'] = json.dumps(_exposure)
    track['recommends'] = []

    # 准备其它点击参数
    for _id in res:
        # 构造字典
        _dic = {}
        _dic['article_id'] = _id
        _dic['param'] = {}

        # 准备click参数
        _p = {"action": "click", "userId": temp.user_id, "articleId": str(_id),
              "algorithmCombine": temp.algo}

        _dic['param']['click'] = json.dumps(_p)
        # 准备collect参数
        _p["action"] = 'collect'
        _dic['param']['collect'] = json.dumps(_p)
        # 准备share参数
        _p["action"] = 'share'
        _dic['param']['share'] = json.dumps(_p)
        # 准备detentionTime参数
        _p["action"] = 'read'
        _dic['param']['read'] = json.dumps(_p)

        track['recommends'].append(_dic)

    track['timestamp'] = temp.time_stamp
    return track


def serve():

    # 多线程服务器
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    # 注册本地服务
    user_reco_pb2_grpc.add_UserRecommendServicer_to_server(UserRecommendServicer(), server)
    # 监听端口
    server.add_insecure_port(DefaultConfig.RPC_SERVER)

    # 开始接收请求进行服务
    server.start()
    # 使用 ctrl+c 可以退出服务
    _ONE_DAY_IN_SECONDS = 60 * 60 * 24
    try:
        while True:
            time.sleep(_ONE_DAY_IN_SECONDS)
    except KeyboardInterrupt:
        server.stop(0)


if __name__ == '__main__':
    # 测试grpc服务
    serve()
</code></pre>
<p>客户端测试代码：</p>
<pre><code class="python">import os
import sys

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))
from abtest import user_reco_pb2_grpc
from abtest import user_reco_pb2
import grpc
from setting.default import DefaultConfig
import time


def test():
    article_dict = {}
    # 构造传入数据

    req_article = user_reco_pb2.User()
    req_article.user_id = '1115629498121846784'
    req_article.channel_id = 18
    req_article.article_num = 10
    req_article.time_stamp = int(time.time() * 1000)
    # req_article.time_stamp = 1555573069870

    with grpc.insecure_channel(DefaultConfig.RPC_SERVER) as rpc_cli:
        print('''''')
        try:
            stub = user_reco_pb2_grpc.UserRecommendStub(rpc_cli)
            resp = stub.user_recommend(req_article)
        except Exception as e:
            print(e)
            article_dict['param'] = []
        else:

            # 解析返回结果参数
            article_dict['exposure_param'] = resp.exposure

            reco_arts = resp.recommends

            reco_art_param = []
            reco_list = []
            for art in reco_arts:
                reco_art_param.append({
                    'artcle_id': art.article_id,
                    'params': {
                        'click': art.params.click,
                        'collect': art.params.collect,
                        'share': art.params.share,
                        'read': art.params.read
                    }
                })

                reco_list.append(art.article_id)
            article_dict['param'] = reco_art_param

            # 文章列表以及参数（曝光参数 以及 每篇文章的点击等参数）
            print(reco_list, article_dict)

if __name__ == '__main__':
    test()
</code></pre>
<h2 id="通过哈希分桶进行流量切分"><a href="#通过哈希分桶进行流量切分" class="headerlink" title="通过哈希分桶进行流量切分"></a>通过哈希分桶进行流量切分</h2><pre><code class="python">def feed_recommend(user_id, channel_id, article_num, time_stamp):
    """
    1、根据web提供的参数，进行分流
    2、找到对应的算法组合之后，去推荐中心调用不同的召回和排序服务
    3、进行埋点参数封装
    :param user_id:用户id
    :param article_num:推荐文章个数
    :return: track:埋点参数结果: 参考上面埋点参数组合
    """

    #  产品前期推荐由于较少的点击行为，所以去做 用户冷启动 + 文章冷启动
    # 用户冷启动：'推荐'频道：热门频道的召回+用户实时行为画像召回（在线的不保存画像）  'C2'组合
    #            # 其它频道：热门召回 + 新文章召回   'C1'组合
    # 定义返回参数的类
    class TempParam(object):
        user_id = -10
        channel_id = -10
        article_num = -10
        time_stamp = -10
        algo = ""

    temp = TempParam()
    temp.user_id = user_id
    temp.channel_id = channel_id
    temp.article_num = article_num
    # 请求的时间戳大小
    temp.time_stamp = time_stamp

    # 先读取缓存数据redis+待推荐hbase结果
    # 如果有返回并加上埋点参数
    # 并且写入hbase 当前推荐时间戳用户（登录和匿名）的历史推荐文章列表

    # 传入用户id为空的直接召回结果
    if temp.user_id == "":
        temp.algo = ""
        return add_track([], temp)
    # 进行分桶实现分流，制定不同的实验策略
    bucket = hashlib.md5(user_id.encode()).hexdigest()[:1]
    if bucket in RAParam.BYPASS[0]['Bucket']:
        temp.algo = RAParam.BYPASS[0]['Strategy']
    else:
        temp.algo = RAParam.BYPASS[1]['Strategy']

    # 推荐服务中心推荐结果(这里做测试)
    track = add_track([], temp)

    return track
</code></pre>
<h2 id="推荐服务中心"><a href="#推荐服务中心" class="headerlink" title="推荐服务中心"></a>推荐服务中心</h2><ul>
<li>根据时间戳<ul>
<li>时间戳T小于HBASE历史推荐记录，则获取历史记录，返回该时间戳T上次的时间戳T-1</li>
<li>时间戳T大于HBASE历史推荐记录，则获取新推荐，则获取HBASE数据库中最近的一次时间戳<ul>
<li>如果有缓存，从缓存中拿，并且写入推荐历史表中</li>
<li>如果没有缓存，就进行一次指定算法组合的召回结果读取，排序，然后写入待推荐wait_recommend中，其中推荐出去的放入历史推荐表中</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="推荐中心业务逻辑"><a href="#推荐中心业务逻辑" class="headerlink" title="推荐中心业务逻辑"></a>推荐中心业务逻辑</h3><pre><code class="python">def feed_recommend_logic(self, temp):
    """推荐流业务逻辑
    :param temp:ABTest传入的业务请求参数
    """

    # 判断用请求的时间戳大小决定获取历史记录还是刷新推荐文章
    try:
        last_stamp = self.hbu.get_table_row('history_recommend', 'reco:his:{}'.format(temp.user_id).encode(),
                                            'channel:{}'.format(temp.channel_id).encode(), include_timestamp=True)[1]
        logger.info("{} INFO get user_id:{} channel:{} history last_stamp".format(
            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))
    except Exception as e:
        logger.warning("{} WARN read history recommend exception:{}".format(
            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))
        last_stamp = 0

    # 如果小于，走一遍正常的推荐流程，缓存或者召回排序
    logger.info("{} INFO history last_stamp:{},temp.time_stamp:{}".
                format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), last_stamp, temp.time_stamp))
    if last_stamp &lt; temp.time_stamp:

        # 获取
        res = redis_cache.get_reco_from_cache(temp, self.hbu)

        # 如果没有，然后走一遍算法推荐 召回+排序，同时写入到hbase待推荐结果列表
        if not res:
            logger.info("{} INFO get user_id:{} channel:{} recall/sort data".
                        format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))

            res = self.user_reco_list(temp)

        temp.time_stamp = int(last_stamp)

        track = add_track(res, temp)

    else:

        logger.info("{} INFO read user_id:{} channel:{} history recommend data".format(
            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))

        try:
            row = self.hbu.get_table_cells('history_recommend',
                                      'reco:his:{}'.format(temp.user_id).encode(),
                                      'channel:{}'.format(temp.channel_id).encode(),
                                      timestamp=temp.time_stamp + 1,
                                      include_timestamp=True)
        except Exception as e:
            logger.warning("{} WARN read history recommend exception:{}".format(
                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))
            row = []
            res = []

        # 1、如果没有历史数据，返回时间戳0以及结果空列表
        # 2、如果历史数据只有一条，返回这一条历史数据以及时间戳正好为请求时间戳，修改时间戳为0
        # 3、如果历史数据多条，返回最近一条历史数据，然后返回
        if not row:
            temp.time_stamp = 0
            res = []
        elif len(row) == 1 and row[0][1] == temp.time_stamp:
            res = eval(row[0][0])
            temp.time_stamp = 0
        elif len(row) &gt;= 2:
            res = eval(row[0][0])
            temp.time_stamp = int(row[1][1])

        res = list(map(int, res))
        logger.info(
            "{} INFO history:{}, {}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), res, temp.time_stamp))
        track = add_track(res, temp)
        # 曝光参数设置为空
        track['param'] = ''
    return track
</code></pre>
<h3 id="获取用户召回结果"><a href="#获取用户召回结果" class="headerlink" title="获取用户召回结果"></a>获取用户召回结果</h3><pre><code class="python">  def user_reco_list(self, temp):
        """
        获取用户的召回结果进行推荐
        :param temp:
        :return:
        """
        reco_set = []
        # 1、循环算法组合参数，遍历不同召回结果进行过滤
        for _num in RAParam.COMBINE[temp.algo][1]:
            # 进行每个召回结果的读取100,101,102,103,104
            if _num == 103:
                # 新文章召回读取
                _res = self.recall_service.read_redis_new_article(temp.channel_id)
                reco_set = list(set(reco_set).union(set(_res)))
            elif _num == 104:
                # 热门文章召回读取
                _res = self.recall_service.read_redis_hot_article(temp.channel_id)
                reco_set = list(set(reco_set).union(set(_res)))
            else:
                _res = self.recall_service.\
                    read_hbase_recall_data(RAParam.RECALL[_num][0],
                                           'recall:user:{}'.format(temp.user_id).encode(),
                                           '{}:{}'.format(RAParam.RECALL[_num][1], temp.channel_id).encode())
                # 进行合并某个协同过滤召回的结果
                reco_set = list(set(reco_set).union(set(_res)))

        # reco_set都是新推荐的结果，进行过滤
        history_list = []
        try:
            data = self.hbu.get_table_cells('history_recommend',
                                            'reco:his:{}'.format(temp.user_id).encode(),
                                            'channel:{}'.format(temp.channel_id).encode())
            for _ in data:
                history_list = list(set(history_list).union(set(eval(_))))

            logger.info("{} INFO filter user_id:{} channel:{} history data".format(
                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))
        except Exception as e:
            logger.warning(
                "{} WARN filter history article exception:{}".format(datetime.now().
                                                                     strftime('%Y-%m-%d %H:%M:%S'), e))

        # 如果0号频道有历史记录，也需要过滤

        try:
            data = self.hbu.get_table_cells('history_recommend',
                                            'reco:his:{}'.format(temp.user_id).encode(),
                                            'channel:{}'.format(0).encode())
            for _ in data:
                history_list = list(set(history_list).union(set(eval(_))))

            logger.info("{} INFO filter user_id:{} channel:{} history data".format(
                datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, 0))
        except Exception as e:
            logger.warning(
                "{} WARN filter history article exception:{}".format(datetime.now().
                                                                     strftime('%Y-%m-%d %H:%M:%S'), e))

        # 过滤操作 reco_set 与history_list进行过滤
        reco_set = list(set(reco_set).difference(set(history_list)))

        # 排序代码逻辑
        # _sort_num = RAParam.COMBINE[temp.algo][2][0]
        # reco_set = sort_dict[RAParam.SORT[_sort_num]](reco_set, temp, self.hbu)

        # 如果没有内容，直接返回
        if not reco_set:
            return reco_set
        else:

            # 类型进行转换
            reco_set = list(map(int, reco_set))

            # 跟后端需要推荐的文章数量进行比对 article_num
            # article_num &gt; reco_set
            if len(reco_set) &lt;= temp.article_num:
                res = reco_set
            else:
                # 之取出推荐出去的内容
                res = reco_set[:temp.article_num]
                # 剩下的推荐结果放入wait_recommend等待下次帅新的时候直接推荐
                self.hbu.get_table_put('wait_recommend',
                                       'reco:{}'.format(temp.user_id).encode(),
                                       'channel:{}'.format(temp.channel_id).encode(),
                                       str(reco_set[temp.article_num:]).encode(),
                                       timestamp=temp.time_stamp)
                logger.info(
                    "{} INFO put user_id:{} channel:{} wait data".format(
                        datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))

            # 放入历史记录表当中
            self.hbu.get_table_put('history_recommend',
                                   'reco:his:{}'.format(temp.user_id).encode(),
                                   'channel:{}'.format(temp.channel_id).encode(),
                                   str(res).encode(),
                                   timestamp=temp.time_stamp)
            # 放入历史记录日志
            logger.info(
                "{} INFO store recall/sorted user_id:{} channel:{} history_recommend data".format(
                    datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))

            return res
</code></pre>
<h3 id="在线预测"><a href="#在线预测" class="headerlink" title="在线预测"></a>在线预测</h3><p>除了对召回集进行排序以外，还可以在在线平台上使用离线训练好的点击率模型，得到高点击召回集。</p>
<pre><code class="python">def lr_sort_service(reco_set, temp, hbu):
    """
    排序返回推荐文章
    :param reco_set:召回合并过滤后的结果
    :param temp: 参数
    :param hbu: Hbase工具
    :return:
    """
    # 排序
    # 1、读取用户特征中心特征
    try:
        user_feature = eval(hbu.get_table_row('ctr_feature_user',
                                              '{}'.format(temp.user_id).encode(),
                                              'channel:{}'.format(temp.channel_id).encode()))
        logger.info("{} INFO get user user_id:{} channel:{} profile data".format(
            datetime.now().strftime('%Y-%m-%d %H:%M:%S'), temp.user_id, temp.channel_id))
    except Exception as e:
        user_feature = []

    if user_feature:
        # 2、读取文章特征中心特征
        result = []

        for article_id in reco_set:
            try:
                article_feature = eval(hbu.get_table_row('ctr_feature_article',
                                                         '{}'.format(article_id).encode(),
                                                         'article:{}'.format(article_id).encode()))
            except Exception as e:

                article_feature = [0.0] * 111
            f = []
            # 第一个channel_id
            f.extend([article_feature[0]])
            # 第二个article_vector
            f.extend(article_feature[11:])
            # 第三个用户权重特征
            f.extend(user_feature)
            # 第四个文章权重特征
            f.extend(article_feature[1:11])
            vector = DenseVector(f)
            result.append([temp.user_id, article_id, vector])

        # 4、预测并进行排序是筛选
        df = pd.DataFrame(result, columns=["user_id", "article_id", "features"])
        test = SORT_SPARK.createDataFrame(df)

        # 加载逻辑回归模型
        model = LogisticRegressionModel.load("hdfs://hadoop-master:9000/headlines/models/LR.obj")
        predict = model.transform(test)

        def vector_to_double(row):
            return float(row.article_id), float(row.probability[1])

        res = predict.select(['article_id', 'probability']).rdd.map(vector_to_double).toDF(
            ['article_id', 'probability']).sort('probability', ascending=False)
        article_list = [i.article_id for i in res.collect()]
        logger.info("{} INFO sorting user_id:{} recommend article".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                                                          temp.user_id))
        # 排序后，只将排名在前100个文章ID返回给用户推荐
        if len(article_list) &gt; 100:
            article_list = article_list[:100]
        reco_set = list(map(int, article_list))

    return reco_set
</code></pre>
<h3 id="多路召回"><a href="#多路召回" class="headerlink" title="多路召回"></a>多路召回</h3><pre><code class="python">import os
import sys
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, os.path.join(BASE_DIR))

from server import redis_client
from server import pool
import logging
from datetime import datetime
from abtest.utils import HBaseUtils

logger = logging.getLogger('recommend')


class ReadRecall(object):
    """读取召回集的结果
    """
    def __init__(self):
        self.client = redis_client
        self.hbu = HBaseUtils(pool)

    def read_hbase_recall_data(self, table_name, key_format, column_format):
        """获取指定用户的对应频道的召回结果,在线画像召回，离线画像召回，离线协同召回
        :return:
        """
        # 获取family对应的值
        # 数据库中的键都是bytes类型，所以需要进行编码相加
        # 读取召回结果多个版本合并
        recall_list = []
        try:

            data = self.hbu.get_table_cells(table_name, key_format, column_format)
            for _ in data:
                recall_list = list(set(recall_list).union(set(eval(_))))

            # 读取所有这个用户的在线推荐的版本，清空该频道的数据
            # self.hbu.get_table_delete(table_name, key_format, column_format)
        except Exception as e:
            logger.warning(
                "{} WARN read recall data exception:{}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))
        return recall_list

    def read_redis_new_data(self, channel_id):
        """获取redis新文章结果
        :param channel_id:
        :return:
        """
        # format结果
        logger.info("{} INFO read channel:{} new recommend data".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id))
        _key = "ch:{}:new".format(channel_id)
        try:
            res = self.client.zrevrange(_key, 0, -1)
        except redis.exceptions.ResponseError as e:
            logger.warning("{} WARN read new article exception:{}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))
            res = []
        return list(map(int, res))

    def read_redis_hot_data(self, channel_id):
        """获取redis热门文章结果
        :param channel_id:
        :return:
        """
        # format结果
        logger.info("{} INFO read channel:{} hot recommend data".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), channel_id))
        _key = "ch:{}:hot".format(channel_id)
        try:
            _res = self.client.zrevrange(_key, 0, -1)
        except redis.exceptions.ResponseError as e:
            logger.warning("{} WARN read hot article exception:{}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))
            _res = []
        # 每次返回前50热门文章
        res = list(map(int, _res))
        if len(res) &gt; 50:
            res = res[:50]
        return res

    def read_hbase_article_similar(self, table_name, key_format, article_num):
        """获取文章相似结果
        :param article_id: 文章id
        :param article_num: 文章数量
        :return:
        """
        # 第一种表结构方式测试：
        # create 'article_similar', 'similar'
        # put 'article_similar', '1', 'similar:1', 0.2
        # put 'article_similar', '1', 'similar:2', 0.34
        try:
            _dic = self.hbu.get_table_row(table_name, key_format)

            res = []
            _srt = sorted(_dic.items(), key=lambda obj: obj[1], reverse=True)
            if len(_srt) &gt; article_num:
                _srt = _srt[:article_num]
            for _ in _srt:
                res.append(int(_[0].decode().split(':')[1]))
        except Exception as e:
            logger.error("{} ERROR read similar article exception: {}".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S'), e))
            res = []
        return res


if __name__ == '__main__':

    rr = ReadRecall()
    print(rr.read_hbase_article_similar('article_similar', b'13342', 10))
    print(rr.read_hbase_recall_data('cb_recall', b'recall:user:1115629498121846784', b'als:18'))

    # rr = ReadRecall()
    # print(rr.read_redis_new_data(18))
</code></pre>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2023/02/13/%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title="复杂网络基础"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: 复杂网络基础</span></a><a class="button is-default" href="/2023/02/11/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6%E7%AE%80%E4%BB%8B/" title="大数据框架简介"><span class="has-text-weight-semibold">Next: 大数据框架简介</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="rockcor/blog-comment" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/rockcor"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> Rockcor 2023</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>