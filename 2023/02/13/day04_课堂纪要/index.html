<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Rockcor's blog</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="HDFS 分布式存储   spark storm  HBase
分布式结构 master slave
name node  client 负责文件的拆分  128MB  3份
data node
MapReduce 分布式计算  离线计算 2.X之前 速度比较慢 对比spark 
编程思想 Map  分  Reduce 合
hadoop streaming  Mrjob
Yarn 资源管理  cpu 内存  MapReduce spark 分布式计算
RM NM AM
社区版 CDH
什么是Hive
基于Hadoop 数据保存到HDFS

数据仓库工具

结构化的数据 映射为一张数据库表
01,张三,89
02,李四,91
03,赵武,92

HQL查询功能 (Hive SQL)

本质 把HQL翻译成Ma.."><meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Rockcor's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center"></p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFHive"><span class="toc-text">什么是Hive</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive%E6%9E%B6%E6%9E%84"><span class="toc-text">Hive架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive%E5%92%8CHadoop%E5%85%B3%E7%B3%BB"><span class="toc-text">Hive和Hadoop关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive%E5%92%8C%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8C%BA%E5%88%AB"><span class="toc-text">Hive和关系型数据库区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hive-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-text">Hive 基本使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UDF%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="toc-text">UDF自定义函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B"><span class="toc-text">综合案例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sqoop-%E4%BB%8B%E7%BB%8D"><span class="toc-text">sqoop 介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HBase%E4%BB%8B%E7%BB%8D"><span class="toc-text">HBase介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%A2%E5%90%91%E5%88%97%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">面向列数据库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hbase-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="toc-text">Hbase 数据模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hbase-%E5%92%8C-%E4%BC%A0%E7%BB%9F%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%8C%BA%E5%88%AB"><span class="toc-text">Hbase 和 传统关系型数据库区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HBase%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-text">HBase基础架构</span></a></div><div class="column is-9"><header class="my-4"></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle"></h1><time class="has-text-grey" datetime="2023-02-13T05:36:25.461Z">2023-02-13</time><article class="mt-2 post-content"><p>HDFS 分布式存储   spark storm  HBase</p>
<p>分布式结构 master slave</p>
<p>name node  client 负责文件的拆分  128MB  3份</p>
<p>data node</p>
<p>MapReduce 分布式计算  离线计算 2.X之前 速度比较慢 对比spark </p>
<p>编程思想 Map  分  Reduce 合</p>
<p>hadoop streaming  Mrjob</p>
<p>Yarn 资源管理  cpu 内存  MapReduce spark 分布式计算</p>
<p>RM NM AM</p>
<p>社区版 CDH</p>
<h3 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h3><ul>
<li><p>基于Hadoop 数据保存到HDFS</p>
</li>
<li><p>数据仓库工具</p>
</li>
<li><p>结构化的数据 映射为一张数据库表</p>
<p>01,张三,89</p>
<p>02,李四,91</p>
<p>03,赵武,92</p>
</li>
<li><p>HQL查询功能 (Hive SQL)</p>
</li>
<li><p>本质 把HQL翻译成MapReduce 降低使用hadoop计算的门槛</p>
</li>
<li><p>离线数据分析开发效率比直接用MapReduce 高</p>
</li>
</ul>
<h3 id="Hive架构"><a href="#Hive架构" class="headerlink" title="Hive架构"></a>Hive架构</h3><ul>
<li>用户接口：shell命令行</li>
<li>元数据存储<ul>
<li>数据库 表 都保存到那些位置上</li>
<li>表中的字段名字 类型</li>
<li>mysql derby(自带)</li>
</ul>
</li>
<li>Drive<ul>
<li>负责把HQL翻译成mapreduce</li>
<li>或者翻译成 shell 命令</li>
</ul>
</li>
</ul>
<h3 id="Hive和Hadoop关系"><a href="#Hive和Hadoop关系" class="headerlink" title="Hive和Hadoop关系"></a>Hive和Hadoop关系</h3><ul>
<li>利用hdfs存数据 利用mr算</li>
<li>Hive只需要跟 Master节点打交道 不需要集群</li>
</ul>
<h3 id="Hive和关系型数据库区别"><a href="#Hive和关系型数据库区别" class="headerlink" title="Hive和关系型数据库区别"></a>Hive和关系型数据库区别</h3><ul>
<li>hive 离线计算 海量查询</li>
<li>hive最主要做查询 不涉及删除修改 默认不支持删除修改，默认不支持事务，并不完全支持标准sql</li>
<li>sql CRUD全部支持， 支撑在线业务，索引完整 支持事务</li>
</ul>
<h3 id="Hive-基本使用"><a href="#Hive-基本使用" class="headerlink" title="Hive 基本使用"></a>Hive 基本使用</h3><ul>
<li>创建表</li>
</ul>
<pre><code class="sql">CREATE TABLE student(classNo string, stuNo string, score int) row format delimited fields terminated by ',';
</code></pre>
<ul>
<li><p>字段不需要指定占多少字节</p>
</li>
<li><p>需要通过row format delimited fields terminated by ‘,’指定列的分隔符 </p>
</li>
<li><p>加载表数据的时候尽量使用 load data方式 把整个文件put上去</p>
<pre><code class="sql">load data local inpath '/home/hadoop/tmp/student.txt'overwrite into table student;
</code></pre>
</li>
<li><p>内部表和外部表</p>
<ul>
<li><p>managed table</p>
<ul>
<li><p>创建表的时候  </p>
<pre><code class="sql">CREATE TABLE 表名（字段名 字段类型，）row format delimited fields terminated by ','
</code></pre>
</li>
<li><p>删除表</p>
<p>元数据和数据一起删除</p>
</li>
<li><p>数据位置</p>
<ul>
<li>默认是/user/hive/warehouse</li>
</ul>
</li>
</ul>
</li>
<li><p>external table</p>
<ul>
<li><p>建表语句</p>
<pre><code class="sql">CREATE External TABLE 表名（字段名 字段类型，）row format delimited fields terminated by ',' location '数据在hdfs上的路径';
</code></pre>
</li>
<li><p>删除表</p>
<ul>
<li>只删除元数据 数据会保留</li>
</ul>
</li>
<li><p>数据可以在hdfs上的任意位置</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>分区表</p>
<ul>
<li><p>当数据量比较大的时候，使用分区表可以缩小查询的数据范围</p>
</li>
<li><p>分区表实际上就是在表的目录下创建的子目录</p>
</li>
<li><p>如果有分区表的话查询的时候，尽量要使用分区字段</p>
</li>
<li><p>创建分区表的语句</p>
<pre><code class="sql">create table 表名 (字段名，字段类型....) partitioned by (分区字段名 分区字段类型) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
</code></pre>
</li>
<li><p>向分区表中插入数据</p>
<pre><code class="sql">load data local inpath '/home/hadoop/tmp/employee.txt' into table 表名 partition(分区字段名字='分区的具体值');
</code></pre>
</li>
<li><p>添加分区</p>
<pre><code class="sql">alter table 表名 add if not exists partition(分区字段名字='分区的具体值');
</code></pre>
</li>
<li><p>动态分区</p>
<ul>
<li><p>插入数据的时候指定分区的字段，会自动帮助创建分区所对应的文件夹</p>
</li>
<li><p>需要关掉默认设置</p>
<pre><code class="sql">set hive.exec.dynamic.partition.mode=nonstrict;
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="UDF自定义函数"><a href="#UDF自定义函数" class="headerlink" title="UDF自定义函数"></a>UDF自定义函数</h3><ul>
<li><p>hive提供的函数不能满足需求的时候就可以使用自定函数</p>
<ul>
<li><p>使用别人已经编译好的.jar</p>
<ul>
<li><p>jar加到 hive环境中</p>
</li>
<li><p>jar 可以在hdfs上 也可是在centos 上</p>
</li>
<li><p>创建一个临时函数</p>
<pre><code class="sql">CREATE TEMPORARY FUNCTION 自定义函数名字 as '自定义函数在jar包中的包名'
</code></pre>
</li>
<li><p>创建一个永久函数</p>
<pre><code class="sql">CREATE FUNCTION 自定义函数名字 as '自定义函数在jar包中的包名' using jar 'jar位置';
</code></pre>
</li>
</ul>
</li>
<li><p>自己写python脚本实现udf、udaf</p>
<ul>
<li><p>add file python文件的位置</p>
</li>
<li><pre><code class="sql">SELECT TRANSFORM(fname, lname) USING 'python udf1.py' AS (fname, l_name) FROM u;
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="综合案例"><a href="#综合案例" class="headerlink" title="综合案例"></a>综合案例</h3><ul>
<li><p>collect_set/collect_list</p>
<ul>
<li>group by之后 针对某一列聚合 结果放到[]</li>
<li>区别 一个去重 一个不去重</li>
</ul>
</li>
<li><p>lateral view explode</p>
<ul>
<li><p>explode函数 把复杂数据类型 array map 拆开 一行变多行</p>
</li>
<li><p>lateral view 和explode函数 配合使用 创建虚拟视图 可以把explode的结果和其它列一起查询</p>
</li>
<li><pre><code class="sql">select article_id,kw from articles
lateral view outer explode(key_words) t as kw
</code></pre>
</li>
</ul>
</li>
<li><p>CONCAT, CONCAT_WS</p>
<ul>
<li>不同列的字符串拼接到一起</li>
<li>concat_ws 可以把array中的元素拼接到同一个字符串中 指定分割符</li>
</ul>
</li>
<li><p>str_to_map 把具有key:value形式的字符串转换成map</p>
</li>
</ul>
<h3 id="sqoop-介绍"><a href="#sqoop-介绍" class="headerlink" title="sqoop 介绍"></a>sqoop 介绍</h3><ul>
<li><p>作用 数据交换工具 可以实现 数据在mysql oracle&lt;==&gt; hdfs之间互相传递</p>
</li>
<li><p>原理 通过写sqoop 命令 把sqoop命令翻译成mapreduce 通过mapreduce连接各种数据源 实现数据的传递</p>
</li>
<li><p>通过sqoop 把数据从mysql导入到hdfs</p>
<ul>
<li>sqoop import –connect jdbc:mysql://mysql数据库地址:3306/数据库名字 –username root –password password –table 要导出数据的表名 -m mrjob的数量</li>
<li>默认会把文件导入到 hdfs上 /user/linux用户名 文件夹下</li>
<li>通过 –target-dir指定其它位置</li>
</ul>
</li>
</ul>
<h3 id="HBase介绍"><a href="#HBase介绍" class="headerlink" title="HBase介绍"></a>HBase介绍</h3><ul>
<li><p>分布式开源数据库</p>
</li>
<li><p>面向列</p>
</li>
<li><p>Big Table开源实现</p>
</li>
<li><p>适合非结构化数据的存储</p>
</li>
<li><p>PB级别数据</p>
</li>
<li><p>可以支撑在线业务</p>
</li>
<li><p>分布式系统特点 ：易于扩展，支持动态伸缩，并发数据处理</p>
</li>
</ul>
<h3 id="面向列数据库"><a href="#面向列数据库" class="headerlink" title="面向列数据库"></a>面向列数据库</h3><ul>
<li><p>关系型数据库：行式存储 每一行数据都是连续的 所有的记录都放到一个连续的存储空间中</p>
</li>
<li><p>列数据库： 列式存储 每一列对应一个文件 不同列并不对应连续的存储空间</p>
</li>
<li><p>结构化数据 V.S. 非结构化数据</p>
<ul>
<li>结构化数据<ul>
<li>预定义的数据模型  模型一旦确定不会经常变化（表结构不会频繁调整）</li>
</ul>
</li>
<li>非结构化数据<ul>
<li>没有预定义数据模型</li>
<li>模型不规则 不完整</li>
<li>文本 图片 视频 音频</li>
</ul>
</li>
</ul>
</li>
<li><p>Hive 和 Hbase区别</p>
<ul>
<li>hive hbase 共同点<ul>
<li>都可以处理海量数据</li>
<li>文件都是保存到hdfs上</li>
</ul>
</li>
<li>hive 和 hbase不同<ul>
<li>计算不是通过mapreduce实现的 自己实现的CRUD功能</li>
<li>hive 通过mapreduce实现 数据查询的</li>
<li>hbase 可以有集群 集群的管理是通过zookeeper实现</li>
<li>hive 只能做离线计算</li>
<li>hbase 提供对数据的随机实时读/写访问功能</li>
</ul>
</li>
</ul>
</li>
<li><p>HBase 对事务的支持 只支持行级别的事务</p>
</li>
<li><p>CAP定理 </p>
<ul>
<li>分区容错性 分布式系统都要有的特性，任何时候都要能提供服务 P保证</li>
<li>HBase CP系统 强一致性</li>
</ul>
</li>
</ul>
<h3 id="Hbase-数据模型"><a href="#Hbase-数据模型" class="headerlink" title="Hbase 数据模型"></a>Hbase 数据模型</h3><ul>
<li>NameSpace 对应 关系型数据库 database  </li>
<li>表(table)：用于存储管理数据，具有稀疏的、面向列的特点。</li>
<li>行 （row）： 每一行都对应一个row key 行键 Hbase有索引但是只是在行键 rowkey有索引</li>
<li>列  Column family 和 Column qualifier 组成</li>
<li>列族（ColumnFamily）保存的就是 键值对集合 key:value</li>
<li>时间戳(TimeStamp)：是列的一个属性</li>
</ul>
<h3 id="Hbase-和-传统关系型数据库区别"><a href="#Hbase-和-传统关系型数据库区别" class="headerlink" title="Hbase 和 传统关系型数据库区别"></a>Hbase 和 传统关系型数据库区别</h3><ul>
<li><p>创建HBase表的时候只需要指定表名 和 列族</p>
</li>
<li><p>每一个行当中 只需要列族相同就可以了 至于每个列族中的 key:value对 key可以完全不同</p>
</li>
<li></li>
<li><h2 id="HBase基础架构"><a href="#HBase基础架构" class="headerlink" title="HBase基础架构"></a>HBase基础架构</h2></li>
<li><p>Client</p>
</li>
<li><p>Zookeeper</p>
<ul>
<li>保证HMaster有一个活着</li>
<li>HRegionServer HMaster地址存储</li>
<li>监控Region Server状态 将Region Server信息通知HMaster</li>
<li>元数据存储</li>
</ul>
</li>
<li><p>HMaster</p>
</li>
<li><p>HRegionServer</p>
</li>
<li><p>HStore</p>
<ul>
<li>每一个column family 对应了一个HStore</li>
</ul>
</li>
<li><p>HRegion</p>
</li>
<li><p>HLog</p>
</li>
</ul>
<p>面向列数据库 列式存储</p>
<p>适合存非关系型数据</p>
<p>hbase 创建表的过程很简单 只需要指定表名和列族的名字就可以了</p>
<p>create ‘表名’,’列族名字’</p>
<p>NameSpace -》数据库</p>
<p>table</p>
<p>row-key 行键  hbase的索引只在 row-key才有</p>
<p>column family 列族 key：value  这里面key 又叫 column quanlifier</p>
<p>不同行的 相同的column family 中  column quanlifier可以完全不同</p>
<p>组件</p>
<ul>
<li><p>HMaster</p>
</li>
<li><p>HRegionServer</p>
<ul>
<li>HRegion<ul>
<li>Hstore (一个列族对应)<ul>
<li>memstore</li>
<li>storefile</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>如果遇到 hdfs safe mode</p>
<p>通过 hdfs dfsadmin -safemode leave</p>
<p>hive 一定要先启动元数据服务</p>
<ul>
<li>hive –service metastore&amp;</li>
</ul>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2023/02/13/Hive&amp;HBase/" title=""><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: </span></a><a class="button is-default" href="/2022/04/29/GNN%E6%A6%82%E8%BF%B0/" title="GNN概述"><span class="has-text-weight-semibold">Next: GNN概述</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="rockcor/blog-comment" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/rockcor"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> Rockcor 2023</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>