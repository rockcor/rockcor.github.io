<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Rockcor's blog</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Hive一 Hive基本概念1 Hive简介 学习目标
- 了解什么是Hive
- 了解为什么使用Hive

1.1 什么是 Hive
Hive 由 Facebook 实现并开源，是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据映射为一张数据库表，并提供 HQL(Hive SQL)查询功能，底层数据是存储在 HDFS 上。
Hive 本质: 将 SQL 语句转换为 MapReduce 任务运行，使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据,是一款基于 HDFS 的 MapReduce 计算框架
主要用途：用来做离线数据分析，比直接用 MapReduce 开发效率更高。

1.2 为什么使用 Hive
直接使用 Hadoop MapReduce .."><meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">Rockcor's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center"></p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Hive"><span class="toc-text">Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80-Hive%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-text">一 Hive基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Hive%E7%AE%80%E4%BB%8B"><span class="toc-text">1 Hive简介 </span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-%E4%BB%80%E4%B9%88%E6%98%AF-Hive"><span class="toc-text">1.1 什么是 Hive</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8-Hive"><span class="toc-text">1.2 为什么使用 Hive</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Hive-%E6%9E%B6%E6%9E%84"><span class="toc-text">2 Hive 架构</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Hive-%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-text">2.1 Hive 架构图</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Hive-%E7%BB%84%E4%BB%B6"><span class="toc-text">2.2 Hive 组件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-Hive-%E4%B8%8E-Hadoop-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-text">2.3 Hive 与 Hadoop 的关系</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Hive-%E4%B8%8E%E4%BC%A0%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E6%AF%94"><span class="toc-text">3 Hive 与传统数据库对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Hive-%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="toc-text">4 Hive 数据模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Hive-%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-text">5 Hive 安装部署</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C-Hive-%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-text">二 Hive 基本操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Hive-HQL%E6%93%8D%E4%BD%9C%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-text">2.1 Hive HQL操作初体验</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Hive%E7%9A%84%E5%86%85%E9%83%A8%E8%A1%A8%E5%92%8C%E5%A4%96%E9%83%A8%E8%A1%A8"><span class="toc-text">2.2 Hive的内部表和外部表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%88%86%E5%8C%BA%E8%A1%A8"><span class="toc-text">2.3 分区表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA"><span class="toc-text">2.4 动态分区</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89-Hive-%E5%87%BD%E6%95%B0"><span class="toc-text">三 Hive 函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E5%86%85%E7%BD%AE%E8%BF%90%E7%AE%97%E7%AC%A6"><span class="toc-text">3.1 内置运算符</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0"><span class="toc-text">3.2 内置函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Hive-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%E5%92%8C-Transform"><span class="toc-text">3.3 Hive 自定义函数和 Transform</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B-hive%E7%BB%BC%E5%90%88%E6%A1%88%E4%BE%8B"><span class="toc-text">四 hive综合案例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Sqoop%E2%80%94%E5%B0%86mysql%E5%86%85%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%88%B0hdfs"><span class="toc-text">Sqoop—将mysql内的数据导入到hdfs</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94-HBase%E7%AE%80%E4%BB%8B%E4%B8%8E%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2"><span class="toc-text">五 HBase简介与环境部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-HBase%E7%AE%80%E4%BB%8B-amp-%E5%9C%A8Hadoop%E7%94%9F%E6%80%81%E4%B8%AD%E7%9A%84%E5%9C%B0%E4%BD%8D"><span class="toc-text">5.1 HBase简介&amp;在Hadoop生态中的地位</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-1-%E4%BB%80%E4%B9%88%E6%98%AFHBase%E2%80%93%E5%B9%B6%E9%9D%9E%E5%88%97%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">5.1.1 什么是HBase–并非列式数据库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-2-BigTable"><span class="toc-text">5.1.2 BigTable</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-3-%E9%9D%A2%E5%90%91%E5%88%97%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-text">5.1.3 面向列的数据库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-4-%E4%BB%80%E4%B9%88%E6%98%AF%E9%9D%9E%E7%BB%93%E6%9E%84%E5%8C%96%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="toc-text">5.1.4 什么是非结构化数据存储</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-5-HBase%E5%9C%A8Hadoop%E7%94%9F%E6%80%81%E4%B8%AD%E7%9A%84%E5%9C%B0%E4%BD%8D"><span class="toc-text">5.1.5 HBase在Hadoop生态中的地位</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-6-HBase%E4%B8%8EHDFS"><span class="toc-text">5.1.6 HBase与HDFS</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-7-HBase%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">5.1.7 HBase使用场景</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD-HBase%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="toc-text">六 HBase的数据模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#cap%E5%AE%9A%E7%90%86"><span class="toc-text">cap定理</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83-HBase-%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%AE%9E%E6%88%98"><span class="toc-text">七 HBase 的安装与实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-HBase%E7%9A%84%E5%AE%89%E8%A3%85%EF%BC%88%E5%90%8E%E9%9D%A2%E8%AE%B2%EF%BC%89"><span class="toc-text">7.1 HBase的安装（后面讲）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-HBase-shell"><span class="toc-text">7.2 HBase shell</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-HappyBase%E6%93%8D%E4%BD%9CHbase"><span class="toc-text">7.3 HappyBase操作Hbase</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB-HBase%E8%A1%A8%E8%AE%BE%E8%AE%A1"><span class="toc-text">八  HBase表设计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-2-HBase%E8%A1%A8%E8%AE%BE%E8%AE%A1%E6%A1%88%E4%BE%8B-%E7%A4%BE%E4%BA%A4%E5%BA%94%E7%94%A8%E4%BA%92%E7%B2%89%E4%BF%A1%E6%81%AF%E8%A1%A8"><span class="toc-text">8.2 HBase表设计案例: 社交应用互粉信息表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D-HBase%E7%BB%84%E4%BB%B6"><span class="toc-text">九 HBase组件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#9-1-HBase-%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-text">9.1 HBase 基础架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-2-HBase%E6%A8%A1%E5%9D%97%E5%8D%8F%E4%BD%9C"><span class="toc-text">9.2 HBase模块协作</span></a></li></ol></li></ol></li></ol></div><div class="column is-9"><header class="my-4"></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle"></h1><time class="has-text-grey" datetime="2023-02-13T05:36:25.471Z">2023-02-13</time><article class="mt-2 post-content"><h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><h2 id="一-Hive基本概念"><a href="#一-Hive基本概念" class="headerlink" title="一 Hive基本概念"></a>一 Hive基本概念</h2><h3 id="1-Hive简介"><a href="#1-Hive简介" class="headerlink" title="1 Hive简介 "></a>1 Hive简介 <img src="/img/hive.jpg"></h3><pre><code>学习目标
- 了解什么是Hive
- 了解为什么使用Hive
</code></pre>
<h4 id="1-1-什么是-Hive"><a href="#1-1-什么是-Hive" class="headerlink" title="1.1 什么是 Hive"></a>1.1 什么是 Hive</h4><ul>
<li>Hive 由 Facebook 实现并开源，是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据映射为一张数据库表，并提供 HQL(Hive SQL)查询功能，底层数据是存储在 HDFS 上。</li>
<li>Hive 本质: 将 SQL 语句转换为 MapReduce 任务运行，使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据,是一款基于 HDFS 的 MapReduce <strong>计算框架</strong></li>
<li>主要用途：用来做离线数据分析，比直接用 MapReduce 开发效率更高。</li>
</ul>
<h4 id="1-2-为什么使用-Hive"><a href="#1-2-为什么使用-Hive" class="headerlink" title="1.2 为什么使用 Hive"></a>1.2 为什么使用 Hive</h4><ul>
<li><p>直接使用 Hadoop MapReduce 处理数据所面临的问题：</p>
<ul>
<li>人员学习成本太高</li>
<li>MapReduce 实现复杂查询逻辑开发难度太大</li>
</ul>
</li>
<li><p>使用 Hive</p>
<ul>
<li><p>操作接口采用类 SQL 语法，提供快速开发的能力</p>
</li>
<li><p>避免了去写 MapReduce，减少开发人员的学习成本</p>
</li>
<li><p>功能扩展很方便</p>
</li>
</ul>
</li>
</ul>
<h3 id="2-Hive-架构"><a href="#2-Hive-架构" class="headerlink" title="2 Hive 架构"></a>2 Hive 架构</h3><h4 id="2-1-Hive-架构图"><a href="#2-1-Hive-架构图" class="headerlink" title="2.1 Hive 架构图"></a>2.1 Hive 架构图</h4><p><img src="/img/hive2.jpg"></p>
<h4 id="2-2-Hive-组件"><a href="#2-2-Hive-组件" class="headerlink" title="2.2 Hive 组件"></a>2.2 Hive 组件</h4><ul>
<li>用户接口：包括 CLI、JDBC/ODBC、WebGUI。<ul>
<li>CLI(command line interface)为 shell 命令行</li>
<li>JDBC/ODBC 是 Hive 的 JAVA 实现，与传统数据库JDBC 类似</li>
<li>WebGUI 是通过浏览器访问 Hive。</li>
<li>HiveServer2基于Thrift, 允许远程客户端使用多种编程语言如Java、Python向Hive提交请求</li>
</ul>
</li>
<li>元数据存储：通常是存储在关系数据库如 mysql/derby 中。<ul>
<li>Hive 将元数据存储在数据库中。</li>
<li>Hive 中的元数据包括<ul>
<li>表的名字</li>
<li>表的列</li>
<li>分区及其属性</li>
<li>表的属性（是否为外部表等）</li>
<li>表的数据所在目录等。</li>
</ul>
</li>
</ul>
</li>
<li>解释器、编译器、优化器、执行器:完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行</li>
</ul>
<h4 id="2-3-Hive-与-Hadoop-的关系"><a href="#2-3-Hive-与-Hadoop-的关系" class="headerlink" title="2.3 Hive 与 Hadoop 的关系"></a>2.3 Hive 与 Hadoop 的关系</h4><p>Hive 利用 HDFS 存储数据，利用 MapReduce 查询分析数据。</p>
<p>Hive是数据仓库工具，没有集群的概念，如果想提交Hive作业只需要在hadoop集群 Master节点上装Hive就可以了</p>
<h3 id="3-Hive-与传统数据库对比"><a href="#3-Hive-与传统数据库对比" class="headerlink" title="3 Hive 与传统数据库对比"></a>3 Hive 与传统数据库对比</h3><ul>
<li>hive 用于海量数据的离线数据分析。</li>
</ul>
<table>
  <tbody><tr>
    <th></th>
    <th>Hive</th>
    <th>关系型数据库</th>
  </tr>
  <tr>
    <td> ANSI SQL </td>
    <td> 不完全支持 </td>
    <td> 支持 </td>
  </tr>
  <tr>
    <td> 更新 </td>
    <td> INSERT OVERWRITE\INTO TABLE(默认) </td>
    <td> UPDATE\INSERT\DELETE </td>
  </tr>
  <tr>
    <td> 事务 </td>
    <td> 不支持(默认) </td>
    <td> 支持 </td>
  </tr>
  <tr>
    <td> 模式 </td>
    <td> 读模式 </td>
    <td> 写模式 </td>
  </tr>
  <tr>
    <td> 查询语言 </td>
    <td> HQL  </td>
    <td> SQL</td>
  </tr>
  <tr>
    <td> 数据存储 </td>
    <td> HDFS </td>
    <td> Raw Device or Local FS </td>
  </tr>
  <tr>
    <td> 执行 </td>
    <td> MapReduce </td>
    <td> Executor</td>
  </tr>
  <tr>
    <td> 执行延迟 </td>
    <td> 高 </td>
    <td> 低 </td>
  </tr>
  <tr>
    <td> 子查询 </td>
    <td> 只能用在From子句中 </td>
    <td> 完全支持 </td>
  </tr>
  <tr>
    <td> 处理数据规模 </td>
    <td> 大 </td>
    <td> 小 </td>
  </tr>
  <tr>
    <td> 可扩展性 </td>
    <td> 高 </td>
    <td> 低 </td>
  </tr>
  <tr>
    <td> 索引 </td>
    <td> 0.8版本后加入位图索引 </td>
    <td> 有复杂的索引 </td>
  </tr>
</tbody></table>

<ul>
<li>hive支持的数据类型<ul>
<li>原子数据类型  <ul>
<li>TINYINT SMALLINT INT BIGINT BOOLEAN FLOAT DOUBLE STRING BINARY TIMESTAMP DECIMAL CHAR VARCHAR DATE</li>
</ul>
</li>
<li>复杂数据类型<ul>
<li>ARRAY</li>
<li>MAP</li>
<li>STRUCT</li>
</ul>
</li>
</ul>
</li>
<li>hive中表的类型<ul>
<li>托管表 (managed table) (内部表)</li>
<li>外部表</li>
</ul>
</li>
</ul>
<h3 id="4-Hive-数据模型"><a href="#4-Hive-数据模型" class="headerlink" title="4 Hive 数据模型"></a>4 Hive 数据模型</h3><ul>
<li>Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式</li>
<li>在创建表时指定数据中的分隔符，Hive 就可以映射成功，解析数据。</li>
<li>Hive 中包含以下数据模型：<ul>
<li>db：在 hdfs 中表现为 hive.metastore.warehouse.dir 目录下一个文件夹</li>
<li>table：在 hdfs 中表现所属 db 目录下一个文件夹</li>
<li>external table：数据存放位置可以在 HDFS 任意指定路径</li>
<li>partition：在 hdfs 中表现为 table 目录下的子目录</li>
<li>bucket：在 hdfs 中表现为同一个表目录下根据 hash 散列之后的多个文件</li>
</ul>
</li>
</ul>
<h3 id="5-Hive-安装部署"><a href="#5-Hive-安装部署" class="headerlink" title="5 Hive 安装部署"></a>5 Hive 安装部署</h3><ul>
<li><p>Hive 安装前需要安装好 JDK 和 Hadoop。配置好环境变量。</p>
</li>
<li><p>下载Hive的安装包 <a target="_blank" rel="noopener" href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a> 并解压(我们hive是2.3.4)</p>
<pre><code class="shell"> tar -zxvf hive-1.1.0-cdh5.7.0.tar.gz  -C ~/app/
</code></pre>
</li>
<li><p>进入到 解压后的hive目录 找到 conf目录, 修改配置文件</p>
<pre><code class="shell">cp hive-env.sh.template hive-env.sh
vi hive-env.sh
</code></pre>
<p>在hive-env.sh中指定hadoop的路径</p>
<pre><code class="shell">HADOOP_HOME=/home/hadoop/app/hadoop-2.6.0-cdh5.7.0
</code></pre>
</li>
<li><p>配置环境变量</p>
<ul>
<li><pre><code class="shell">vi ~/.bash_profile
</code></pre>
</li>
<li><pre><code class="shell">export HIVE_HOME=/home/hadoop/app/hive-1.1.0-cdh5.7.0
export PATH=$HIVE_HOME/bin:$PATH
</code></pre>
</li>
<li><pre><code class="shell">source ~/.bash_profile
</code></pre>
</li>
</ul>
</li>
<li><p>根据元数据存储的介质不同，分为下面两个版本，其中 derby 属于内嵌模式。实际生产环境中则使用 mysql 来进行元数据的存储。</p>
<ul>
<li><p>内置 derby 版：<br>bin/hive 启动即可使用<br>缺点：不同路径启动 hive，每一个 hive 拥有一套自己的元数据，无法共享，每次需要去一个确定的路径启动hive</p>
</li>
<li><p>mysql 版： </p>
<ul>
<li><p>上传 mysql驱动到 hive安装目录的lib目录下</p>
<p>mysql-connector-java-5.*.jar</p>
</li>
<li><p>vi conf/hive-site.xml 配置 Mysql 元数据库信息(MySql安装见文档)</p>
<pre><code class="xml-dtd">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
&lt;!-- 插入以下代码 --&gt;
    &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
        &lt;value&gt;hive&lt;/value&gt;&lt;!-- 指定mysql用户名 --&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
        &lt;value&gt;hive&lt;/value&gt;&lt;!-- 指定mysql密码 --&gt;
    &lt;/property&gt;
   &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;mysql
        &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive?useSSL=false&lt;/value&gt;
    &lt;/property&gt;&lt;!-- 指定mysql数据库地址 --&gt;
    &lt;property&gt;
        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;!-- 指定mysql驱动 --&gt;
    &lt;/property&gt;
        &lt;!-- 到此结束代码 --&gt;
  &lt;property&gt;
    &lt;name&gt;hive.exec.script.wrapper&lt;/name&gt;
    &lt;value/&gt;
    &lt;description/&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
<li><p>hive启动</p>
<ul>
<li><p>启动docker （给大家的centos是默认安装好docker的）</p>
<p><code>service docker start</code></p>
</li>
<li><p>通过docker 启动mysql</p>
<p><code>docker start mysql</code></p>
</li>
<li><p>启动 hive的metastore元数据服务–注意这个让它在后台运行</p>
<p><code>hive --service metastore</code> &amp;</p>
</li>
<li><p>启动hive</p>
<p><code>hive</code></p>
</li>
<li><p>MySQL root 密码 password         hive用户 密码 hive</p>
<p>理解hive使用mysql的原理</p>
<pre><code>docker exec -it mysql bash
然后才能够执行mysql -u root -p的操作
</code></pre>
</li>
</ul>
</li>
</ul>
<h2 id="二-Hive-基本操作"><a href="#二-Hive-基本操作" class="headerlink" title="二 Hive 基本操作"></a>二 Hive 基本操作</h2><h3 id="2-1-Hive-HQL操作初体验"><a href="#2-1-Hive-HQL操作初体验" class="headerlink" title="2.1 Hive HQL操作初体验"></a>2.1 Hive HQL操作初体验</h3><ul>
<li><p>创建数据库</p>
<pre><code class="sql">CREATE DATABASE test;
</code></pre>
</li>
<li><p>显示所有数据库</p>
<pre><code class="sql">SHOW DATABASES;
</code></pre>
</li>
<li><p>创建表</p>
<pre><code class="sql">CREATE TABLE student(classNo string, stuNo string, score int) row format delimited fields terminated by ',';
</code></pre>
<ul>
<li>row format delimited fields terminated by ‘,’  指定了字段的分隔符为逗号，所以load数据的时候，load的文本也要为逗号，否则加载后为NULL。hive只支持单个字符的分隔符，hive默认的分隔符是\001</li>
</ul>
</li>
<li><p>将数据load到表中</p>
<ul>
<li><p>在本地文件系统创建一个如下的文本文件：/root/hadoop_code/hive/student.txt</p>
<pre><code>C01,N0101,82
C01,N0102,59
C01,N0103,65
C02,N0201,81
C02,N0202,82
C02,N0203,79
C03,N0301,56
C03,N0302,92
C03,N0306,72
</code></pre>
</li>
<li><pre><code class="sql"> load data local inpath '/root/hadoop_code/hive/student.txt'overwrite into table student;
</code></pre>
</li>
<li><p>这个命令将student.txt文件复制到hive的warehouse目录中，这个目录由hive.metastore.warehouse.dir配置项设置，默认值为/user/hive/warehouse。Overwrite选项将导致Hive事先删除student目录下所有的文件, 并将文件内容映射到表中。<br> Hive不会对student.txt做任何格式处理，因为Hive本身并不强调数据的存储格式。</p>
</li>
</ul>
</li>
<li><p>查询表中的数据 跟SQL类似</p>
<pre><code class="sql">hive&gt;select * from student;
</code></pre>
</li>
<li><p>分组查询group by和统计 count</p>
<pre><code class="sql">hive&gt;select classNo,count(score) from student where score&gt;=60 group by classNo;
</code></pre>
<p>从执行结果可以看出 hive把查询的结果变成了MapReduce作业通过hadoop执行</p>
</li>
</ul>
<h3 id="2-2-Hive的内部表和外部表"><a href="#2-2-Hive的内部表和外部表" class="headerlink" title="2.2 Hive的内部表和外部表"></a>2.2 Hive的内部表和外部表</h3><table>
  <tbody><tr>
    <th></th>
    <th>内部表(managed table)</th>
    <th>外部表(external table)</th>
  </tr>
  <tr>
    <td> 概念 </td>
    <td> 创建表时无external修饰 </td>
    <td> 创建表时被external修饰 </td>
  </tr>
  <tr>
    <td> 数据管理 </td>
    <td> 由Hive自身管理 </td>
    <td> 由HDFS管理 </td>
  </tr>
  <tr>
    <td> 数据保存位置 </td>
    <td> hive.metastore.warehouse.dir  （默认：/user/hive/warehouse） </td>
    <td> hdfs中任意位置 </td>
  </tr>
  <tr>
    <td> 删除时影响 </td>
    <td> 直接删除元数据（metadata）及存储数据 </td>
    <td> 仅会删除元数据，HDFS上的文件并不会被删除 </td>
  </tr>
  <tr>
    <td> 表结构修改时影响 </td>
    <td> 修改会将修改直接同步给元数据  </td>
    <td> 表结构和分区进行修改，则需要修复（MSCK REPAIR TABLE table_name;）</td>
  </tr>
</tbody></table>

<ul>
<li><p>案例</p>
<ul>
<li>创建一个外部表student2，/tmp/student是hadoop上的</li>
</ul>
<pre><code class="sql">CREATE EXTERNAL TABLE student2 (classNo string, stuNo string, score int) row format delimited fields terminated by ',' location '/tmp/student';
</code></pre>
<ul>
<li><p>装载数据—默认放到/user/hive/warehouse/,删除表后，数据还在</p>
<pre><code class="sql">load data local inpath '/root/hadoop_code/hive/student.txt' overwrite into table student2;
</code></pre>
</li>
</ul>
</li>
<li><p>显示表信息</p>
<pre><code class="sql">desc formatted table_name;
</code></pre>
</li>
<li><p>删除表查看结果</p>
<pre><code class="sql">drop table student;
</code></pre>
</li>
<li><p>再次创建外部表 student2</p>
</li>
<li><p>不插入数据直接查询查看结果</p>
<pre><code class="sql">select * from student2;
</code></pre>
</li>
</ul>
<h3 id="2-3-分区表"><a href="#2-3-分区表" class="headerlink" title="2.3 分区表"></a>2.3 分区表</h3><ul>
<li><p>什么是分区表</p>
<ul>
<li>随着表的不断增大，对于新纪录的增加，查找，删除等(DML)的维护也更加困难。对于数据库中的超大型表，可以通过把它的数据分成若干个小表，从而简化数据库的管理活动，对于每一个简化后的小表，我们称为一个单个的分区。</li>
<li>hive中分区表实际就是对应hdfs文件系统上独立的文件夹，该文件夹内的文件是该分区所有数据文件。</li>
<li>分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。</li>
<li>分类的标准就是分区字段，可以一个，也可以多个。</li>
<li>分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。</li>
</ul>
</li>
<li><p>创建分区表</p>
<pre><code class="shell">tom,4300
jerry,12000
mike,13000
jake,11000
rob,10000
</code></pre>
<pre><code class="sql">create table employee (name string,salary bigint) partitioned by (date1 string) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
</code></pre>
</li>
<li><p>查看表的分区</p>
<pre><code class="sql">show partitions employee;
</code></pre>
</li>
<li><p>添加分区</p>
<pre><code>alter table employee add if not exists partition(date1='2019-12-01');
</code></pre>
</li>
<li><p>加载数据到分区</p>
<pre><code>load data local inpath '/root/hadoop_code/hive/employee.txt' into table employee partition(date1='2019-12-01');
</code></pre>
</li>
<li><p>如果重复加载同名文件，不会报错，会自动创建一个*_copy_1.txt</p>
</li>
<li><p>外部分区表即使有分区的目录结构, 也必须要通过hql添加分区, 才能看到相应的数据</p>
<pre><code class="shell">hadoop fs -mkdir /user/hive/warehouse/emp/dt=2019-12-04
hadoop fs -copyFromLocal /tmp/employee.txt /user/hive/warehouse/test.db/emp/dt=2019-12-04/employee.txt
</code></pre>
<ul>
<li><p>此时查看表中数据发现数据并没有变化, 需要通过hql添加分区</p>
<pre><code>alter table emp add if not exists partition(dt='2018-12-04');
</code></pre>
</li>
<li><p>此时再次查看才能看到新加入的数据</p>
</li>
</ul>
</li>
<li><p>总结</p>
<ul>
<li>利用分区表方式减少查询时需要扫描的数据量<ul>
<li>分区字段不是表中的列, 数据文件中没有对应的列</li>
<li>分区仅仅是一个目录名</li>
<li>查看数据时, hive会自动添加分区列</li>
<li>支持多级分区, 多级子目录</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-4-动态分区"><a href="#2-4-动态分区" class="headerlink" title="2.4 动态分区"></a>2.4 动态分区</h3><ul>
<li><p>在写入数据时自动创建分区(包括目录结构)</p>
</li>
<li><p>创建表</p>
<pre><code>create table employee2 (name string,salary bigint) partitioned by (date1 string) row format delimited fields terminated by ',' lines terminated by '\n' stored as textfile;
</code></pre>
</li>
<li><p>导入数据</p>
<pre><code class="sql">insert into table employee2 partition(date1) select name,salary,date1 from employee;
</code></pre>
</li>
<li><p>使用动态分区需要设置参数</p>
<pre><code class="shell">set hive.exec.dynamic.partition.mode=nonstrict;
</code></pre>
</li>
</ul>
<h2 id="三-Hive-函数"><a href="#三-Hive-函数" class="headerlink" title="三 Hive 函数"></a>三 Hive 函数</h2><h3 id="3-1-内置运算符"><a href="#3-1-内置运算符" class="headerlink" title="3.1 内置运算符"></a>3.1 内置运算符</h3><p>在 Hive 有四种类型的运算符：</p>
<ul>
<li><p>关系运算符</p>
</li>
<li><p>算术运算符</p>
</li>
<li><p>逻辑运算符</p>
</li>
<li><p>复杂运算</p>
<p>(内容较多，见《Hive 官方文档》》)</p>
</li>
</ul>
<h3 id="3-2-内置函数"><a href="#3-2-内置函数" class="headerlink" title="3.2 内置函数"></a>3.2 内置函数</h3><p><a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF</a></p>
<ul>
<li>简单函数: 日期函数 字符串函数 类型转换 </li>
<li>统计函数: sum avg distinct</li>
<li>集合函数</li>
<li>分析函数</li>
<li>show functions;  显示所有函数</li>
<li>desc function 函数名;</li>
<li>desc function extended 函数名;</li>
</ul>
<h3 id="3-3-Hive-自定义函数和-Transform"><a href="#3-3-Hive-自定义函数和-Transform" class="headerlink" title="3.3 Hive 自定义函数和 Transform"></a>3.3 Hive 自定义函数和 Transform</h3><ul>
<li><p>UDF</p>
<ul>
<li><p>当 Hive 提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。</p>
</li>
<li><p><strong>TRANSFORM</strong>,and <strong>UDF</strong> and <strong>UDAF</strong></p>
<p>it is possible to plug in your own custom mappers and reducers</p>
<p> A UDF is basically only a transformation done by a mapper meaning that each row should be mapped to exactly one row. A UDAF on the other hand allows us to transform a group of rows into one or more rows, meaning that we can reduce the number of input rows to a single output row by some custom aggregation.</p>
<p><strong>UDF</strong>：就是做一个mapper，对每一条输入数据，映射为一条输出数据。</p>
<p><strong>UDAF</strong>:就是一个reducer，把一组输入数据映射为一条(或多条)输出数据。</p>
<p>一个脚本至于是做mapper还是做reducer，又或者是做udf还是做udaf，取决于我们把它放在什么样的hive操作符中。放在select中的基本就是udf，放在distribute by和cluster by中的就是reducer。</p>
<p>We can control if the script is run in a mapper or reducer step by the way we formulate our HiveQL query.</p>
<p>The statements DISTRIBUTE BY and CLUSTER BY allow us to indicate that we want to actually perform an aggregation.</p>
<p>User-Defined Functions (UDFs) for transformations and even aggregations which are therefore called User-Defined Aggregation Functions (UDAFs)</p>
</li>
</ul>
</li>
<li><p>UDF示例(运行java已经编写好的UDF)</p>
<ul>
<li><p>在hdfs中创建 /user/hive/lib目录</p>
<pre><code class="shell">hadoop fs -mkdir /user/hive/lib
</code></pre>
</li>
<li><p>把 hive目录下 lib/hive-contrib-hive-contrib-1.1.0-cdh5.7.0.jar 放到hdfs中</p>
<pre><code class="shell">hadoop fs -put hive-contrib-1.1.0-cdh5.7.0.jar /user/hive/lib/
</code></pre>
</li>
<li><p>把集群中jar包的位置添加到hive中</p>
<pre><code class="shell">hive&gt; add jar hdfs:///user/hive/lib/hive-contrib-1.1.0-cdh5.7.0.jar;
</code></pre>
</li>
<li><p>在hive中创建<strong>临时</strong>UDF，我们的自定义函数row_sequence，对应jar包中的UDFRowSequence</p>
<pre><code class="sql">hive&gt; CREATE TEMPORARY FUNCTION row_sequence as 'org.apache.hadoop.hive.contrib.udf.UDFRowSequence'
</code></pre>
</li>
<li><p>在之前的案例中使用<strong>临时</strong>自定义函数(函数功能: 添加自增长的行号)</p>
<pre><code class="sql">Select row_sequence(),* from employee;
</code></pre>
</li>
<li><p>创建<strong>非临时</strong>自定义函数</p>
<pre><code>CREATE FUNCTION row_sequence as 'org.apache.hadoop.hive.contrib.udf.UDFRowSequence' using jar 'hdfs:///user/hive/lib/hive-contrib-1.1.0-cdh5.7.0.jar';
</code></pre>
</li>
</ul>
</li>
<li><p>Python UDF</p>
<ul>
<li><p>准备案例环境</p>
<ul>
<li><p>创建表</p>
<pre><code class="sql">CREATE table u2(fname STRING,lname STRING);
</code></pre>
</li>
<li><p>向表中插入数据</p>
<pre><code class="sql">insert into table u2 values('George','washington');
insert into table u2 values('George','bush');
insert into table u2 values('Bill','clinton');
insert into table u2 values('Bill','gates');
</code></pre>
</li>
</ul>
</li>
<li><p>编写map风格脚本</p>
<pre><code class="python">import sys
for line in sys.stdin:
    line = line.strip()
    fname , lname = line.split('\t')
    l_name = lname.upper()
    print('\t'.join([fname, str(l_name)]))
</code></pre>
</li>
<li><p>通过hdfs向hive中ADD file</p>
<ul>
<li><p>加载文件到hdfs</p>
<pre><code class="shell">hadoop fs -put udf.py /user/hive/lib/
</code></pre>
</li>
<li><p>hive从hdfs中加载python脚本</p>
<pre><code class="shell">ADD FILE hdfs:///user/hive/lib/udf.py;  --这个是hadoop
ADD FILE /root/tmp/udf1.py;  ----本地的
</code></pre>
</li>
</ul>
<p>Transform</p>
<pre><code class="sql">SELECT TRANSFORM(fname, lname) USING 'python udf.py' AS (fname, l_name) FROM u2;
</code></pre>
</li>
</ul>
</li>
<li><p>Python UDAF</p>
</li>
</ul>
<h2 id="四-hive综合案例"><a href="#四-hive综合案例" class="headerlink" title="四 hive综合案例"></a>四 hive综合案例</h2><ul>
<li><p>内容推荐数据处理</p>
<p><img src="/./img/hive3.png"></p>
<ul>
<li>需求<ul>
<li>根据用户行为以及文章标签筛选出用户最感兴趣(阅读最多)的标签</li>
</ul>
</li>
</ul>
</li>
<li><p>相关数据</p>
<p>​    user_id article_id event_time</p>
<pre><code>11,101,2018-12-01 06:01:10
22,102,2018-12-01 07:28:12
33,103,2018-12-01 07:50:14
11,104,2018-12-01 09:08:12
22,103,2018-12-01 13:37:12
33,102,2018-12-02 07:09:12
11,101,2018-12-02 18:42:12
35,105,2018-12-03 09:21:12
22,104,2018-12-03 16:42:12
77,103,2018-12-03 18:31:12
99,102,2018-12-04 00:04:12
33,101,2018-12-04 19:10:12
11,101,2018-12-05 09:07:12
35,102,2018-12-05 11:00:12
22,103,2018-12-05 12:11:12
77,104,2018-12-05 18:02:02
99,105,2018-12-05 20:09:11
</code></pre>
<ul>
<li>文章数据</li>
</ul>
<pre><code>artical_id,artical_url,artical_keywords
101,http://www.wangdao.cn/1.html,kw8|kw1
102,http://www.wangdao.cn/2.html,kw6|kw3
103,http://www.wangdao.cn/3.html,kw7
104,http://www.wangdao.cn/4.html,kw5|kw1|kw4|kw9
105,http://www.wangdao.cn/5.html,
</code></pre>
</li>
<li><p>数据上传hdfs</p>
<pre><code class="shell">hadoop fs -mkdir /tmp/demo
hadoop fs -mkdir /tmp/demo/user_action
</code></pre>
</li>
<li><p>创建外部表</p>
<ul>
<li>用户行为表</li>
</ul>
<pre><code class="sql">drop table if exists user_actions;
CREATE EXTERNAL TABLE user_actions(
    user_id STRING,
    article_id STRING,
    time_stamp STRING
)
ROW FORMAT delimited fields terminated by ','
LOCATION '/tmp/demo/user_action';
</code></pre>
<ul>
<li>文章表</li>
</ul>
<pre><code class="sql">drop table if exists articles;
CREATE EXTERNAL TABLE articles(
    article_id STRING,
    url STRING,
    key_words array&lt;STRING&gt;
)
ROW FORMAT delimited fields terminated by ',' 
COLLECTION ITEMS terminated BY '|' 
LOCATION '/tmp/demo/article_keywords';
/*
key_words array&lt;STRING&gt;  数组的数据类型
COLLECTION ITEMS terminated BY '|'  数组的元素之间用'|'分割
*/
</code></pre>
<ul>
<li>查看数据</li>
</ul>
<pre><code class="sql">select * from user_actions;
select * from articles;
</code></pre>
<ul>
<li><p>分组查询每个用户的浏览记录</p>
<ul>
<li>collect_set/collect_list作用:<ul>
<li>将group by中的某列转为一个数组返回</li>
<li>collect_list<strong>不去重</strong>而collect_set<strong>去重</strong></li>
</ul>
</li>
<li>collect_set</li>
</ul>
<pre><code class="sql">select user_id,collect_set(article_id) 
from user_actions group by user_id;
</code></pre>
<pre><code class="shell">11      ["101","104"]
22      ["102","103","104"]
33      ["103","102","101"]
35      ["105","102"]
77      ["103","104"]
99      ["102","105"]
</code></pre>
<ul>
<li>collect_list</li>
</ul>
<pre><code class="sql">select user_id,collect_list(article_id) 
from user_actions group by user_id;
</code></pre>
<pre><code class="shell">
11      ["101","104","101","101"]
22      ["102","103","104","103"]
33      ["103","102","101"]
35      ["105","102"]
77      ["103","104"]
99      ["102","105"]
</code></pre>
<ul>
<li>sort_array: 对数组排序</li>
</ul>
<pre><code class="sql">select user_id,sort_array(collect_list(article_id)) as contents 
from user_actions group by user_id;
</code></pre>
<pre><code class="shell">11      ["101","101","101","104"]
22      ["102","103","103","104"]
33      ["101","102","103"]
35      ["102","105"]
77      ["103","104"]
99      ["102","105"]
</code></pre>
</li>
<li><p>查看每一篇文章的关键字 lateral view explode</p>
<ul>
<li>explode函数 将array 拆分</li>
</ul>
<pre><code class="sql">select explode(key_words) from articles;
</code></pre>
<ul>
<li>lateral view 和 explode 配合使用,将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合</li>
</ul>
<pre><code class="sql">select article_id,kw from articles lateral view explode(key_words) t as kw;
</code></pre>
<pre><code class="shell">101     kw8
101     kw1
102     kw6
102     kw3
103     kw7
104     kw5
104     kw1
104     kw4
104     kw9
</code></pre>
<pre><code class="sql">select article_id,kw from articles lateral view outer explode(key_words) t as kw;
</code></pre>
<pre><code class="shell">101     kw8
101     kw1
102     kw6
102     kw3
103     kw7
104     kw5
104     kw1
104     kw4
104     kw9
105     NULL
#含有outer
</code></pre>
</li>
<li><p>根据文章id找到用户查看文章的关键字</p>
<ul>
<li>原始数据</li>
</ul>
<pre><code class="shell">101     http://www.wangdao.cn/1.html     ["kw8","kw1"]
102     http://www.wangdao.cn/2.html     ["kw6","kw3"]
103     http://www.wangdao.cn/3.html     ["kw7"]
104     http://www.wangdao.cn/4.html     ["kw5","kw1","kw4","kw9"]
105     http://www.wangdao.cn/5.html     []
</code></pre>
<pre><code class="sql">select a.user_id, b.kw from user_actions 
as a left outer JOIN (select article_id,kw from articles
lateral view outer explode(key_words) t as kw) b
on (a.article_id = b.article_id)
order by a.user_id;
</code></pre>
<pre><code class="shell">11      kw1
11      kw8
11      kw5
11      kw1
11      kw4
11      kw1
11      kw9
11      kw8
11      kw1
11      kw8
22      kw1
22      kw7
22      kw9
22      kw4
22      kw5
22      kw7
22      kw3
22      kw6
33      kw8
33      kw1
33      kw3
33      kw6
33      kw7
35      NULL
35      kw6
35      kw3
77      kw9
77      kw1
77      kw7
77      kw4
77      kw5
99      kw3
99      kw6
99      NULL
</code></pre>
</li>
<li><p>根据文章id找到用户查看文章的关键字并统计频率</p>
<pre><code class="sql">select a.user_id, b.kw,count(1) as weight 
from user_actions as a 
left outer JOIN (select article_id,kw from articles
lateral view outer explode(key_words) t as kw) b
on (a.article_id = b.article_id)
group by a.user_id,b.kw 
order by a.user_id,weight desc;
</code></pre>
<pre><code class="shell">11      kw1     4
11      kw8     3
11      kw5     1
11      kw9     1
11      kw4     1
22      kw7     2
22      kw9     1
22      kw1     1
22      kw3     1
22      kw4     1
22      kw5     1
22      kw6     1
33      kw3     1
33      kw8     1
33      kw7     1
33      kw6     1
33      kw1     1
35      NULL    1
35      kw3     1
35      kw6     1
77      kw1     1
77      kw4     1
77      kw5     1
77      kw7     1
77      kw9     1
99      NULL    1
99      kw3     1
99      kw6     1
</code></pre>
</li>
<li><p>CONCAT：<br>CONCAT(str1,str2,…)  </p>
<p>返回结果为连接参数产生的字符串。如有任何一个参数为NULL ，则返回值为 NULL。</p>
<pre><code class="sql">select concat(user_id,article_id) from user_actions;
</code></pre>
<p>CONCAT_WS:</p>
<p>使用语法为：CONCAT_WS(separator,str1,str2,…)</p>
<p>CONCAT_WS() 代表 CONCAT With Separator ，是CONCAT()的特殊形式。第一个参数是其它参数的分隔符。分隔符的位置放在要连接的两个字符串之间。分隔符可以是一个字符串，也可以是其它参数。如果分隔符为 NULL，则结果为 NULL。</p>
<pre><code class="sql">select concat_ws(':',user_id,article_id) from user_actions;
</code></pre>
</li>
<li><p>将用户查看的关键字和频率合并成 key:value形式</p>
<pre><code class="sql">select a.user_id, concat_ws(':',b.kw,cast (count(1) as string)) as kw_w 
from user_actions as a 
left outer JOIN (select article_id,kw from articles
lateral view outer explode(key_words) t as kw) b
on (a.article_id = b.article_id)
group by a.user_id,b.kw;
</code></pre>
<pre><code class="shell">11      kw1:4
11      kw4:1
11      kw5:1
11      kw8:3
11      kw9:1
22      kw1:1
22      kw3:1
22      kw4:1
22      kw5:1
22      kw6:1
22      kw7:2
22      kw9:1
33      kw1:1
33      kw3:1
33      kw6:1
33      kw7:1
33      kw8:1
35      1
35      kw3:1
35      kw6:1
77      kw1:1
77      kw4:1
77      kw5:1
77      kw7:1
77      kw9:1
99      1
99      kw3:1
99      kw6:1
</code></pre>
</li>
<li><p>将用户查看的关键字和频率合并成 key:value形式并按用户聚合</p>
<pre><code class="sql">select cc.user_id,concat_ws(',',collect_set(cc.kw_w))
from(
select a.user_id, concat_ws(':',b.kw,cast (count(1) as string)) as kw_w 
from user_actions as a 
left outer JOIN (select article_id,kw from articles
lateral view outer explode(key_words) t as kw) b
on (a.article_id = b.article_id)
group by a.user_id,b.kw
) as cc 
group by cc.user_id;
</code></pre>
<pre><code class="shell">11      kw1:4,kw4:1,kw5:1,kw8:3,kw9:1
22      kw1:1,kw3:1,kw4:1,kw5:1,kw6:1,kw7:2,kw9:1
33      kw1:1,kw3:1,kw6:1,kw7:1,kw8:1
35      1,kw3:1,kw6:1
77      kw1:1,kw4:1,kw5:1,kw7:1,kw9:1
99      1,kw3:1,kw6:1
</code></pre>
</li>
<li><p>将上面聚合结果转换成map，通过str_to_map可以将字符串转为map</p>
<pre><code class="sql">select cc.user_id,str_to_map(concat_ws(',',collect_set(cc.kw_w))) as wm
from(
select a.user_id, concat_ws(':',b.kw,cast (count(1) as string)) as kw_w 
from user_actions as a 
left outer JOIN (select article_id,kw from articles
lateral view outer explode(key_words) t as kw) b
on (a.article_id = b.article_id)
group by a.user_id,b.kw
) as cc 
group by cc.user_id;
</code></pre>
<pre><code class="shell">11      {"kw1":"4","kw4":"1","kw5":"1","kw8":"3","kw9":"1"}
22      {"kw1":"1","kw3":"1","kw4":"1","kw5":"1","kw6":"1","kw7":"2","kw9":"1"}
33      {"kw1":"1","kw3":"1","kw6":"1","kw7":"1","kw8":"1"}
35      {"1":null,"kw3":"1","kw6":"1"}
77      {"kw1":"1","kw4":"1","kw5":"1","kw7":"1","kw9":"1"}
99      {"1":null,"kw3":"1","kw6":"1"}
</code></pre>
</li>
<li><p>将用户的阅读偏好结果保存到表中</p>
<pre><code class="sql">create table user_kws as 
select cc.user_id,str_to_map(concat_ws(',',collect_set(cc.kw_w))) as wm
from(
select a.user_id, concat_ws(':',b.kw,cast (count(1) as string)) as kw_w 
from user_actions as a 
left outer JOIN (select article_id,kw from articles
lateral view outer explode(key_words) t as kw) b
on (a.article_id = b.article_id)
group by a.user_id,b.kw
) as cc 
group by cc.user_id;
</code></pre>
</li>
<li><p>从表中通过key查询map中的值，这样知道每一个用户看kw1关键词的次数</p>
<pre><code class="sql">select user_id, wm['kw1'] from user_kws;
</code></pre>
<pre><code class="shell">11      4
22      1
33      1
35      NULL
77      1
99      NULL
</code></pre>
</li>
<li><p>从表中获取map中所有的key 和 所有的value</p>
<pre><code class="sql">select user_id,map_keys(wm),map_values(wm) from user_kws;
</code></pre>
<pre><code class="shell">11      ["kw1","kw4","kw5","kw8","kw9"] ["4","1","1","3","1"]
22      ["kw1","kw3","kw4","kw5","kw6","kw7","kw9"]     ["1","1","1","1","1","2","1"]
33      ["kw1","kw3","kw6","kw7","kw8"] ["1","1","1","1","1"]
35      ["1","kw3","kw6"]       [null,"1","1"]
77      ["kw1","kw4","kw5","kw7","kw9"] ["1","1","1","1","1"]
99      ["1","kw3","kw6"]       [null,"1","1"]
</code></pre>
</li>
<li><p>用lateral view explode把map中的数据转换成多列</p>
<pre><code class="sql">select user_id,keyword,weight from user_kws lateral view explode(wm) t as keyword,weight;
</code></pre>
<pre><code class="shell">11      kw1     4
11      kw4     1
11      kw5     1
11      kw8     3
11      kw9     1
22      kw1     1
22      kw3     1
22      kw4     1
22      kw5     1
22      kw6     1
22      kw7     2
22      kw9     1
33      kw1     1
33      kw3     1
33      kw6     1
33      kw7     1
33      kw8     1
35      1       NULL
35      kw3     1
35      kw6     1
77      kw1     1
77      kw4     1
77      kw5     1
77      kw7     1
77      kw9     1
99      1       NULL
99      kw3     1
99      kw6     1
</code></pre>
</li>
</ul>
</li>
</ul>
<h1 id="Sqoop—将mysql内的数据导入到hdfs"><a href="#Sqoop—将mysql内的数据导入到hdfs" class="headerlink" title="Sqoop—将mysql内的数据导入到hdfs"></a>Sqoop—将mysql内的数据导入到hdfs</h1><p>配置sqoop到环境变量里，然后执行sqoop-version看是否安装好</p>
<p>docker exec -ti mysql bash</p>
<p>mysql -u root -p 密码是password</p>
<p>给表u插入数据</p>
<pre><code class="sql">insert into u (fname,lname) values( ' George ' , 'washington ');
insert into u (fname,lname) values ( ' Geor ge ' , ' bush ' );
insert into u (fname,lname) values ('Bi17', 'clinton ' );
insert into u (fname,lname) values('Bi17' , ' gates ' );
</code></pre>
<p>Sqoop导入命令介绍</p>
<p>。命令语法: sqoop import(控制参数)(导入参数)</p>
<p>。命令元素:导入操作,数据源,访问方式，导入控制,目标地址。</p>
<p>。命令理解:数据从哪里来,有什么控制,到哪里去</p>
<p>执行下面语句之前，注意把/root/bigdata/hive/lib/mysql-connector-java-5.1.47.jar 放到/root/bigdata/sqoop/lib路径下</p>
<pre><code class="shell">sqoop import --connect jdbc:mysql://192.168.19.137:3306/test --username root --password password --table u -m 1
</code></pre>
<p>。添加–target-dir指定hdfs上数据存放的目录</p>
<pre><code class="shell">sqoop import --connect jdbc:mysql://localhost:3306/test --username root --password password --table u --target-dir /tmp/u1 -m 1
</code></pre>
<h2 id="五-HBase简介与环境部署"><a href="#五-HBase简介与环境部署" class="headerlink" title="五 HBase简介与环境部署"></a>五 HBase简介与环境部署</h2><h3 id="5-1-HBase简介-amp-在Hadoop生态中的地位"><a href="#5-1-HBase简介-amp-在Hadoop生态中的地位" class="headerlink" title="5.1 HBase简介&amp;在Hadoop生态中的地位"></a>5.1 HBase简介&amp;在Hadoop生态中的地位</h3><h4 id="5-1-1-什么是HBase–并非列式数据库"><a href="#5-1-1-什么是HBase–并非列式数据库" class="headerlink" title="5.1.1 什么是HBase–并非列式数据库"></a>5.1.1 什么是HBase–并非列式数据库</h4><ul>
<li>HBase是一个分布式的、面向列的开源数据库</li>
<li>HBase是Google BigTable的开源实现</li>
<li>HBase不同于一般的关系数据库, 适合非结构化数据存储</li>
</ul>
<h4 id="5-1-2-BigTable"><a href="#5-1-2-BigTable" class="headerlink" title="5.1.2 BigTable"></a>5.1.2 BigTable</h4><ul>
<li>BigTable是Google设计的分布式数据存储系统，用来处理海量的数据的一种非关系型(NoSQL)的数据库。<ul>
<li>适合大规模海量数据，PB级数据；</li>
<li>分布式、并发数据处理，效率极高；</li>
<li>易于扩展，支持动态伸缩</li>
<li>适用于廉价设备；</li>
<li>不适用于传统关系型数据的存储；</li>
</ul>
</li>
</ul>
<h4 id="5-1-3-面向列的数据库"><a href="#5-1-3-面向列的数据库" class="headerlink" title="5.1.3 面向列的数据库"></a>5.1.3 面向列的数据库</h4><p><strong>HBase 与 传统关系数据库的区别</strong></p>
<table>
  <tbody><tr>
    <th></th>
    <th>HBase</th>
    <th>关系型数据库</th>
  </tr>
  <tr>
    <td> 数据库大小 </td>
    <td> PB级别  </td>
    <td>GB TB</td>
  </tr>
  <tr>
    <td> 数据类型 </td>
    <td> Bytes </td>
    <td> 丰富的数据类型 </td>
  </tr>
    <tr>
    <td> 事务支持 </td>
    <td> ACID只支持单个Row级别 </td>
    <td> 全面的ACID支持, 对Row和表</td>
  </tr>
  <tr>
    <td> 索引 </td>
    <td> 只支持Row-key </td>
    <td> 支持 </td>
  </tr>
    <tr>
    <td> 吞吐量 </td>
    <td> 百万写入/秒 </td>
    <td> 数千写入/秒</td>
  </tr>
</tbody></table>

<ul>
<li>关系型数据库中数据示例</li>
</ul>
<table>
  <tbody><tr>
    <th>ID</th>
    <th>FILE NAME</th>
    <th>FILE PATH</th>
    <th>FILE TYPE</th>
    <th>FILE SIZE</th>
    <th>CREATOR</th>
  </tr>
  <tr>
    <td> 1 </td>
    <td> file1.txt  </td>
    <td>/home</td>
    <td> txt </td>
    <td> 1024 </td>
    <td> tom </td>
  </tr>
  <tr>
    <td> 2 </td>
    <td> file2.txt  </td>
    <td>/home/pics</td>
    <td> jpg </td>
    <td> 5032 </td>
    <td> jerry </td>
  </tr>
</tbody></table>

<ul>
<li>同样数据保存到列式数据库中</li>
</ul>
<table>
<tbody><tr>
<th>RowKey</th>
<th>FILE INFO</th>
<th>SAVE INFO</th>
</tr>
<tr>
<td> 1 </td>
<td> name:file1.txt
type:txt
size:1024</td>
<td>path:/home/pics
creator:Jerry
</td>
</tr>
<tr>
<td> 2 </td>
<td>name:file2.jpg
type:jpg
size:5032</td>
<td> path:/home
creator:Tom</td>
</tr>
</tbody></table>



<ul>
<li>行数据库&amp;列数据库存储方式比较</li>
</ul>
<p><img src="/./img/hbase4.png"></p>
<h4 id="5-1-4-什么是非结构化数据存储"><a href="#5-1-4-什么是非结构化数据存储" class="headerlink" title="5.1.4 什么是非结构化数据存储"></a>5.1.4 什么是非结构化数据存储</h4><ul>
<li>结构化数据<ul>
<li>适合用二维表来展示的数据</li>
</ul>
</li>
<li>非结构化数据<ul>
<li>非结构化数据是数据结构不规则或不完整</li>
<li>没有预定义的数据模型</li>
<li>不方便用数据库二维逻辑表来表现</li>
<li>办公文档、文本、图片、XML, HTML、各类报表、图像和音频/视频信息等</li>
</ul>
</li>
</ul>
<h4 id="5-1-5-HBase在Hadoop生态中的地位"><a href="#5-1-5-HBase在Hadoop生态中的地位" class="headerlink" title="5.1.5 HBase在Hadoop生态中的地位"></a>5.1.5 HBase在Hadoop生态中的地位</h4><ul>
<li><p>HBase是Apache基金会顶级项目</p>
</li>
<li><p>HBase基于HDFS进行数据存储</p>
</li>
<li><p>HBase可以存储超大数据并适合用来进行大数据的实时查询</p>
<p><img src="/./img/hbase&amp;hive.png"></p>
</li>
</ul>
<h4 id="5-1-6-HBase与HDFS"><a href="#5-1-6-HBase与HDFS" class="headerlink" title="5.1.6 HBase与HDFS"></a>5.1.6 HBase与HDFS</h4><ul>
<li>HBase建立在Hadoop文件系统上, 利用了HDFS的容错能力</li>
<li>HBase提供对数据的随机实时读/写访问功能</li>
<li>HBase内部使用哈希表, 并存储索引, 可以快速查找HDFS中数据</li>
</ul>
<h4 id="5-1-7-HBase使用场景"><a href="#5-1-7-HBase使用场景" class="headerlink" title="5.1.7 HBase使用场景"></a>5.1.7 HBase使用场景</h4><ul>
<li>瞬间写入量很大</li>
<li>大量数据需要长期保存, 且数量会持续增长</li>
<li>HBase不适合有join, 多级索引, 表关系复杂的数据模型</li>
</ul>
<h2 id="六-HBase的数据模型"><a href="#六-HBase的数据模型" class="headerlink" title="六 HBase的数据模型"></a>六 HBase的数据模型</h2><ul>
<li><p>NameSpace: 关系型数据库的”数据库”(database)</p>
</li>
<li><p>表(table)：用于存储管理数据，具有稀疏的、面向列的特点。HBase中的每一张表，就是所谓的大表(Bigtable)，可以有上亿行，上百万列。对于为值为空的列，并不占用存储空间，因此表可以设计的非常稀疏。</p>
</li>
<li><p>行(Row)：在表里面,每一行代表着一个数据对象,每一行都是以一个行键(Row Key)来进行唯一标识的, 行键并没有什么特定的数据类型, 以二进制的字节来存储—总体说明</p>
</li>
<li><p>列(Column): HBase的列由 Column family 和 Column qualifier 组成, 由冒号: 进行行间隔, 如 family: qualifier—总体说明</p>
</li>
<li><p>行键(RowKey)：类似于MySQL中的主键，HBase根据行键来快速检索数据，一个行键对应一条记录。与MySQL主键不同的是，HBase的行键是天然固有的，每一行数据都存在行键。</p>
</li>
<li><p>列族(ColumnFamily)：是列的集合。列族在表定义时需要指定，而列在插入数据时动态指定。列中的数据都是以二进制形式存在，没有数据类型。在物理存储结构上，每个表中的每个列族单独以一个文件存储。一个表可以有多个列簇。</p>
</li>
<li><p>列修饰符(<em>Column</em> <em>Qualifier</em>) : 列族中的数据通过列标识来进行映射, 可以理解为一个键值对(key-value), 列修饰符(<em>Column</em> <em>Qualifier</em>) 就是key 对应关系型数据库的列</p>
</li>
<li><p>时间戳(TimeStamp)：是列的一个属性，是一个64位整数。由行键和列确定的单元格，可以存储多个数据，每个数据含有时间戳属性，数据具有版本特性。可根据版本(VERSIONS)或时间戳来指定查询历史版本数据，如果都不指定，则默认返回最新版本的数据。</p>
</li>
<li><p>区域(Region)：HBase自动把表水平划分成的多个区域，划分的区域随着数据的增大而增多。</p>
</li>
<li><p>HBase 支持特定场景下的 ACID，即对行级别的操作保证完全的 ACID</p>
</li>
<li><h4 id="cap定理"><a href="#cap定理" class="headerlink" title="cap定理"></a>cap定理</h4><ul>
<li><p>分布式系统的最大难点，就是各个节点的状态如何同步。CAP 定理是这方面的基本定理，也是理解分布式系统的起点。</p>
<ul>
<li><p>一致性(所有节点在同一时间具有相同的数据)</p>
<p><img src="/img/Consistency.png" alt="img"></p>
</li>
<li><p>可用性(保证每个请求不管成功或失败都有响应,但不保证获取的数据的正确性)</p>
</li>
<li><p>分区容错性(系统中任意信息的丢失或失败不会影响系统的运行,系统如果不能在某个时限内达成数据一致性,就必须在上面两个操作之间做出选择)</p>
</li>
</ul>
<p><img src="/img/cap.jpg" alt="img"></p>
<p><strong>hbase是CAP中的CP系统,即hbase是强一致性的</strong></p>
</li>
</ul>
</li>
</ul>
<h2 id="七-HBase-的安装与实战"><a href="#七-HBase-的安装与实战" class="headerlink" title="七 HBase 的安装与实战"></a>七 HBase 的安装与实战</h2><h3 id="7-1-HBase的安装（后面讲）"><a href="#7-1-HBase的安装（后面讲）" class="headerlink" title="7.1 HBase的安装（后面讲）"></a>7.1 HBase的安装（后面讲）</h3><ul>
<li><p>下载安装包 <a target="_blank" rel="noopener" href="http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.7.0.tar.gz">http://archive.cloudera.com/cdh5/cdh/5/hbase-1.2.0-cdh5.7.0.tar.gz</a></p>
</li>
<li><p>配置伪分布式环境</p>
<ul>
<li><p>环境变量配置</p>
<pre><code class="shell">export HBASE_HOME=/usr/local/development/hbase-1.2.4
export PATH=$HBASE_HOME/bin:$PATH
</code></pre>
</li>
<li><p>配置hbase-env.sh</p>
<pre><code class="shell">export JAVA_HOME=/usr/local/development/jdk1.7.0_15
export HBASE_MANAGES_ZK=false  --如果你是使用hbase自带的zk就是true，如果使用自己的zk就是false
</code></pre>
</li>
<li><p>配置hbase-site.xml</p>
<pre><code class="xml">&lt;property&gt;
      &lt;name&gt;hbase.rootdir&lt;/name&gt;　　--hbase持久保存的目录
      &lt;value&gt;hdfs://hadoop001:8020/opt/hbase&lt;/value&gt;   
&lt;/property&gt;
&lt;property&gt;
      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;  --是否是分布式
      &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;     
          &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;    --指定要连接zk的端口
          &lt;value&gt;2181&lt;/value&gt;    
&lt;/property&gt;    
&lt;property&gt;        
          &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;            &lt;value&gt;/home/hadoop/app/hbase/zkData&lt;/value&gt;    
&lt;/property&gt;          
</code></pre>
</li>
<li><p>启动hbase（启动的hbase的时候要保证hadoop集群已经启动）</p>
<pre><code class="shell">/hbase/bin/start-hbase.sh
</code></pre>
</li>
<li><p>如果有<strong>HQuorumPeer，HRegionServer，ResourceManager</strong>就代表启动成功</p>
</li>
<li><p>输入hbase shell（进入shell命令行）</p>
</li>
<li><p>hbase执行list失败处理方法</p>
<p><a target="_blank" rel="noopener" href="https://my.oschina.net/2devil/blog/3034907">https://my.oschina.net/2devil/blog/3034907</a></p>
<p><strong>如果没启动成功，就做如下操作</strong></p>
<p>当你使用已存在的zookeeper 时， 以上的方案都不管用，你刚打开集群就有这样的问题： 可以清除掉 zookeeper 空间中的 /hbase 路径 关闭 hbase</p>
<pre><code>cd $HBASE_HOME/bin
./hbase zkcli
ls /
rmr /hbase
ls /
</code></pre>
<p>ctrl + c 退出 zookeeper 清理掉 HDFS 上的 hbase 存储路径 hdfs dfs -rm -r /hbase</p>
</li>
</ul>
</li>
</ul>
<h3 id="7-2-HBase-shell"><a href="#7-2-HBase-shell" class="headerlink" title="7.2 HBase shell"></a>7.2 HBase shell</h3><ul>
<li>HBase DDL 和 DML 命令—-千万别在命令后面加分号，加了反而不能运行</li>
</ul>
<table>
  <tbody><tr>
    <th>名称</th>
    <th>命令表达式</th>
  </tr>
  <tr>
    <td> 创建表 </td>
   <td> create '表名', '列族名1','列族名2','列族名n' </td>
  </tr>
  <tr>
    <td> 添加记录 </td>
    <td> put '表名','行名','列名:','值 </td>
  </tr>
    <tr>
    <td> 查看记录 </td>
    <td> get '表名','行名' </td>
  </tr>
  <tr>
    <td> 查看表中的记录总数 </td>
    <td> count '表名' </td>
  </tr>
    <tr>
    <td> 删除记录 </td>
    <td> delete '表名', '行名','列名' </td>
  </tr>
  <tr>
    <td> 删除一张表 </td>
    <td> 第一步 disable '表名' 第二步 drop '表名' </td>
  </tr>
  <tr>
    <td> 查看所有记录 </td>
    <td> scan "表名称" </td>
  </tr>
  <tr>
    <td> 查看指定表指定列所有数据 </td>
    <td> scan '表名' ,{COLUMNS=&gt;'列族名:列名'} </td>
  </tr>
   <tr>
    <td> 更新记录 </td>
    <td> 重写覆盖 </td>
  </tr>
</tbody></table>

<ul>
<li>连接集群</li>
</ul>
<pre><code>hbase shell
</code></pre>
<ul>
<li>创建表</li>
</ul>
<pre><code class="sql">create 'user','base_info'
</code></pre>
<ul>
<li>删除表</li>
</ul>
<pre><code class="sql">disable 'user'
drop 'user'
</code></pre>
<ul>
<li>创建名称空间</li>
</ul>
<pre><code class="sql">create_namespace 'test'
</code></pre>
<ul>
<li>展示现有名称空间</li>
</ul>
<pre><code class="sql">list_namespace
</code></pre>
<ul>
<li>创建表的时候添加namespace</li>
</ul>
<pre><code class="sql">create 'test:user','base_info'
</code></pre>
<ul>
<li>显示某个名称空间下有哪些表</li>
</ul>
<pre><code>list_namespace_tables 'test'
</code></pre>
<ul>
<li><p>插入数据</p>
<p>put  ‘表名’，‘rowkey的值’，’列族：列标识符‘，’值‘</p>
</li>
</ul>
<pre><code>put 'user','rowkey_10','base_info:username','Tom'
put 'user','rowkey_10','base_info:birthday','2014-07-10'
put 'user','rowkey_10','base_info:sex','1'
put 'user','rowkey_10','base_info:address','Tokyo'

put 'user','rowkey_16','base_info:username','Mike'
put 'user','rowkey_16','base_info:birthday','2014-07-10'
put 'user','rowkey_16','base_info:sex','1'
put 'user','rowkey_16','base_info:address','beijing'

put 'user','rowkey_22','base_info:username','Jerry'
put 'user','rowkey_22','base_info:birthday','2014-07-10'
put 'user','rowkey_22','base_info:sex','1'
put 'user','rowkey_22','base_info:address','Newyork'

put 'user','rowkey_24','base_info:username','Nico'
put 'user','rowkey_24','base_info:birthday','2014-07-10'
put 'user','rowkey_24','base_info:sex','1'
put 'user','rowkey_24','base_info:address','shanghai'

put 'user','rowkey_25','base_info:username','Rose'
put 'user','rowkey_25','base_info:birthday','2014-07-10'
put 'user','rowkey_25','base_info:sex','1'
put 'user','rowkey_25','base_info:address','Soul'
</code></pre>
<ul>
<li>查询表中的所有数据</li>
</ul>
<pre><code>scan 'user'
</code></pre>
<ul>
<li>查询某个rowkey的数据</li>
</ul>
<pre><code>get 'user','rowkey_16'
</code></pre>
<ul>
<li>查询某个列簇的数据</li>
</ul>
<pre><code class="shell">get 'user','rowkey_16','base_info'
get 'user','rowkey_16','base_info:username'
get 'user', 'rowkey_16', {COLUMN =&gt; ['base_info:username','base_info:sex']}
</code></pre>
<ul>
<li>删除表中的数据</li>
</ul>
<pre><code>delete 'user', 'rowkey_16', 'base_info:username'
</code></pre>
<ul>
<li>清空数据</li>
</ul>
<pre><code>truncate 'user'
</code></pre>
<ul>
<li>操作列簇</li>
</ul>
<pre><code>alter 'user', NAME =&gt; 'f2'
alter 'user', 'delete' =&gt; 'f2'
</code></pre>
<ul>
<li><p>HBase 追加型数据库 会保留多个版本数据</p>
<pre><code class="sql">desc 'user'
Table user is ENABLED
user
COLUMN FAMILIES DESCRIPTION
{NAME =&gt; 'base_info', VERSIONS =&gt; '1', EVICT_BLOCKS_ON_CLOSE =&gt; 'false', NEW_VERSION_B
HE_DATA_ON_WRITE =&gt; 'false', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', MI
ER =&gt; 'NONE', CACHE_INDEX_ON_WRITE =&gt; 'false', IN_MEMORY =&gt; 'false', CACHE_BLOOM
se', COMPRESSION =&gt; 'NONE', BLOCKCACHE =&gt; 'false', BLOCKSIZE =&gt; '65536'}
</code></pre>
<ul>
<li>VERSIONS=&gt;’1’说明最多可以显示一个版本 修改数据</li>
</ul>
<pre><code class="sql">put 'user','rowkey_10','base_info:username','Tom'
</code></pre>
<ul>
<li>指定显示多个版本</li>
</ul>
<pre><code class="shell">get 'user','rowkey_10',{COLUMN=&gt;'base_info:username',VERSIONS=&gt;2}
</code></pre>
<ul>
<li>修改可以显示的版本数量</li>
</ul>
<pre><code class="shell">alter 'user',NAME=&gt;'base_info',VERSIONS=&gt;10
</code></pre>
</li>
<li><p>命令表</p>
</li>
</ul>
<p><img src="/img/2017-12-27_230420.jpg"></p>
<p>可以通过HbaseUi界面查看表的信息</p>
<p>端口60010打不开的情况，是因为hbase 1.0 以后的版本，需要自己手动配置，在文件 hbase-site</p>
<pre><code>&lt;property&gt;  
&lt;name&gt;hbase.master.info.port&lt;/name&gt;  
&lt;value&gt;60010&lt;/value&gt;  
&lt;/property&gt; 
</code></pre>
<h3 id="7-3-HappyBase操作Hbase"><a href="#7-3-HappyBase操作Hbase" class="headerlink" title="7.3 HappyBase操作Hbase"></a>7.3 HappyBase操作Hbase</h3><ul>
<li><p>什么是HappyBase</p>
<ul>
<li><strong>HappyBase</strong> is a developer-friendly <a target="_blank" rel="noopener" href="http://python.org/">Python</a> library to interact with <a target="_blank" rel="noopener" href="http://hbase.apache.org/">Apache HBase</a>. HappyBase is designed for use in standard HBase setups, and offers application developers a Pythonic API to interact with HBase. Below the surface, HappyBase uses the <a target="_blank" rel="noopener" href="http://pypi.python.org/pypi/thrift">Python Thrift library</a> to connect to HBase using its <a target="_blank" rel="noopener" href="http://thrift.apache.org/">Thrift</a> gateway, which is included in the standard HBase 0.9x releases.</li>
</ul>
</li>
<li><p>HappyBase 是FaceBook员工开发的操作HBase的python库, 其基于Python Thrift, 但使用方式比Thrift简单, 已被广泛应用</p>
</li>
<li><p><strong>启动hbase thrift server : hbase-daemon.sh start thrift</strong></p>
<p>hbase thrift start-port:9090</p>
</li>
<li><p>安装happy base</p>
<ul>
<li>pip install happybase</li>
</ul>
</li>
<li><p>使用happy base时可能出现的问题(windows系统)</p>
<ul>
<li>happybase1.0在win下不支持绝对路径</li>
<li>解决方案：将488行的url_scheme == ”改为url_scheme in (‘代码盘符’, ”)</li>
</ul>
</li>
<li><p>如何使用HappyBase</p>
<ul>
<li>建立连接</li>
</ul>
<pre><code class="python">import happybase
connection = happybase.Connection('somehost')
</code></pre>
<ul>
<li>当连接建立时, 会自动创建一个与 HBase Thrift server的socket链接. 可以通过参数禁止自动链接, 然后再需要连接是调用 <a target="_blank" rel="noopener" href="https://happybase.readthedocs.io/en/latest/api.html#happybase.Connection.open"><code>Connection.open()</code></a>:</li>
</ul>
<pre><code class="python">connection = happybase.Connection('somehost', autoconnect=False)
# before first use:
connection.open()
</code></pre>
<ul>
<li><a target="_blank" rel="noopener" href="https://happybase.readthedocs.io/en/latest/api.html#happybase.Connection"><code>Connection</code></a>  这个类提供了一个与HBase交互的入口, 比如获取HBase中所有的表:  <a target="_blank" rel="noopener" href="https://happybase.readthedocs.io/en/latest/api.html#happybase.Connection.tables"><code>Connection.tables()</code></a>:</li>
</ul>
<pre><code class="python">print(connection.tables())
</code></pre>
<ul>
<li>操作表<ul>
<li>Table类提供了大量API, 这些API用于检索和操作HBase中的数据。 在上面的示例中，我们已经使用Connection.tables（）方法查询HBase中的表。 如果还没有任何表，可使用Connection.create_table（）创建一个新表：</li>
</ul>
</li>
</ul>
<pre><code class="python">connection.create_table('users',{'cf1': dict()})
</code></pre>
<ul>
<li><p>创建表之后可以传入表名获取到Table类的实例:</p>
<pre><code>table = connection.table('mytable')
</code></pre>
</li>
<li><p>查询操作</p>
</li>
</ul>
<pre><code class="python"># api
table.scan() #全表查询
table.row(row_keys[0]) # 查询一行
table.rows(row_keys) # 查询多行
#封装函数
def show_rows(table, row_keys=None):
    if row_keys:
        print('show value of row named %s' % row_keys)
        if len(row_keys) == 1:
            print(table.row(row_keys[0]))
        else:
            print(table.rows(row_keys))
    else:
        print('show all row values of table named %s' % table.name)
        for key, value in table.scan():
            print(key, value)
</code></pre>
<ul>
<li>插入数据</li>
</ul>
<pre><code class="python">#api
table.put(row_key, {cf:cq:value})
def put_row(table, column_family, row_key, value):
    print('insert one row to hbase')
    #put 'user','rowkey_10','base_info:username','Tom'
    #{'cf:cq':’数据‘}
    table.put(row_key, {'%s:name' % column_family:'name_%s' % value})

def put_rows(table, column_family, row_lines=30):
    print('insert rows to hbase now')
    for i in range(row_lines):
        put_row(table, column_family, 'row_%s' % i, i)
</code></pre>
<ul>
<li>删除数据</li>
</ul>
<pre><code class="python">#api
table.delete(row_key, cf_list)
    
#函数封装    
def delete_row(table, row_key, column_family=None, keys=None):
    if keys:
        print('delete keys:%s from row_key:%s' % (keys, row_key))
        key_list = ['%s:%s' % (column_family, key) for key in keys]
        table.delete(row_key, key_list)
    else:
        print('delete row(column_family:) from hbase')
        table.delete(row_key)
</code></pre>
<ul>
<li>删除表</li>
</ul>
<pre><code class="python">#api
conn.delete_table(table_name, True)
#函数封装
def delete_table(table_name):
    pretty_print('delete table %s now.' % table_name)
    conn.delete_table(table_name, True)
</code></pre>
</li>
<li><p>完整代码</p>
</li>
</ul>
<pre><code class="python">  import happybase
  
  hostname = '192.168.199.188'
  table_name = 'users'
  column_family = 'cf'
  row_key = 'row_1'
  
  conn = happybase.Connection(hostname)
  
  def show_tables():
      print('show all tables now')
      tables =  conn.tables()
      for t in tables:
          print t
  
  def create_table(table_name, column_family):
      print('create table %s' % table_name)
      conn.create_table(table_name, {column_family:dict()})
  
  
  def show_rows(table, row_keys=None):
      if row_keys:
          print('show value of row named %s' % row_keys)
          if len(row_keys) == 1:
              print table.row(row_keys[0])
          else:
              print table.rows(row_keys)
      else:
          print('show all row values of table named %s' % table.name)
          for key, value in table.scan():
              print key, value
  
  def put_row(table, column_family, row_key, value):
      print('insert one row to hbase')
      table.put(row_key, {'%s:name' % column_family:'name_%s' % value})
  
  def put_rows(table, column_family, row_lines=30):
      print('insert rows to hbase now')
      for i in range(row_lines):
          put_row(table, column_family, 'row_%s' % i, i)
  
  def delete_row(table, row_key, column_family=None, keys=None):
      if keys:
          print('delete keys:%s from row_key:%s' % (keys, row_key))
          key_list = ['%s:%s' % (column_family, key) for key in keys]
          table.delete(row_key, key_list)
      else:
          print('delete row(column_family:) from hbase')
          table.delete(row_key)
  
  def delete_table(table_name):
      pretty_print('delete table %s now.' % table_name)
      conn.delete_table(table_name, True)
  
  def pool():
      pretty_print('test pool connection now.')
      pool = happybase.ConnectionPool(size=3, host=hostname)
      with pool.connection() as connection:
          print connection.tables()
  
  def main():
      # show_tables()
      # create_table(table_name, column_family)
      # show_tables()
  
      table = conn.table(table_name)
      show_rows(table)
      put_rows(table, column_family)
      show_rows(table)
      #
      # # 更新操作
      # put_row(table, column_family, row_key, 'xiaoh.me')
      # show_rows(table, [row_key])
      #
      # # 删除数据
      # delete_row(table, row_key)
      # show_rows(table, [row_key])
      #
      # delete_row(table, row_key, column_family, ['name'])
      # show_rows(table, [row_key])
      #
      # counter(table, row_key, column_family)
      #
      # delete_table(table_name)
  
  if __name__ == "__main__":
      main()
</code></pre>
<h2 id="八-HBase表设计"><a href="#八-HBase表设计" class="headerlink" title="八  HBase表设计"></a>八  HBase表设计</h2><ul>
<li>设计HBase表时需要注意的特点<ul>
<li>HBase中表的索引是通过rowkey实现的</li>
<li>在表中是通过Row key的字典顺序来对数据进行排序的, 表中Region的划分通过起始Rowkey和结束Rowkey来决定的</li>
<li>所有存储在HBase中的数据都是二进制字节, 没有数据类型</li>
<li>原子性只在行内保证, HBase表中没有多行事务</li>
<li>列族(Column Family)在表创建之前就要定义好</li>
<li>列族中的列标识(Column Qualifier)可以在表创建后动态插入数据的时候添加</li>
<li>不同的column family保存在不同的文件中。</li>
</ul>
</li>
<li>如何设计HBase表<ul>
<li>Row key的结构该如何设置, Row key中又该包含什么样的信息</li>
<li>表中应该有多少的列族</li>
<li>列族中应该存储什么样的数据</li>
<li>每个列族中存储多少列数据</li>
<li>列的名字分别是什么</li>
<li>cell中应该存储什么样的信息</li>
<li>每个cell中存储多少个版本信息</li>
</ul>
</li>
<li>DDI  目的是为了克服HBase架构上的缺陷(join繁琐 只有row key索引等)<ul>
<li>Denormalization (反规范化, 解决join麻烦的问题)</li>
<li>Duplication (数据冗余)</li>
<li>Intelligent keys(通过row key设计实现 索引 排序对读写优化)</li>
</ul>
</li>
</ul>
<h3 id="8-2-HBase表设计案例-社交应用互粉信息表"><a href="#8-2-HBase表设计案例-社交应用互粉信息表" class="headerlink" title="8.2 HBase表设计案例: 社交应用互粉信息表"></a>8.2 HBase表设计案例: 社交应用互粉信息表</h3><ul>
<li><p>设计表保存应用中用户互粉的信息</p>
<ul>
<li>读场景:<ul>
<li>某用户都关注了哪些用户</li>
<li>用户A有没有关注用户B</li>
<li>谁关注了用户A</li>
</ul>
</li>
<li>写场景<ul>
<li>用户关注了某个用户</li>
<li>用户取消关注了某个用户</li>
</ul>
</li>
</ul>
</li>
<li><p>设计1:</p>
<ul>
<li>colunm qulifier(列名)  1:  2:</li>
</ul>
<p><img src="/img/table1.png"></p>
</li>
<li><p>设计2</p>
<ul>
<li>添加了一个 count 记录当前的最后一个记录的列名</li>
</ul>
<p><img src="/img/table2.png"></p>
</li>
<li><p>设计3</p>
<ul>
<li>列名 user_id</li>
</ul>
<p><img src="/img/table3.png"></p>
</li>
<li><p>最终设计(DDI)</p>
<ul>
<li>解决谁关注了用户A问题<ul>
<li>① 设计一张新表, 里面保存某个用户和他的粉丝</li>
<li>② 在同一张表中同时记录粉丝列表的和用户关注的列表, 并通过Rowkey来区分<ul>
<li>01_userid: 用户关注列表</li>
<li>02_userid: 粉丝列表</li>
</ul>
</li>
<li>上两种设计方案的问题(事务)</li>
</ul>
</li>
</ul>
</li>
<li><p>案例总结</p>
<ul>
<li>Rowkey是HBase表结构设计中很重要的环节, 直接影响到HBase的效率和性能</li>
<li>HBase的表结构比传统关系型数据库更灵活, 能存储任何二进制数据,无需考虑数据类型</li>
<li>利用列标识(Column Qualifier)来存储数据</li>
<li>衡量设计好坏的简单标准 是否会全表查询</li>
</ul>
</li>
</ul>
<h2 id="九-HBase组件"><a href="#九-HBase组件" class="headerlink" title="九 HBase组件"></a>九 HBase组件</h2><h3 id="9-1-HBase-基础架构"><a href="#9-1-HBase-基础架构" class="headerlink" title="9.1 HBase 基础架构"></a>9.1 HBase 基础架构</h3><p><img src="/img/structure.jpg"></p>
<p><strong>Client</strong></p>
<ul>
<li>①与zookeeper通信, 找到数据入口地址</li>
<li>②使用HBase RPC机制与HMaster和HRegionServer进行通信；</li>
<li>③Client与HMaster进行通信进行管理类操作；</li>
<li>④Client与HRegionServer进行数据读写类操作。</li>
</ul>
<p><strong>Zookeeper</strong></p>
<ul>
<li>①保证任何时候，集群中只有一个running master，避免单点问题；</li>
<li>②存贮所有Region的寻址入口，包括-ROOT-表地址、HMaster地址；</li>
<li>③实时监控Region Server的状态，将Region server的上线和下线信息，实时通知给Master；</li>
<li>④存储Hbase的schema，包括有哪些table，每个table有哪些column family。</li>
</ul>
<p><strong>HMaster</strong>(主)</p>
<p>可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。</p>
<p>角色功能：</p>
<ul>
<li>①为Region server分配region；</li>
<li>②负责region server的负载均衡；</li>
<li>③发现失效的region serve并重新分配其上的region；</li>
<li>④HDFS上的垃圾文件回收；</li>
<li>⑤处理用户对表的增删改查操作。</li>
</ul>
<p><strong>HRegionServer</strong>（从）</p>
<p>HBase中最核心的模块，主要负责响应用户I/O请求，向HDFS文件系统中读写数据。</p>
<p>作用：</p>
<ul>
<li>①维护Master分配给它的region，处理对这些region的IO请求；</li>
<li>②负责切分在运行过程中变得过大的region。</li>
<li>此外，HRegionServer管理一系列HRegion对象，每个HRegion对应Table中一个Region，HRegion由多个HStore组成，每个HStore对应Table中一个Column Family的存储，Column Family就是一个集中的存储单元，故将具有相同IO特性的Column放在一个Column Family会更高效。</li>
</ul>
<p><strong>HStore</strong></p>
<ul>
<li>HBase存储的核心，由MemStore和StoreFile组成。</li>
</ul>
<p><img src="/img/2.png"></p>
<ul>
<li>用户写入数据的流程为：client访问ZK, ZK返回RegionServer地址-&gt; client访问RegionServer写入数据 -&gt; 数据存入MemStore，一直到MemStore满 -&gt; Flush成StoreFile</li>
</ul>
<p><strong>HRegion</strong></p>
<ul>
<li>一个表最开始存储的时候，是一个region。</li>
<li>一个Region中会有个多个store，每个store用来存储一个列簇。如果只有一个column family，就只有一个store。</li>
<li>region会随着插入的数据越来越多，会进行拆分。默认大小是10G一个。</li>
</ul>
<p><strong>HLog</strong></p>
<ul>
<li>在分布式系统环境中，无法避免系统出错或者宕机，一旦HRegionServer意外退出，MemStore中的内存数据就会丢失，引入HLog就是防止这种情况。</li>
</ul>
<h3 id="9-2-HBase模块协作"><a href="#9-2-HBase模块协作" class="headerlink" title="9.2 HBase模块协作"></a>9.2 HBase模块协作</h3><ul>
<li>HBase启动<ul>
<li>HMaster启动, 注册到Zookeeper, 等待RegionServer汇报</li>
<li>RegionServer注册到Zookeeper, 并向HMaster汇报</li>
<li>对各个RegionServer(包括失效的)的数据进行整理, 分配Region和meta信息</li>
</ul>
</li>
<li>RegionServer失效<ul>
<li>HMaster将失效RegionServer上的Region分配到其他节点</li>
<li>HMaster更新hbase: meta 表以保证数据正常访问</li>
</ul>
</li>
<li>HMaster失效<ul>
<li>处于Backup状态的其他HMaster节点推选出一个转为Active状态</li>
<li>数据能正常读写, 但是不能创建删除表, 也不能更改表结构</li>
</ul>
</li>
</ul>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2023/02/13/%E5%A4%8D%E6%9D%82%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" title=""><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: </span></a><a class="button is-default" href="/2023/02/13/day04_%E8%AF%BE%E5%A0%82%E7%BA%AA%E8%A6%81/" title=""><span class="has-text-weight-semibold">Next: </span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="rockcor/blog-comment" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/rockcor"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> Rockcor 2023</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>